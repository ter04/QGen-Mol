THESIS TO-DO LIST: 

1. Metrics
* Use Negative Log Likelihood (NLL)???
* Other metrics:
    * Validity: Percentage of generated sequences that are valid SMILES.
    * Uniqueness: Percentage of valid molecules that are unique.
    * Novelty: Percentage of molecules not found in the training set.
    * FCD (Optional): Frechet ChemNet Distance to measure distributional similarity.


2. Define the Classical Baselines (The "Control Group")
    * Architecture: Explicitly define as a Conditional Gated Recurrent Unit (cGRU).
    * Conditioning Strategy: Explain "Input Concatenation." The property vector P (logP, QED, MW) is embedded, broadcasted, and added to the token embedding at every time step.
    * Recurrence: Use of nn.scan wrapping a standard GRUCell (Ref: Cho et al., 2014) for efficient autoregression.
    * Multi-Task Objective: Explain the dual output heads. 1) Token Head: Predicts next SMILES token (Cross-Entropy Loss). 2) Property Head: Reconstructs properties from the hidden state (MSE Loss).
    * Justification: Cite Caruana (1997) for Multi-Task Learning and Segler et al. (2018) for RNNs in chemistry. Explain that the auxiliary property head forces the latent space to organize according to chemical properties.



3. Define the Quantum Model (The "Proposal")

    * Scope Definition: State clearly that this section focuses on the Variational Quantum Circuit (VQC) as the sequence generator.
    * Ansatz Description:
    * Input Encoding: Classical context -> Rotation Gates (Ry, Rz).
    * Entanglement: Strongly Entangling Layers (CNOTs + Rotations).
    * Property Injection: How are logP/QED encoded into the circuit? (e.g., as rotation angles).


    * Parameter Count: Calculate P_Q (Quantum Params) vs. P_C (Classical Params) to highlight the compression or expressivity trade-off.

PHASE 2: IMPLEMENTATION & EXPERIMENTS

4. Experimental Setup

* Data Splitting: Create a fixed split (e.g., 80% Train, 10% Val, 10% Test). Crucial: Verify that the exact same splits are used for the MLP, cGRU, and Quantum models.
* Preprocessing: Explain Tokenization (SMILES char-level or SELFIES) and Normalization of properties (MinMax scaling to [0, 1] or [0, pi] for quantum rotations).

5. Result Logging (The "Lab Notebook")

* Hyperparameter Tracking: Log n_qubits, n_layers, and learning_rate strategy (Fixed vs. Cosine Decay). Explain that decay helps settle into minima in rough quantum landscapes.
* Performance Metrics: Log Training Loss (NLL + MSE), Validation Loss, and Accuracy.

PHASE 3: ANALYSIS & THESIS WRITING

6. Numerical Results Section

* Comparative Table: Columns for MLP, cGRU, Quantum. Rows for NLL, Validity, Uniqueness.
* The "Why" of Parameters: Discuss Pros/Cons. Classical cGRU has high parameter count, stable training, high memory. Quantum has low parameter count, expressive but hard to optimize (barren plateaus), limited context window.
* Generation Visuals: Show top 5 generated molecules for each model. Highlight "success" cases (where properties match the target logP) and "failure" cases (broken rings, invalid valency).
