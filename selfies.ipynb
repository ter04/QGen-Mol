{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import selfies as sf\n",
    "\n",
    "import numpy as np\n",
    "from math import ceil, log2\n",
    "import re\n",
    "import pandas as pd\n",
    "import optax\n",
    "import csv\n",
    "import flax.linen as nn\n",
    "import math\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import qchem\n",
    "from pennylane.templates import StronglyEntanglingLayers\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn.initializers import normal\n",
    "\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training molecules set:  65778\n"
     ]
    }
   ],
   "source": [
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "# Using the ChEMBL API to get the molecules dataset\n",
    "\n",
    "molecule = new_client.molecule\n",
    "\n",
    "# Filter for drug-like small molecules interesting for human use\n",
    "druglike_molecules = molecule.filter(\n",
    "    molecule_properties__heavy_atoms__lte=15,           # Heavy atoms less than 20\n",
    "    molecule_properties__alogp__lte=5,                  # LogP less than 5 (Lipophilicity and membrane permeability)\n",
    "    molecule_properties__mw_freebase__lte=300,          # Molecular weight less than 300 g/mol\n",
    "    molecule_properties__qed_weighted__gte=0.5,         # QED weighted greater than 0.5 (Drug-likeness)\n",
    "    molecule_properties__num_ro5_violations__lte=1,     # At most 1 Rule of 5 violation (Drug-likeness filter)\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Training molecules set: \", len(druglike_molecules))  # Check how many molecules match the filter criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the alphabet used for the SMILEs representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the alphabet considering the structure of some special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subset of molecules we are going to train the model with\n",
    "molecules_subset = druglike_molecules[:500]\n",
    "\n",
    "max_len = 0\n",
    "alphabet = set()\n",
    "# We use a subset of molecules to build the alphabet\n",
    "for mol in molecules_subset:  # Limiting to 1000 molecules for performance\n",
    "    smiles = mol.get('molecule_structures', {}).get('canonical_smiles')\n",
    "    selfies = sf.encoder(smiles)\n",
    "    if selfies:\n",
    "        # Skip if contains '.'\n",
    "        if \".\" in selfies:\n",
    "            continue\n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if max_len < len(tokens):\n",
    "            max_len = len(tokens)\n",
    "        alphabet.update(tokens)\n",
    "\n",
    "alphabet = sorted(alphabet)\n",
    "alphabet = ['<SOS>'] + alphabet  + ['<EOS>'] # Add Start-of-Secuence, End-of-secuence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet of SMILES characters: ['<SOS>', '[#Branch1]', '[#Branch2]', '[#C]', '[/C]', '[/N]', '[/S]', '[=Branch1]', '[=Branch2]', '[=C]', '[=N+1]', '[=N]', '[=O]', '[=P]', '[=Ring1]', '[=S]', '[Br]', '[Branch1]', '[Branch2]', '[C@@H1]', '[C@@]', '[C@H1]', '[C@]', '[C]', '[Cl]', '[F]', '[I]', '[N+1]', '[NH1]', '[N]', '[O-1]', '[O]', '[PH1]', '[P]', '[Ring1]', '[Ring2]', '[S+1]', '[S]', '[\\\\C]', '[\\\\Cl]', '[\\\\N]', '<EOS>']\n",
      "Total unique characters in SMILES: 42\n",
      "Maximum length of SMILES in dataset: 29\n",
      "Bits per token: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Alphabet of SMILES characters:\", alphabet)\n",
    "\n",
    "VOCABULARY_SIZE = len(alphabet)\n",
    "BITS_PER_TOKEN = ceil(log2(VOCABULARY_SIZE))  # n bits por token\n",
    "\n",
    "print(\"Total unique characters in SMILES:\", VOCABULARY_SIZE)\n",
    "print(\"Maximum length of SMILES in dataset:\", max_len)\n",
    "print(\"Bits per token:\", BITS_PER_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tSímbolos de enlaces y paréntesis: #, (, ), /, \\, =\n",
    "\n",
    "•\tDígitos simples para cierres de anillos: '1', '2', '3', '4', '5'\n",
    "\n",
    "•\tÁtomos orgánicos y halógenos comunes, tanto mayúsculas (alifáticos) como minúsculas (aromáticos)\n",
    "\n",
    "•\tTokens entre corchetes para isótopos, estados de carga, quiralidad, etc.\n",
    "\n",
    "•\tUn token especial '<PAD>' para padding en modelos ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<SOS>' → index 0 → 000000\n",
      "'[#Branch1]' → index 1 → 000001\n",
      "'[#Branch2]' → index 2 → 000010\n",
      "'[#C]' → index 3 → 000011\n",
      "'[/C]' → index 4 → 000100\n",
      "'[/N]' → index 5 → 000101\n",
      "'[/S]' → index 6 → 000110\n",
      "'[=Branch1]' → index 7 → 000111\n",
      "'[=Branch2]' → index 8 → 001000\n",
      "'[=C]' → index 9 → 001001\n",
      "'[=N+1]' → index 10 → 001010\n",
      "'[=N]' → index 11 → 001011\n",
      "'[=O]' → index 12 → 001100\n",
      "'[=P]' → index 13 → 001101\n",
      "'[=Ring1]' → index 14 → 001110\n",
      "'[=S]' → index 15 → 001111\n",
      "'[Br]' → index 16 → 010000\n",
      "'[Branch1]' → index 17 → 010001\n",
      "'[Branch2]' → index 18 → 010010\n",
      "'[C@@H1]' → index 19 → 010011\n",
      "'[C@@]' → index 20 → 010100\n",
      "'[C@H1]' → index 21 → 010101\n",
      "'[C@]' → index 22 → 010110\n",
      "'[C]' → index 23 → 010111\n",
      "'[Cl]' → index 24 → 011000\n",
      "'[F]' → index 25 → 011001\n",
      "'[I]' → index 26 → 011010\n",
      "'[N+1]' → index 27 → 011011\n",
      "'[NH1]' → index 28 → 011100\n",
      "'[N]' → index 29 → 011101\n",
      "'[O-1]' → index 30 → 011110\n",
      "'[O]' → index 31 → 011111\n",
      "'[PH1]' → index 32 → 100000\n",
      "'[P]' → index 33 → 100001\n",
      "'[Ring1]' → index 34 → 100010\n",
      "'[Ring2]' → index 35 → 100011\n",
      "'[S+1]' → index 36 → 100100\n",
      "'[S]' → index 37 → 100101\n",
      "'[\\C]' → index 38 → 100110\n",
      "'[\\Cl]' → index 39 → 100111\n",
      "'[\\N]' → index 40 → 101000\n",
      "'<EOS>' → index 41 → 101001\n"
     ]
    }
   ],
   "source": [
    "# Diccionario token → índice\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def print_token_bits(tokens, token_to_index):\n",
    "    for tok in tokens:\n",
    "        idx = token_to_index.get(tok, None)\n",
    "        if idx is None:\n",
    "            print(f\"Token '{tok}' no está en el diccionario.\")\n",
    "            continue\n",
    "        binary = format(idx, f'0{BITS_PER_TOKEN}b')\n",
    "        print(f\"'{tok}' → index {idx} → {binary}\")\n",
    "\n",
    "print_token_bits(alphabet, token_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input SMILEs must be the same size, so we need to use the padding to make it uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_encoded_dataset = []\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def smiles_to_bits(tokens: list) -> np.ndarray:\n",
    "    \"\"\"Convert tokens to a 2D array\"\"\"\n",
    "    padded_tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "    bit_matrix = []\n",
    "    for tok in padded_tokens:\n",
    "        idx = token_to_index[tok]\n",
    "        bits = list(f\"{idx:0{BITS_PER_TOKEN}b}\")  # length of the binary string depends on the number of bits required to represent the alphabet\n",
    "        bit_matrix.append([int(b) for b in bits])\n",
    "    return np.array(bit_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain the molecular properties of interest\n",
    "\n",
    "logP (o cx_logp) -> Coeficiente de partición octanol/agua (lipofilia)\n",
    "\n",
    "QED (quantitative estimate of drug-likeness) -> Escala combinada que evalúa qué tan “drug-like” es una molécula\n",
    "\n",
    "SAS (Synthetic Accessibility Score) -> Qué tan difícil sería sintetizar la molécula en laboratorio * Needs to be calculated separately!!\n",
    "\n",
    "MW (peso molecular) -> Masa total, típicamente ≤ 500 Da para buenos fármacos orales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the max and min range values in order to later normalize the properties in the range (0, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP range: -1.5 to 3.89\n",
      "QED range: 0.5 to 0.92\n",
      "MW range: 111.14 to 298.11\n"
     ]
    }
   ],
   "source": [
    "min_logp = float('inf')\n",
    "max_logp = float('-inf')\n",
    "min_qed = float('inf')\n",
    "max_qed = float('-inf')\n",
    "min_mw = float('inf')\n",
    "max_mw = float('-inf')\n",
    "\n",
    "\n",
    "# Iterate through the subset of molecules to find min/max properties to normalize them\n",
    "for mol in molecules_subset:\n",
    "    logP = mol.get('molecule_properties', {}).get('alogp')\n",
    "    qed = mol.get('molecule_properties', {}).get('qed_weighted')\n",
    "    mw = mol.get('molecule_properties', {}).get('mw_freebase')\n",
    "\n",
    "    if logP is None or qed is None or mw is None:\n",
    "        continue  # Skip if any property is missing\n",
    "\n",
    "    logP = float(logP)\n",
    "    qed = float(qed)\n",
    "    mw = float(mw)\n",
    "\n",
    "    if logP < min_logp:\n",
    "        min_logp = logP\n",
    "    if logP > max_logp:\n",
    "        max_logp = logP\n",
    "\n",
    "    if qed < min_qed:\n",
    "        min_qed = qed\n",
    "    if qed > max_qed:\n",
    "        max_qed = qed\n",
    "\n",
    "    if mw < min_mw:\n",
    "        min_mw = mw\n",
    "    if mw > max_mw:\n",
    "        max_mw = mw\n",
    "\n",
    "print(f\"LogP range: {min_logp} to {max_logp}\")\n",
    "print(f\"QED range: {min_qed} to {max_qed}\")\n",
    "print(f\"MW range: {min_mw} to {max_mw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_val, max_val, target_max=np.pi):\n",
    "    ''' Normalize a value to a range [0, [0, pi] to later encode them as rotation angles'''\n",
    "    norm = (value - min_val) / (max_val - min_val) * target_max\n",
    "    return float(f\"{norm:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequences in the subset: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum length of sequences in the subset:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the structured data to a CSV file\n",
    "with open(\"structured_data_selfies.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    n_tokens = max_len + 2  # +2 for <SOS> and <EOS>\n",
    "    header = [\"logP\", \"qed\", \"mw\"] + [f\"token_{i}\" for i in range(n_tokens)]\n",
    "    writer.writerow(header)\n",
    "\n",
    "\n",
    "    for mol in molecules_subset:\n",
    "        smiles = mol.get('molecule_structures', {}).get('canonical_smiles')\n",
    "        selfies = sf.encoder(smiles)\n",
    "        props = mol.get('molecule_properties', {})\n",
    "        if not selfies:\n",
    "            continue\n",
    "        if \".\" in selfies:\n",
    "            continue\n",
    "        try:\n",
    "            logP = float(props.get('alogp'))\n",
    "            qed = float(props.get('qed_weighted'))\n",
    "            mw = float(props.get('mw_freebase'))\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "\n",
    "        norm_logp = normalize(logP, min_logp, max_logp)\n",
    "        norm_qed = normalize(qed, min_qed, max_qed)\n",
    "        norm_mw = normalize(mw, min_mw, max_mw)\n",
    "\n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if not all(tok in token_to_index for tok in tokens):\n",
    "            continue\n",
    "\n",
    "        bit_matrix = smiles_to_bits(tokens)  # shape (n_tokens, 6)\n",
    "        token_bits_as_strings = [\"\".join(map(str, row)) for row in bit_matrix]\n",
    "\n",
    "        row = [norm_logp, norm_qed, norm_mw] + token_bits_as_strings\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantum Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def zstring_combos(wires):\n",
    "    \"\"\"\n",
    "    Return an ordered list of wire-tuples for all Z-strings up to order H_LOCAL.\n",
    "    Order: all 1-local, then all 2-local, ..., up to H_LOCAL.\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    for k in range(1, H_LOCAL + 1):\n",
    "        L.extend(itertools.combinations(wires, k))\n",
    "    return [tuple(c) for c in L]\n",
    "\n",
    "def num_zstrings(n_wires):\n",
    "    \"\"\"Count how many Z-strings up to order H_LOCAL.\"\"\"\n",
    "    from math import comb\n",
    "    return sum(comb(n_wires, k) for k in range(1, H_LOCAL + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Attention mechanism using SWAP test\n",
    "\n",
    "# --- Device for attention ---\n",
    "n_past = 5\n",
    "attn_dev = qml.device(\"default.qubit\", wires=(BITS_PER_TOKEN+1)*n_past)\n",
    "\n",
    "@qml.qnode(attn_dev, interface=\"jax\")\n",
    "def quantum_attention_qnode(Q_vec, K_vecs):\n",
    "    \"\"\"\n",
    "    Q_vec: projected query vector of current token\n",
    "    K_vecs: list of projected key vectors for past tokens\n",
    "    Returns: attention scores ⟨q_i | k_j⟩ for each j\n",
    "    \"\"\"\n",
    "    n_tokens = len(K_vecs)\n",
    "    q_wires = list(range(BITS_PER_TOKEN))\n",
    "\n",
    "    # Encode Q and all K in parallel (different wire registers)\n",
    "    def encode_token(angles, wires):\n",
    "        # Use AngleEmbedding for compactness, then entangle\n",
    "        qml.templates.AngleEmbedding(angles, wires=wires, rotation=\"Y\")\n",
    "        for i in range(len(wires)-1):\n",
    "            qml.CNOT(wires=[wires[i], wires[i+1]])\n",
    "\n",
    "    # Encode Q\n",
    "    encode_token(Q_vec, wires=q_wires)\n",
    "\n",
    "    # Encode K_j\n",
    "    # Collect expectation values (one per K_j)\n",
    "    measurements = []\n",
    "    for j, K_j in enumerate(K_vecs):\n",
    "        start = (BITS_PER_TOKEN+1) + j*(BITS_PER_TOKEN+1)\n",
    "        k_wires = list(range(start, start+BITS_PER_TOKEN))\n",
    "        encode_token(K_j, k_wires)\n",
    "\n",
    "        # SWAP test\n",
    "        ancilla = start + BITS_PER_TOKEN\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "        for qw, kw in zip(q_wires, k_wires):\n",
    "            qml.CSWAP(wires=[ancilla, qw, kw])\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "\n",
    "        measurements.append(qml.expval(qml.PauliZ(ancilla)))\n",
    "\n",
    "    # **Return as tuple** so PennyLane converts to JAX array\n",
    "    return tuple(measurements)\n",
    "\n",
    "\n",
    "def quantum_attention(Q_vec, K_vecs, V_vecs):\n",
    "    # Make non-traced (concrete) copies for the QNode\n",
    "    Q_safe = jax.lax.stop_gradient(Q_vec)\n",
    "    K_safe = [jax.lax.stop_gradient(k) for k in K_vecs]\n",
    "\n",
    "    raw_expvals = quantum_attention_qnode(Q_safe, K_safe)\n",
    "    raw_expvals = jnp.asarray(raw_expvals)\n",
    "    # Convert from expectation values (in [-1,1]) to probabilities [0,1]\n",
    "    overlaps = (1.0 - raw_expvals) / 2.0\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def classical_attention(Q_vec, K_vecs, V_vecs, scale=True):\n",
    "    # Q_vec: (proj_dim,) ; K_vecs: list of (proj_dim,) ; V_vecs: list of (proj_dim,)\n",
    "    if len(K_vecs) == 0:\n",
    "        return V_vecs[0] if len(V_vecs) > 0 else jnp.zeros_like(Q_vec)\n",
    "\n",
    "    K_mat = jnp.stack(K_vecs)  # shape (n_past, proj_dim)\n",
    "    # dot-product softmax (fully differentiable)\n",
    "    scores = jnp.dot(K_mat, Q_vec)  # shape (n_past,)\n",
    "    if scale:\n",
    "        scores = scores / jnp.sqrt(Q_vec.shape[0])\n",
    "    # Causal mask: only past tokens (K_vecs are already past tokens)\n",
    "    weights = jax.nn.softmax(scores)\n",
    "    # Weighted sum over classical V\n",
    "    output = jnp.sum(weights[:, None] * jnp.stack(V_vecs), axis=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and qubit setup\n",
    "# BITS_PER_TOKEN number of qubits needed to encode each token\n",
    "n_prop_qubits = 3  # number of qubits needed to encode properties (logP, QED, MW)\n",
    "n_ancillas = 3  # number of ancilla qubits that represent the environment\n",
    "n_total_qubits = n_prop_qubits + BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "N_LAYERS = 6  # number of variational layers\n",
    "H_LOCAL = 3 # h_local sets the maximum number of qubits that can interact in each Z-string term of Σ\n",
    "\n",
    "\n",
    "# Name them explicitly\n",
    "prop_wires = [f\"prop_{i}\" for i in range(n_prop_qubits)]\n",
    "token_wires = [f\"token_{i}\" for i in range(BITS_PER_TOKEN)]\n",
    "ancilla_wires = [f\"ancilla_{i}\" for i in range(n_ancillas)]\n",
    "all_wires = prop_wires + token_wires + ancilla_wires\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=all_wires)\n",
    "\n",
    "\n",
    "def molecular_property_encoder(props):\n",
    "    \"\"\"Encode continuous props on property qubits via RY rotations\"\"\"\n",
    "    for wire, val in zip(prop_wires, props):\n",
    "        qml.RY(val, wires=wire)\n",
    "\n",
    "def token_encoder(token_bits):\n",
    "    \"\"\"Basis-encode token bits on token qubits\"\"\"\n",
    "    qml.BasisState(token_bits, wires=token_wires)\n",
    "\n",
    "\n",
    "def operator_layer(theta_params, theta_prop, wires):\n",
    "    \"\"\"\n",
    "    Variational layer where:\n",
    "      - theta_params[...] are rotations for token + ancilla qubits\n",
    "      - theta_prop encodes property→token entanglement\n",
    "    \"\"\"\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "\n",
    "    # Property → token entanglement \n",
    "    for p, prop_wire in enumerate(prop_wires):\n",
    "        for t, t_a_wire in enumerate(token_ancilla_ws):\n",
    "            qml.CRX(theta_prop[p, t, 0], wires=[prop_wire, t_a_wire])\n",
    "            qml.CRY(theta_prop[p, t, 1], wires=[prop_wire, t_a_wire])\n",
    "\n",
    " \n",
    "    qml.StronglyEntanglingLayers(\n",
    "        weights=theta_params[None,:,:],  # shape: (n_token_ancilla, 3)\n",
    "        wires=token_ancilla_ws\n",
    "    )\n",
    "\n",
    "def Sigma_layer_vec(gamma_vec, token_ancilla_ws, time=1.0, combos=None):\n",
    "    \"\"\"\n",
    "    Diagonal multi-Z unitary Σ = exp(i * sum_s gamma_s * Z^{⊗|s|} * t)\n",
    "    using a flat parameter vector 'gamma_vec' aligned with 'combos'.\n",
    "    \"\"\"\n",
    "    #token_ancilla_ws = list(wires)  # pass token+ancilla here\n",
    "    if combos is None:\n",
    "        combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    # Safety: ensure the vector length matches the number of combos\n",
    "    assert gamma_vec.shape[0] == len(combos), \\\n",
    "        f\"gamma_vec has length {gamma_vec.shape[0]} but expected {len(combos)}\"\n",
    "\n",
    "    # MultiRZ(phi) = exp(-i * phi/2 * Z^{⊗k}); choose phi = -2 * gamma * time\n",
    "    for gamma, combo in zip(gamma_vec, combos):\n",
    "        qml.MultiRZ(-2.0 * gamma * time, wires=list(combo))\n",
    "\n",
    "\n",
    "# QNode combining encoding and variational layers\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def autoregressive_model(token_bits, props, theta_params, theta_prop, sigma_params, output_i):\n",
    "    molecular_property_encoder(props)      # Encode MW, logP, QED\n",
    "    token_encoder(token_bits)              # Basis-encode token bits\n",
    "\n",
    "    # --- Encode output_i embedding safely ---\n",
    "    for i, val in enumerate(output_i):\n",
    "        qml.RY(val, wires=token_wires[i])\n",
    "\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "    combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    for l in range(N_LAYERS):\n",
    "        # Forward V(θ)\n",
    "        operator_layer(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "\n",
    "        # Diagonal Σ(γ,t): vector API\n",
    "        Sigma_layer_vec(sigma_params[l], token_ancilla_ws, time=1.0, combos=combos)\n",
    "\n",
    "        # Backward V(θ)†\n",
    "        qml.adjoint(operator_layer)(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "\n",
    "    return qml.probs(wires=token_wires)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitstr_to_array(bitstr):\n",
    "    \"\"\"Convert a string of bits (e.g., '010101') to a numpy float32 array.\"\"\"\n",
    "    return np.array([int(b) for b in bitstr], dtype=np.float32)\n",
    "\n",
    "def build_training_data(df):\n",
    "    \"\"\"\n",
    "    Build dataset tuples of (input_token_bits, molecular_properties, target_token_bits)\n",
    "    from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing molecular properties and token bit strings.\n",
    "        n_token_cols (int): Number of token columns in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (x_token: np.array, x_props: np.array, y_target: np.array)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract molecular properties as a numpy float32 array\n",
    "        props = [row['logP'], row['qed'], row['mw']]\n",
    "        x_props = np.array(props, dtype=np.float32)\n",
    "\n",
    "        tokens = row[3:]  # token columns after properties\n",
    "\n",
    "        # Iterate over token sequence to create input-target pairs\n",
    "        for i in range(len(tokens) - 1):\n",
    "            current_token = tokens.iloc[i]\n",
    "            next_token = tokens.iloc[i + 1]\n",
    "\n",
    "            # Skip missing or NaN tokens\n",
    "            if current_token is None or (isinstance(current_token, float) and math.isnan(current_token)):\n",
    "                continue\n",
    "            if next_token is None or (isinstance(next_token, float) and math.isnan(next_token)):\n",
    "                continue\n",
    "\n",
    "            # Convert token strings (e.g., '01011') to bit arrays\n",
    "            x_token = bitstr_to_array(current_token)\n",
    "            y_target = bitstr_to_array(next_token)\n",
    "\n",
    "            dataset.append((x_token, x_props, y_target))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "token_cols = [f\"token_{i}\" for i in range(n_tokens)]\n",
    "df = pd.read_csv(\"structured_data_selfies.csv\", dtype={col: str for col in token_cols})\n",
    "dataset = build_training_data(df)  # Should return list/array of (x_token, x_props, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_index(bits):\n",
    "    powers = 2 ** jnp.arange(len(bits) - 1, -1, -1)\n",
    "    return jnp.dot(bits, powers).astype(jnp.int32)\n",
    "\n",
    "def categorical_crossentropy(pred_probs, target_index):\n",
    "    epsilon = 1e-10\n",
    "    return -jnp.log(pred_probs[target_index] + epsilon)\n",
    "\n",
    "def label_smoothing_crossentropy_normalized(pred_probs, target_index, epsilon=0.1):\n",
    "    \"\"\"Cross-entropy loss with label smoothing, normalized to [0,1].\"\"\"\n",
    "    num_classes = pred_probs.shape[0] # Number of classes (tokens)\n",
    "    \n",
    "    # Build smoothed target\n",
    "    smooth_target = jnp.full_like(pred_probs, epsilon / (num_classes - 1))\n",
    "    smooth_target = smooth_target.at[target_index].set(1.0 - epsilon)\n",
    "    \n",
    "    # Compute cross-entropy\n",
    "    loss = -jnp.sum(smooth_target * jnp.log(pred_probs + 1e-10))\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    max_loss = jnp.log(num_classes)  # worst-case (uniform distribution)\n",
    "    norm_loss = loss / max_loss\n",
    "    \n",
    "    return norm_loss\n",
    "\n",
    "\n",
    "def compute_accuracy(pred_probs, target_index):\n",
    "    predicted_index = jnp.argmax(pred_probs)\n",
    "    return jnp.array(predicted_index == target_index, dtype=jnp.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token embedding\n",
    "key = jax.random.PRNGKey(42)\n",
    "EMBEDDING_SIZE = BITS_PER_TOKEN + n_ancillas      # size of embeddings\n",
    "key, k_emb = jax.random.split(key)\n",
    "embedding_table = jax.random.normal(k_emb, (VOCABULARY_SIZE, EMBEDDING_SIZE)) * 0.1\n",
    "\n",
    "# Projection matrices\n",
    "key = jax.random.PRNGKey(42)\n",
    "proj_dim = BITS_PER_TOKEN  # number of qubits for quantum attention\n",
    "key, k_WQ, k_WK, k_WV = jax.random.split(key, 4)\n",
    "W_Q = jax.random.normal(k_WQ, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_K = jax.random.normal(k_WK, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_V = jax.random.normal(k_WV, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "\n",
    "\n",
    "# Effective qubit counts in variational layers\n",
    "n_token_ancilla = BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "# Initialize theta and sigma params\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, k_theta, k_theta_prop, k_sigma = jax.random.split(key, 4)\n",
    "\n",
    "# Precompute Z-string combos once\n",
    "token_ancilla_ws = token_wires + ancilla_wires\n",
    "combos = zstring_combos(token_ancilla_ws)\n",
    "n_strings = len(combos)\n",
    "\n",
    "# Combine all trainable parameters into a single dictionary\n",
    "combined_params = {\n",
    "    'theta': jax.random.normal(k_theta, (N_LAYERS, n_token_ancilla, 3)) * 0.1,\n",
    "    'theta_prop': jax.random.normal(k_theta_prop, (N_LAYERS, n_prop_qubits, n_token_ancilla, 4)) * 0.1,\n",
    "    'sigma': jax.random.normal(k_sigma, (N_LAYERS, n_strings)) * 0.1,\n",
    "    'embedding_table': embedding_table,\n",
    "    'W_Q': W_Q,\n",
    "    'W_K': W_K,\n",
    "    'W_V': W_V\n",
    "}\n",
    "# Training hyperparams\n",
    "learning_rate = 0.001\n",
    "n_epochs = 500\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(combined_params)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def training_step(params, opt_state, x_token, x_props, y_target, past_token_indices=None):\n",
    "    if past_token_indices is None:\n",
    "        past_token_indices = []\n",
    "\n",
    "    def loss_fn(params):\n",
    "        theta_params = params['theta']\n",
    "        theta_prop = params['theta_prop']\n",
    "        sigma_params = params['sigma']\n",
    "        embedding_table = params['embedding_table']\n",
    "        W_Q = params['W_Q']\n",
    "        W_K = params['W_K']\n",
    "        W_V = params['W_V']\n",
    "\n",
    "        # --- 1. Embedding lookup ---\n",
    "        #start = time.time()\n",
    "        token_index = bits_to_index(x_token)\n",
    "        x_i = embedding_table[token_index]\n",
    "        #print(\"Embedding lookup:\", time.time() - start, \"seconds\")\n",
    "        \n",
    "        # Positional encoding (sin/cos)\n",
    "        position = len(past_token_indices)   # current position in the sequence\n",
    "        dim_indices = jnp.arange(EMBEDDING_SIZE)\n",
    "        pos_enc = jnp.where(\n",
    "            dim_indices % 2 == 0,\n",
    "            jnp.sin(position / (10000 ** (dim_indices / EMBEDDING_SIZE))),\n",
    "            jnp.cos(position / (10000 ** ((dim_indices-1) / EMBEDDING_SIZE)))\n",
    "        )\n",
    "\n",
    "        # Combine token embedding + positional encoding\n",
    "        x_i_pos = x_i + pos_enc\n",
    "        # --- 2. Q/K/V projections ---\n",
    "        #start = time.time()\n",
    "        Q_i = x_i_pos @ W_Q\n",
    "        \n",
    "        if len(past_token_indices) == 0:\n",
    "            # No past tokens: use V projection of current token as output\n",
    "            output_i = x_i_pos @ W_V\n",
    "        else:\n",
    "            # Normal attention over past tokens\n",
    "            past_embeddings = [embedding_table[idx] for idx in past_token_indices]\n",
    "            K_vecs = [x @ W_K for x in past_embeddings]\n",
    "            V_vecs = [x @ W_V for x in past_embeddings]\n",
    "            # --- 3. Quantum attention ---\n",
    "            # Use classical attention during training (differentiable)\n",
    "            output_i = classical_attention(Q_i, K_vecs, V_vecs)\n",
    "        #print(\"Attention:\", time.time() - start, \"seconds\")\n",
    "\n",
    "        # --- 4. Variational model ---\n",
    "        # Before calling autoregressive_model, make output non-traced\n",
    "        #start = time.time()\n",
    "        pred_probs = autoregressive_model(x_token, x_props, theta_params, theta_prop, sigma_params, output_i)\n",
    "        #print(\"Variational model:\", time.time() - start, \"seconds\")\n",
    "\n",
    "        target_index = bits_to_index(y_target)\n",
    "        # Return scalar loss for gradient computation\n",
    "        return label_smoothing_crossentropy_normalized(pred_probs, target_index), pred_probs\n",
    "\n",
    "    # value_and_grad computes both loss and grads in one pass\n",
    "    (loss, pred_probs), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    # Update parameters\n",
    "    #start = time.time()\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    #print(\"Update params:\", time.time() - start, \"seconds\")\n",
    "    \n",
    "    # Accuracy from pred_probs (already computed!)\n",
    "    target_index = bits_to_index(y_target)\n",
    "    acc = compute_accuracy(pred_probs, target_index)\n",
    "\n",
    "    return new_params, loss, opt_state, grads, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.complex128'> requested in astype is not available, and will be truncated to dtype complex64. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 0.7100 | Accuracy = 0.3118\n",
      "Epoch 2 | Loss = 0.6382 | Accuracy = 0.3794\n",
      "Epoch 3 | Loss = 0.6229 | Accuracy = 0.3989\n",
      "Epoch 4 | Loss = 0.6138 | Accuracy = 0.4155\n",
      "Epoch 5 | Loss = 0.6053 | Accuracy = 0.4214\n",
      "Epoch 6 | Loss = 0.5998 | Accuracy = 0.4296\n",
      "Epoch 7 | Loss = 0.5975 | Accuracy = 0.4334\n",
      "Epoch 8 | Loss = 0.5948 | Accuracy = 0.4306\n",
      "Epoch 9 | Loss = 0.5911 | Accuracy = 0.4319\n",
      "Epoch 10 | Loss = 0.5907 | Accuracy = 0.4295\n",
      "Epoch 11 | Loss = 0.5869 | Accuracy = 0.4289\n",
      "Epoch 12 | Loss = 0.5904 | Accuracy = 0.4265\n",
      "Epoch 13 | Loss = 0.5851 | Accuracy = 0.4345\n",
      "Epoch 14 | Loss = 0.5855 | Accuracy = 0.4356\n",
      "Epoch 15 | Loss = 0.5811 | Accuracy = 0.4350\n",
      "Epoch 16 | Loss = 0.5824 | Accuracy = 0.4338\n",
      "Epoch 17 | Loss = 0.5797 | Accuracy = 0.4396\n",
      "Epoch 18 | Loss = 0.5785 | Accuracy = 0.4408\n",
      "Epoch 19 | Loss = 0.5792 | Accuracy = 0.4397\n",
      "Epoch 20 | Loss = 0.5782 | Accuracy = 0.4413\n",
      "Epoch 21 | Loss = 0.5799 | Accuracy = 0.4382\n",
      "Epoch 22 | Loss = 0.5769 | Accuracy = 0.4388\n",
      "Epoch 23 | Loss = 0.5748 | Accuracy = 0.4400\n",
      "Epoch 24 | Loss = 0.5745 | Accuracy = 0.4410\n",
      "Epoch 25 | Loss = 0.5753 | Accuracy = 0.4411\n",
      "Epoch 26 | Loss = 0.5736 | Accuracy = 0.4444\n",
      "Epoch 27 | Loss = 0.5714 | Accuracy = 0.4434\n",
      "Epoch 28 | Loss = 0.5720 | Accuracy = 0.4413\n",
      "Epoch 29 | Loss = 0.5697 | Accuracy = 0.4426\n",
      "Epoch 30 | Loss = 0.5685 | Accuracy = 0.4428\n",
      "Epoch 31 | Loss = 0.5674 | Accuracy = 0.4456\n",
      "Epoch 32 | Loss = 0.5664 | Accuracy = 0.4453\n",
      "Epoch 33 | Loss = 0.5681 | Accuracy = 0.4441\n",
      "Epoch 34 | Loss = 0.5647 | Accuracy = 0.4539\n",
      "Epoch 35 | Loss = 0.5635 | Accuracy = 0.4545\n",
      "Epoch 36 | Loss = 0.5637 | Accuracy = 0.4544\n",
      "Epoch 37 | Loss = 0.5629 | Accuracy = 0.4544\n",
      "Epoch 38 | Loss = 0.5618 | Accuracy = 0.4531\n",
      "Epoch 39 | Loss = 0.5591 | Accuracy = 0.4563\n",
      "Epoch 40 | Loss = 0.5578 | Accuracy = 0.4552\n",
      "Epoch 41 | Loss = 0.5569 | Accuracy = 0.4588\n",
      "Epoch 42 | Loss = 0.5578 | Accuracy = 0.4588\n",
      "Epoch 43 | Loss = 0.5587 | Accuracy = 0.4590\n",
      "Epoch 44 | Loss = 0.5574 | Accuracy = 0.4576\n",
      "Epoch 45 | Loss = 0.5577 | Accuracy = 0.4578\n",
      "Epoch 46 | Loss = 0.5559 | Accuracy = 0.4589\n",
      "Epoch 47 | Loss = 0.5546 | Accuracy = 0.4649\n",
      "Epoch 48 | Loss = 0.5556 | Accuracy = 0.4600\n",
      "Epoch 49 | Loss = 0.5569 | Accuracy = 0.4618\n",
      "Epoch 50 | Loss = 0.5576 | Accuracy = 0.4649\n",
      "Epoch 51 | Loss = 0.5545 | Accuracy = 0.4670\n",
      "Epoch 52 | Loss = 0.5548 | Accuracy = 0.4655\n",
      "Epoch 53 | Loss = 0.5561 | Accuracy = 0.4647\n",
      "Epoch 54 | Loss = 0.5539 | Accuracy = 0.4665\n",
      "Epoch 55 | Loss = 0.5542 | Accuracy = 0.4691\n",
      "Epoch 56 | Loss = 0.5543 | Accuracy = 0.4656\n",
      "Epoch 57 | Loss = 0.5532 | Accuracy = 0.4645\n",
      "Epoch 58 | Loss = 0.5531 | Accuracy = 0.4659\n",
      "Epoch 59 | Loss = 0.5528 | Accuracy = 0.4657\n",
      "Epoch 60 | Loss = 0.5506 | Accuracy = 0.4697\n",
      "Epoch 61 | Loss = 0.5525 | Accuracy = 0.4684\n",
      "Epoch 62 | Loss = 0.5506 | Accuracy = 0.4683\n",
      "Epoch 63 | Loss = 0.5490 | Accuracy = 0.4725\n",
      "Epoch 64 | Loss = 0.5500 | Accuracy = 0.4680\n",
      "Epoch 65 | Loss = 0.5492 | Accuracy = 0.4717\n",
      "Epoch 66 | Loss = 0.5498 | Accuracy = 0.4719\n",
      "Epoch 67 | Loss = 0.5488 | Accuracy = 0.4713\n",
      "Epoch 68 | Loss = 0.5502 | Accuracy = 0.4699\n",
      "Epoch 69 | Loss = 0.5506 | Accuracy = 0.4688\n",
      "Epoch 70 | Loss = 0.5493 | Accuracy = 0.4674\n",
      "Epoch 71 | Loss = 0.5478 | Accuracy = 0.4732\n",
      "Epoch 72 | Loss = 0.5483 | Accuracy = 0.4742\n",
      "Epoch 73 | Loss = 0.5530 | Accuracy = 0.4678\n",
      "Epoch 74 | Loss = 0.5516 | Accuracy = 0.4682\n",
      "Epoch 75 | Loss = 0.5510 | Accuracy = 0.4715\n",
      "Epoch 76 | Loss = 0.5550 | Accuracy = 0.4684\n",
      "Epoch 77 | Loss = 0.5490 | Accuracy = 0.4731\n",
      "Epoch 78 | Loss = 0.5485 | Accuracy = 0.4723\n",
      "Epoch 79 | Loss = 0.5465 | Accuracy = 0.4749\n",
      "Epoch 80 | Loss = 0.5471 | Accuracy = 0.4748\n",
      "Epoch 81 | Loss = 0.5467 | Accuracy = 0.4719\n",
      "Epoch 82 | Loss = 0.5470 | Accuracy = 0.4781\n",
      "Epoch 83 | Loss = 0.5463 | Accuracy = 0.4725\n",
      "Epoch 84 | Loss = 0.5451 | Accuracy = 0.4748\n",
      "Epoch 85 | Loss = 0.5462 | Accuracy = 0.4716\n",
      "Epoch 86 | Loss = 0.5472 | Accuracy = 0.4715\n",
      "Epoch 87 | Loss = 0.5484 | Accuracy = 0.4725\n",
      "Epoch 88 | Loss = 0.5468 | Accuracy = 0.4742\n",
      "Epoch 89 | Loss = 0.5456 | Accuracy = 0.4766\n",
      "Epoch 90 | Loss = 0.5422 | Accuracy = 0.4813\n",
      "Epoch 91 | Loss = 0.5422 | Accuracy = 0.4770\n",
      "Epoch 92 | Loss = 0.5424 | Accuracy = 0.4797\n",
      "Epoch 93 | Loss = 0.5428 | Accuracy = 0.4741\n",
      "Epoch 94 | Loss = 0.5447 | Accuracy = 0.4752\n",
      "Epoch 95 | Loss = 0.5433 | Accuracy = 0.4757\n",
      "Epoch 96 | Loss = 0.5429 | Accuracy = 0.4731\n",
      "Epoch 97 | Loss = 0.5420 | Accuracy = 0.4769\n",
      "Epoch 98 | Loss = 0.5435 | Accuracy = 0.4785\n",
      "Epoch 99 | Loss = 0.5431 | Accuracy = 0.4819\n",
      "Epoch 100 | Loss = 0.5452 | Accuracy = 0.4776\n",
      "Epoch 101 | Loss = 0.5426 | Accuracy = 0.4793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m x_props \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(x_props, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m y_target \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(y_target, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mall(x_token \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# If current token is <SOS>, reset past tokens\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     past_token_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m combined_params, loss, opt_state, grads, acc \u001b[38;5;241m=\u001b[39m training_step(combined_params, opt_state, x_token, x_props, y_target, past_token_indices)\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/array.py:306\u001b[0m, in \u001b[0;36mArrayImpl.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    305\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_bool_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 306\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/profiler.py:354\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/array.py:644\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    643\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharding\u001b[38;5;241m.\u001b[39m_internal_device_list\u001b[38;5;241m.\u001b[39maddressable_device_list):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     npy_value, did_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = total_acc = 0.0\n",
    "    past_token_indices = [] # Reset past tokens at the start of each epoch\n",
    "\n",
    "    for x_token, x_props, y_target in dataset:\n",
    "        x_token = jnp.array(x_token, dtype=jnp.int32)\n",
    "        x_props = jnp.array(x_props, dtype=jnp.float32)\n",
    "        y_target = jnp.array(y_target, dtype=jnp.int32)\n",
    "\n",
    "        if jnp.all(x_token == 0):\n",
    "            # If current token is <SOS>, reset past tokens\n",
    "            past_token_indices = []\n",
    "\n",
    "        combined_params, loss, opt_state, grads, acc = training_step(combined_params, opt_state, x_token, x_props, y_target, past_token_indices)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc  += acc\n",
    "\n",
    "        past_token_indices.append(bits_to_index(x_token))\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    avg_acc  = total_acc / len(dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss = {avg_loss:.4f} | Accuracy = {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 | Loss = 1.2955 | Accuracy = 0.0841\n",
    "Epoch 2 | Loss = 0.9395 | Accuracy = 0.1449\n",
    "Epoch 3 | Loss = 0.8566 | Accuracy = 0.2056\n",
    "Epoch 4 | Loss = 0.8215 | Accuracy = 0.2150\n",
    "Epoch 5 | Loss = 0.8026 | Accuracy = 0.2196\n",
    "Epoch 6 | Loss = 0.7900 | Accuracy = 0.2430\n",
    "Epoch 7 | Loss = 0.7800 | Accuracy = 0.2430\n",
    "Epoch 8 | Loss = 0.7710 | Accuracy = 0.2383\n",
    "Epoch 9 | Loss = 0.7621 | Accuracy = 0.2570\n",
    "Epoch 10 | Loss = 0.7531 | Accuracy = 0.2664\n",
    "Epoch 11 | Loss = 0.7447 | Accuracy = 0.2804\n",
    "Epoch 12 | Loss = 0.7367 | Accuracy = 0.2944\n",
    "Epoch 13 | Loss = 0.7290 | Accuracy = 0.3318\n",
    "Epoch 14 | Loss = 0.7221 | Accuracy = 0.3458\n",
    "Epoch 15 | Loss = 0.7160 | Accuracy = 0.3738\n",
    "Epoch 16 | Loss = 0.7108 | Accuracy = 0.3925\n",
    "Epoch 17 | Loss = 0.7063 | Accuracy = 0.3785\n",
    "Epoch 18 | Loss = 0.7022 | Accuracy = 0.3879\n",
    "Epoch 19 | Loss = 0.6984 | Accuracy = 0.3972\n",
    "Epoch 20 | Loss = 0.6950 | Accuracy = 0.4065\n",
    "Epoch 21 | Loss = 0.6917 | Accuracy = 0.4112\n",
    "Epoch 22 | Loss = 0.6885 | Accuracy = 0.4065\n",
    "Epoch 23 | Loss = 0.6854 | Accuracy = 0.4065\n",
    "Epoch 24 | Loss = 0.6824 | Accuracy = 0.4112\n",
    "Epoch 25 | Loss = 0.6795 | Accuracy = 0.4112\n",
    "...\n",
    "Epoch 97 | Loss = 0.5662 | Accuracy = 0.6308\n",
    "Epoch 98 | Loss = 0.5655 | Accuracy = 0.6308\n",
    "Epoch 99 | Loss = 0.5647 | Accuracy = 0.6355\n",
    "Epoch 100 | Loss = 0.5640 | Accuracy = 0.6355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Without JAX JIT for circuit representation and debugging\\ndef training_step(params, opt_state, x_token, x_props, y_target):\\n    def loss_fn(params):\\n        theta_params = params[\\'theta\\']\\n        theta_prop = params[\\'theta_prop\\']\\n        sigma_params = params[\\'sigma\\']\\n\\n        # Predict using theta_effective and sigma_params\\n        pred_probs = autoregressive_model(x_token, x_props, theta_effective, theta_prop, sigma_params)\\n        index = bits_to_index(y_target)\\n        return categorical_crossentropy(pred_probs, index)\\n\\n    grads = jax.grad(loss_fn)(params)\\n    updates, opt_state = optimizer.update(grads, opt_state, params)\\n    new_params = optax.apply_updates(params, updates)\\n    print(\"\\nQuantum Circuit:\")\\n    print(qml.draw(autoregressive_model)(\\n        x_token, x_props, \\n        theta_effective, \\n        new_params[\\'theta_prop\\'], \\n        new_params[\\'sigma\\']\\n    ))    #print(\"Target index:\", int(jax.device_get(bits_to_index(y_target))))\\n    loss = loss_fn(new_params)\\n    return new_params, loss, opt_state, grads'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''# Without JAX JIT for circuit representation and debugging\n",
    "def training_step(params, opt_state, x_token, x_props, y_target):\n",
    "    def loss_fn(params):\n",
    "        theta_params = params['theta']\n",
    "        theta_prop = params['theta_prop']\n",
    "        sigma_params = params['sigma']\n",
    "\n",
    "        # Predict using theta_effective and sigma_params\n",
    "        pred_probs = autoregressive_model(x_token, x_props, theta_effective, theta_prop, sigma_params)\n",
    "        index = bits_to_index(y_target)\n",
    "        return categorical_crossentropy(pred_probs, index)\n",
    "\n",
    "    grads = jax.grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    print(\"\\nQuantum Circuit:\")\n",
    "    print(qml.draw(autoregressive_model)(\n",
    "        x_token, x_props, \n",
    "        theta_effective, \n",
    "        new_params['theta_prop'], \n",
    "        new_params['sigma']\n",
    "    ))    #print(\"Target index:\", int(jax.device_get(bits_to_index(y_target))))\n",
    "    loss = loss_fn(new_params)\n",
    "    return new_params, loss, opt_state, grads'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_layers = 6\n",
    "h_local = 3\n",
    "prob_mask = NO MASK\n",
    "\n",
    "Epoch 1 | Loss = 0.6208 | Accuracy = 0.4185\n",
    "Epoch 2 | Loss = 0.5437 | Accuracy = 0.4909\n",
    "Epoch 3 | Loss = 0.5238 | Accuracy = 0.5092\n",
    "Epoch 4 | Loss = 0.5112 | Accuracy = 0.5222\n",
    "Epoch 5 | Loss = 0.5034 | Accuracy = 0.5330\n",
    "Epoch 6 | Loss = 0.5005 | Accuracy = 0.5372\n",
    "Epoch 7 | Loss = 0.4984 | Accuracy = 0.5426\n",
    "Epoch 8 | Loss = 0.4958 | Accuracy = 0.5470\n",
    "Epoch 9 | Loss = 0.4954 | Accuracy = 0.5492\n",
    "Epoch 10 | Loss = 0.4933 | Accuracy = 0.5497\n",
    "Epoch 11 | Loss = 0.4887 | Accuracy = 0.5585\n",
    "Epoch 12 | Loss = 0.4865 | Accuracy = 0.5625\n",
    "Epoch 13 | Loss = 0.4855 | Accuracy = 0.5594\n",
    "Epoch 14 | Loss = 0.4844 | Accuracy = 0.5634\n",
    "Epoch 15 | Loss = 0.4821 | Accuracy = 0.5645\n",
    "Epoch 16 | Loss = 0.4787 | Accuracy = 0.5690\n",
    "Epoch 17 | Loss = 0.4776 | Accuracy = 0.5752\n",
    "Epoch 18 | Loss = 0.4783 | Accuracy = 0.5689\n",
    "Epoch 19 | Loss = 0.4774 | Accuracy = 0.5783\n",
    "Epoch 20 | Loss = 0.4756 | Accuracy = 0.5784\n",
    "Epoch 21 | Loss = 0.4753 | Accuracy = 0.5751\n",
    "Epoch 22 | Loss = 0.4741 | Accuracy = 0.5789\n",
    "Epoch 23 | Loss = 0.4762 | Accuracy = 0.5767\n",
    "Epoch 24 | Loss = 0.4747 | Accuracy = 0.5771\n",
    "Epoch 25 | Loss = 0.4732 | Accuracy = 0.5787\n",
    "Epoch 26 | Loss = 0.4715 | Accuracy = 0.5852\n",
    "Epoch 27 | Loss = 0.4725 | Accuracy = 0.5805\n",
    "Epoch 28 | Loss = 0.4746 | Accuracy = 0.5792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h=2\n",
    "\n",
    "Epoch 1 | Loss = 2.6945 | Accuracy = 0.4050 \n",
    "Epoch 2 | Loss = 2.4173 | Accuracy = 0.4816 \n",
    "Epoch 3 | Loss = 2.3229 | Accuracy = 0.5047 \n",
    "Epoch 4 | Loss = 2.2768 | Accuracy = 0.5220\n",
    "Epoch 5 | Loss = 2.2505 | Accuracy = 0.5267 \n",
    "Epoch 6 | Loss = 2.2214 | Accuracy = 0.5399 \n",
    "Epoch 7 | Loss = 2.2051 | Accuracy = 0.5448 \n",
    "Epoch 8 | Loss = 2.1918 | Accuracy = 0.5523 \n",
    "Epoch 9 | Loss = 2.1790 | Accuracy = 0.5544 \n",
    "Epoch 10 | Loss = 2.1683 | Accuracy = 0.5577 \n",
    "Epoch 11 | Loss = 2.1654 | Accuracy = 0.5582 \n",
    "Epoch 12 | Loss = 2.1519 | Accuracy = 0.5617 \n",
    "Epoch 13 | Loss = 2.1452 | Accuracy = 0.5647 \n",
    "Epoch 14 | Loss = 2.1371 | Accuracy = 0.5670 \n",
    "Epoch 15 | Loss = 2.1380 | Accuracy = 0.5630 \n",
    "Epoch 16 | Loss = 2.1330 | Accuracy = 0.5654 \n",
    "Epoch 17 | Loss = 2.1251 | Accuracy = 0.5661 \n",
    "Epoch 18 | Loss = 2.1189 | Accuracy = 0.5703 \n",
    "Epoch 19 | Loss = 2.1176 | Accuracy = 0.5718 \n",
    "Epoch 20 | Loss = 2.1152 | Accuracy = 0.5698 \n",
    "Epoch 21 | Loss = 2.1162 | Accuracy = 0.5682 \n",
    "Epoch 22 | Loss = 2.1117 | Accuracy = 0.5704 \n",
    "Epoch 23 | Loss = 2.1126 | Accuracy = 0.5729 \n",
    "Epoch 24 | Loss = 2.1097 | Accuracy = 0.5735 \n",
    "Epoch 25 | Loss = 2.1040 | Accuracy = 0.5756 \n",
    "Epoch 26 | Loss = 2.1001 | Accuracy = 0.5751 \n",
    "Epoch 27 | Loss = 2.0959 | Accuracy = 0.5794 \n",
    "Epoch 28 | Loss = 2.0881 | Accuracy = 0.5808 \n",
    "Epoch 29 | Loss = 2.0891 | Accuracy = 0.5806 \n",
    "Epoch 30 | Loss = 2.0869 | Accuracy = 0.5808 \n",
    "Epoch 31 | Loss = 2.0843 | Accuracy = 0.5855 \n",
    "Epoch 32 | Loss = 2.0859 | Accuracy = 0.5839 \n",
    "Epoch 33 | Loss = 2.0841 | Accuracy = 0.5830"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
