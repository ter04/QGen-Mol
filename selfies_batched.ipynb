{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import selfies as sf\n",
    "\n",
    "import numpy as np\n",
    "from math import ceil, log2\n",
    "import re\n",
    "import pandas as pd\n",
    "import optax\n",
    "import csv\n",
    "import flax.linen as nn\n",
    "import math\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import qchem\n",
    "from pennylane.templates import StronglyEntanglingLayers\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn.initializers import normal\n",
    "\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training molecules set:  65778\n"
     ]
    }
   ],
   "source": [
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "# Using the ChEMBL API to get the molecules dataset\n",
    "\n",
    "molecule = new_client.molecule\n",
    "\n",
    "# Filter for drug-like small molecules interesting for human use\n",
    "druglike_molecules = molecule.filter(\n",
    "    molecule_properties__heavy_atoms__lte=15,           # Heavy atoms less than 20\n",
    "    molecule_properties__alogp__lte=5,                  # LogP less than 5 (Lipophilicity and membrane permeability)\n",
    "    molecule_properties__mw_freebase__lte=300,          # Molecular weight less than 300 g/mol\n",
    "    molecule_properties__qed_weighted__gte=0.5,         # QED weighted greater than 0.5 (Drug-likeness)\n",
    "    molecule_properties__num_ro5_violations__lte=1,     # At most 1 Rule of 5 violation (Drug-likeness filter)\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Training molecules set: \", len(druglike_molecules))  # Check how many molecules match the filter criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the alphabet used for the SMILEs representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the alphabet considering the structure of some special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subset of molecules we are going to train the model with\n",
    "molecules_subset = druglike_molecules[:600]\n",
    "\n",
    "max_len = 0\n",
    "alphabet = set()\n",
    "# We use a subset of molecules to build the alphabet\n",
    "for mol in molecules_subset:  # Limiting to 1000 molecules for performance\n",
    "    smiles = mol.get('molecule_structures', {}).get('canonical_smiles')\n",
    "    selfies = sf.encoder(smiles)\n",
    "    if selfies:\n",
    "        # Skip if contains '.'\n",
    "        if \".\" in selfies:\n",
    "            continue\n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if max_len < len(tokens):\n",
    "            max_len = len(tokens)\n",
    "        alphabet.update(tokens)\n",
    "\n",
    "alphabet = sorted(alphabet)\n",
    "alphabet = ['<SOS>'] + alphabet  + ['<EOS>'] # Add Start-of-Secuence, End-of-secuence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet of SMILES characters: ['<SOS>', '[#Branch1]', '[#Branch2]', '[#C]', '[#N]', '[/C]', '[/Cl]', '[/N]', '[/S]', '[=Branch1]', '[=Branch2]', '[=C]', '[=N+1]', '[=N]', '[=O]', '[=P]', '[=Ring1]', '[=S]', '[Br]', '[Branch1]', '[Branch2]', '[C@@H1]', '[C@@]', '[C@H1]', '[C@]', '[C]', '[Cl]', '[F]', '[I]', '[N+1]', '[NH1]', '[N]', '[O-1]', '[O]', '[PH1]', '[P]', '[Ring1]', '[Ring2]', '[S+1]', '[S]', '[\\\\C]', '[\\\\Cl]', '[\\\\N]', '<EOS>']\n",
      "Total unique characters in SMILES: 44\n",
      "Maximum length of SMILES in dataset: 29\n",
      "Bits per token: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Alphabet of SMILES characters:\", alphabet)\n",
    "\n",
    "VOCABULARY_SIZE = len(alphabet)\n",
    "BITS_PER_TOKEN = ceil(log2(VOCABULARY_SIZE))  # n bits por token\n",
    "\n",
    "print(\"Total unique characters in SMILES:\", VOCABULARY_SIZE)\n",
    "print(\"Maximum length of SMILES in dataset:\", max_len)\n",
    "print(\"Bits per token:\", BITS_PER_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tSímbolos de enlaces y paréntesis: #, (, ), /, \\, =\n",
    "\n",
    "•\tDígitos simples para cierres de anillos: '1', '2', '3', '4', '5'\n",
    "\n",
    "•\tÁtomos orgánicos y halógenos comunes, tanto mayúsculas (alifáticos) como minúsculas (aromáticos)\n",
    "\n",
    "•\tTokens entre corchetes para isótopos, estados de carga, quiralidad, etc.\n",
    "\n",
    "•\tUn token especial '<PAD>' para padding en modelos ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<SOS>' → index 0 → 000000\n",
      "'[#Branch1]' → index 1 → 000001\n",
      "'[#Branch2]' → index 2 → 000010\n",
      "'[#C]' → index 3 → 000011\n",
      "'[#N]' → index 4 → 000100\n",
      "'[/C]' → index 5 → 000101\n",
      "'[/Cl]' → index 6 → 000110\n",
      "'[/N]' → index 7 → 000111\n",
      "'[/S]' → index 8 → 001000\n",
      "'[=Branch1]' → index 9 → 001001\n",
      "'[=Branch2]' → index 10 → 001010\n",
      "'[=C]' → index 11 → 001011\n",
      "'[=N+1]' → index 12 → 001100\n",
      "'[=N]' → index 13 → 001101\n",
      "'[=O]' → index 14 → 001110\n",
      "'[=P]' → index 15 → 001111\n",
      "'[=Ring1]' → index 16 → 010000\n",
      "'[=S]' → index 17 → 010001\n",
      "'[Br]' → index 18 → 010010\n",
      "'[Branch1]' → index 19 → 010011\n",
      "'[Branch2]' → index 20 → 010100\n",
      "'[C@@H1]' → index 21 → 010101\n",
      "'[C@@]' → index 22 → 010110\n",
      "'[C@H1]' → index 23 → 010111\n",
      "'[C@]' → index 24 → 011000\n",
      "'[C]' → index 25 → 011001\n",
      "'[Cl]' → index 26 → 011010\n",
      "'[F]' → index 27 → 011011\n",
      "'[I]' → index 28 → 011100\n",
      "'[N+1]' → index 29 → 011101\n",
      "'[NH1]' → index 30 → 011110\n",
      "'[N]' → index 31 → 011111\n",
      "'[O-1]' → index 32 → 100000\n",
      "'[O]' → index 33 → 100001\n",
      "'[PH1]' → index 34 → 100010\n",
      "'[P]' → index 35 → 100011\n",
      "'[Ring1]' → index 36 → 100100\n",
      "'[Ring2]' → index 37 → 100101\n",
      "'[S+1]' → index 38 → 100110\n",
      "'[S]' → index 39 → 100111\n",
      "'[\\C]' → index 40 → 101000\n",
      "'[\\Cl]' → index 41 → 101001\n",
      "'[\\N]' → index 42 → 101010\n",
      "'<EOS>' → index 43 → 101011\n"
     ]
    }
   ],
   "source": [
    "# Diccionario token → índice\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def print_token_bits(tokens, token_to_index):\n",
    "    for tok in tokens:\n",
    "        idx = token_to_index.get(tok, None)\n",
    "        if idx is None:\n",
    "            print(f\"Token '{tok}' no está en el diccionario.\")\n",
    "            continue\n",
    "        binary = format(idx, f'0{BITS_PER_TOKEN}b')\n",
    "        print(f\"'{tok}' → index {idx} → {binary}\")\n",
    "\n",
    "print_token_bits(alphabet, token_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input SMILEs must be the same size, so we need to use the padding to make it uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_encoded_dataset = []\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def smiles_to_bits(tokens: list) -> np.ndarray:\n",
    "    \"\"\"Convert tokens to a 2D array\"\"\"\n",
    "    padded_tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "    bit_matrix = []\n",
    "    for tok in padded_tokens:\n",
    "        idx = token_to_index[tok]\n",
    "        bits = list(f\"{idx:0{BITS_PER_TOKEN}b}\")  # length of the binary string depends on the number of bits required to represent the alphabet\n",
    "        bit_matrix.append([int(b) for b in bits])\n",
    "    return np.array(bit_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain the molecular properties of interest\n",
    "\n",
    "logP (o cx_logp) -> Coeficiente de partición octanol/agua (lipofilia)\n",
    "\n",
    "QED (quantitative estimate of drug-likeness) -> Escala combinada que evalúa qué tan “drug-like” es una molécula\n",
    "\n",
    "SAS (Synthetic Accessibility Score) -> Qué tan difícil sería sintetizar la molécula en laboratorio * Needs to be calculated separately!!\n",
    "\n",
    "MW (peso molecular) -> Masa total, típicamente ≤ 500 Da para buenos fármacos orales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the max and min range values in order to later normalize the properties in the range (0, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP range: -1.5 to 3.89\n",
      "QED range: 0.5 to 0.92\n",
      "MW range: 107.11 to 298.11\n"
     ]
    }
   ],
   "source": [
    "min_logp = float('inf')\n",
    "max_logp = float('-inf')\n",
    "min_qed = float('inf')\n",
    "max_qed = float('-inf')\n",
    "min_mw = float('inf')\n",
    "max_mw = float('-inf')\n",
    "\n",
    "\n",
    "# Iterate through the subset of molecules to find min/max properties to normalize them\n",
    "for mol in molecules_subset:\n",
    "    logP = mol.get('molecule_properties', {}).get('alogp')\n",
    "    qed = mol.get('molecule_properties', {}).get('qed_weighted')\n",
    "    mw = mol.get('molecule_properties', {}).get('mw_freebase')\n",
    "\n",
    "    if logP is None or qed is None or mw is None:\n",
    "        continue  # Skip if any property is missing\n",
    "\n",
    "    logP = float(logP)\n",
    "    qed = float(qed)\n",
    "    mw = float(mw)\n",
    "\n",
    "    if logP < min_logp:\n",
    "        min_logp = logP\n",
    "    if logP > max_logp:\n",
    "        max_logp = logP\n",
    "\n",
    "    if qed < min_qed:\n",
    "        min_qed = qed\n",
    "    if qed > max_qed:\n",
    "        max_qed = qed\n",
    "\n",
    "    if mw < min_mw:\n",
    "        min_mw = mw\n",
    "    if mw > max_mw:\n",
    "        max_mw = mw\n",
    "\n",
    "print(f\"LogP range: {min_logp} to {max_logp}\")\n",
    "print(f\"QED range: {min_qed} to {max_qed}\")\n",
    "print(f\"MW range: {min_mw} to {max_mw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_val, max_val, target_max=np.pi):\n",
    "    ''' Normalize a value to a range [0, [0, pi] to later encode them as rotation angles'''\n",
    "    norm = (value - min_val) / (max_val - min_val) * target_max\n",
    "    return float(f\"{norm:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequences in the subset: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum length of sequences in the subset:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the structured data to a CSV file\n",
    "with open(\"structured_data_selfies.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    n_tokens = max_len + 2  # +2 for <SOS> and <EOS>\n",
    "    header = [\"logP\", \"qed\", \"mw\"] + [f\"token_{i}\" for i in range(n_tokens)]\n",
    "    writer.writerow(header)\n",
    "\n",
    "\n",
    "    for mol in molecules_subset:\n",
    "        smiles = mol.get('molecule_structures', {}).get('canonical_smiles')\n",
    "        selfies = sf.encoder(smiles)\n",
    "        props = mol.get('molecule_properties', {})\n",
    "        if not selfies:\n",
    "            continue\n",
    "        if \".\" in selfies:\n",
    "            continue\n",
    "        try:\n",
    "            logP = float(props.get('alogp'))\n",
    "            qed = float(props.get('qed_weighted'))\n",
    "            mw = float(props.get('mw_freebase'))\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "\n",
    "        norm_logp = normalize(logP, min_logp, max_logp)\n",
    "        norm_qed = normalize(qed, min_qed, max_qed)\n",
    "        norm_mw = normalize(mw, min_mw, max_mw)\n",
    "\n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if not all(tok in token_to_index for tok in tokens):\n",
    "            continue\n",
    "\n",
    "        bit_matrix = smiles_to_bits(tokens)  # shape (n_tokens, 6)\n",
    "        token_bits_as_strings = [\"\".join(map(str, row)) for row in bit_matrix]\n",
    "\n",
    "        row = [norm_logp, norm_qed, norm_mw] + token_bits_as_strings\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantum Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def zstring_combos(wires):\n",
    "    \"\"\"\n",
    "    Return an ordered list of wire-tuples for all Z-strings up to order H_LOCAL.\n",
    "    Order: all 1-local, then all 2-local, ..., up to H_LOCAL.\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    for k in range(1, H_LOCAL + 1):\n",
    "        L.extend(itertools.combinations(wires, k))\n",
    "    return [tuple(c) for c in L]\n",
    "\n",
    "def num_zstrings(n_wires):\n",
    "    \"\"\"Count how many Z-strings up to order H_LOCAL.\"\"\"\n",
    "    from math import comb\n",
    "    return sum(comb(n_wires, k) for k in range(1, H_LOCAL + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Attention mechanism using SWAP test\n",
    "\n",
    "# --- Device for attention ---\n",
    "n_past = 5\n",
    "attn_dev = qml.device(\"default.qubit\", wires=(BITS_PER_TOKEN+1)*n_past)\n",
    "\n",
    "@qml.qnode(attn_dev, interface=\"jax\")\n",
    "def quantum_attention_qnode(Q_vec, K_vecs):\n",
    "    \"\"\"\n",
    "    Q_vec: projected query vector of current token\n",
    "    K_vecs: list of projected key vectors for past tokens\n",
    "    Returns: attention scores ⟨q_i | k_j⟩ for each j\n",
    "    \"\"\"\n",
    "    n_tokens = len(K_vecs)\n",
    "    q_wires = list(range(BITS_PER_TOKEN))\n",
    "\n",
    "    # Encode Q and all K in parallel (different wire registers)\n",
    "    def encode_token(angles, wires):\n",
    "        # Use AngleEmbedding for compactness, then entangle\n",
    "        qml.templates.AngleEmbedding(angles, wires=wires, rotation=\"Y\")\n",
    "        for i in range(len(wires)-1):\n",
    "            qml.CNOT(wires=[wires[i], wires[i+1]])\n",
    "\n",
    "    # Encode Q\n",
    "    encode_token(Q_vec, wires=q_wires)\n",
    "\n",
    "    # Encode K_j\n",
    "    # Collect expectation values (one per K_j)\n",
    "    measurements = []\n",
    "    for j, K_j in enumerate(K_vecs):\n",
    "        start = (BITS_PER_TOKEN+1) + j*(BITS_PER_TOKEN+1)\n",
    "        k_wires = list(range(start, start+BITS_PER_TOKEN))\n",
    "        encode_token(K_j, k_wires)\n",
    "\n",
    "        # SWAP test\n",
    "        ancilla = start + BITS_PER_TOKEN\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "        for qw, kw in zip(q_wires, k_wires):\n",
    "            qml.CSWAP(wires=[ancilla, qw, kw])\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "\n",
    "        measurements.append(qml.expval(qml.PauliZ(ancilla)))\n",
    "\n",
    "    # **Return as tuple** so PennyLane converts to JAX array\n",
    "    return tuple(measurements)\n",
    "\n",
    "\n",
    "def quantum_attention(Q_vec, K_vecs, V_vecs):\n",
    "    # Make non-traced (concrete) copies for the QNode\n",
    "    Q_safe = jax.lax.stop_gradient(Q_vec)\n",
    "    K_safe = [jax.lax.stop_gradient(k) for k in K_vecs]\n",
    "\n",
    "    raw_expvals = quantum_attention_qnode(Q_safe, K_safe)\n",
    "    raw_expvals = jnp.asarray(raw_expvals)\n",
    "    # Convert from expectation values (in [-1,1]) to probabilities [0,1]\n",
    "    overlaps = (1.0 - raw_expvals) / 2.0\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def classical_attention(Q_vec, K_vecs, V_vecs, scale=True):\n",
    "    # Q_vec: (proj_dim,) ; K_vecs: list of (proj_dim,) ; V_vecs: list of (proj_dim,)\n",
    "    if len(K_vecs) == 0:\n",
    "        return V_vecs[0] if len(V_vecs) > 0 else jnp.zeros_like(Q_vec)\n",
    "\n",
    "    K_mat = jnp.stack(K_vecs)  # shape (n_past, proj_dim)\n",
    "    # dot-product softmax (fully differentiable)\n",
    "    scores = jnp.dot(K_mat, Q_vec)  # shape (n_past,)\n",
    "    if scale:\n",
    "        scores = scores / jnp.sqrt(Q_vec.shape[0])\n",
    "    # Causal mask: only past tokens (K_vecs are already past tokens)\n",
    "    weights = jax.nn.softmax(scores)\n",
    "    # Weighted sum over classical V\n",
    "    output = jnp.sum(weights[:, None] * jnp.stack(V_vecs), axis=0)\n",
    "    return output\n",
    "\n",
    "def batched_classical_attention(Q_batch, K_batch_list, V_batch_list, scale=True):\n",
    "    \"\"\"\n",
    "    Q_batch: (batch_size, proj_dim)\n",
    "    K_batch_list: list of lists of past K vectors per batch element\n",
    "    V_batch_list: list of lists of past V vectors per batch element\n",
    "    \"\"\"\n",
    "    def attention_single(Q_vec, K_vecs, V_vecs):\n",
    "        if len(K_vecs) == 0:\n",
    "            return x_i @ W_V if len(V_vecs) > 0 else jnp.zeros_like(Q_vec)\n",
    "        K_mat = jnp.stack(K_vecs)  # shape (n_past, proj_dim)\n",
    "        scores = jnp.dot(K_mat, Q_vec)\n",
    "        if scale:\n",
    "            scores = scores / jnp.sqrt(Q_vec.shape[0])\n",
    "        weights = jax.nn.softmax(scores)\n",
    "        return jnp.sum(weights[:, None] * jnp.stack(V_vecs), axis=0)\n",
    "\n",
    "    return jax.vmap(attention_single)(Q_batch, K_batch_list, V_batch_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and qubit setup\n",
    "# BITS_PER_TOKEN number of qubits needed to encode each token\n",
    "n_prop_qubits = 3  # number of qubits needed to encode properties (logP, QED, MW)\n",
    "n_ancillas = 3  # number of ancilla qubits that represent the environment\n",
    "n_total_qubits = n_prop_qubits + BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "N_LAYERS = 6  # number of variational layers\n",
    "H_LOCAL = 3 # h_local sets the maximum number of qubits that can interact in each Z-string term of Σ\n",
    "\n",
    "\n",
    "# Name them explicitly\n",
    "prop_wires = [f\"prop_{i}\" for i in range(n_prop_qubits)]\n",
    "token_wires = [f\"token_{i}\" for i in range(BITS_PER_TOKEN)]\n",
    "ancilla_wires = [f\"ancilla_{i}\" for i in range(n_ancillas)]\n",
    "all_wires = prop_wires + token_wires + ancilla_wires\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=all_wires)\n",
    "\n",
    "\n",
    "def molecular_property_encoder(props):\n",
    "    \"\"\"Encode continuous props on property qubits via RY rotations\"\"\"\n",
    "    for wire, val in zip(prop_wires, props):\n",
    "        qml.RY(val, wires=wire)\n",
    "\n",
    "\n",
    "def token_encoder(token_bits):\n",
    "    \"\"\"Basis-encode token bits on token qubits\"\"\"\n",
    "    qml.BasisState(token_bits, wires=token_wires)\n",
    "\n",
    "\n",
    "def operator_layer(theta_params, theta_prop, wires):\n",
    "    \"\"\"\n",
    "    Variational layer where:\n",
    "      - theta_params[...] are rotations for token + ancilla qubits\n",
    "      - theta_prop encodes property→token entanglement\n",
    "    \"\"\"\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "\n",
    "    # Property → token entanglement \n",
    "    for p, prop_wire in enumerate(prop_wires):\n",
    "        for t, t_a_wire in enumerate(token_ancilla_ws):\n",
    "            qml.CRX(theta_prop[p, t, 0], wires=[prop_wire, t_a_wire])\n",
    "            qml.CRY(theta_prop[p, t, 1], wires=[prop_wire, t_a_wire])\n",
    "\n",
    " \n",
    "    qml.StronglyEntanglingLayers(\n",
    "        weights=theta_params[None,:,:],  # shape: (n_token_ancilla, 3)\n",
    "        wires=token_ancilla_ws\n",
    "    )\n",
    "\n",
    "def Sigma_layer_vec(gamma_vec, token_ancilla_ws, time=1.0, combos=None):\n",
    "    \"\"\"\n",
    "    Diagonal multi-Z unitary Σ = exp(i * sum_s gamma_s * Z^{⊗|s|} * t)\n",
    "    using a flat parameter vector 'gamma_vec' aligned with 'combos'.\n",
    "    \"\"\"\n",
    "    #token_ancilla_ws = list(wires)  # pass token+ancilla here\n",
    "    if combos is None:\n",
    "        combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    # Safety: ensure the vector length matches the number of combos\n",
    "    assert gamma_vec.shape[0] == len(combos), \\\n",
    "        f\"gamma_vec has length {gamma_vec.shape[0]} but expected {len(combos)}\"\n",
    "\n",
    "    # MultiRZ(phi) = exp(-i * phi/2 * Z^{⊗k}); choose phi = -2 * gamma * time\n",
    "    for gamma, combo in zip(gamma_vec, combos):\n",
    "        qml.MultiRZ(-2.0 * gamma * time, wires=list(combo))\n",
    "\n",
    "\n",
    "# QNode combining encoding and variational layers\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def single_qnode(token_bits, props, theta_params, theta_prop, sigma_params, output_i):\n",
    "    molecular_property_encoder(props)\n",
    "    token_encoder(token_bits)\n",
    "\n",
    "    for i, val in enumerate(output_i):\n",
    "        qml.RY(val, wires=token_wires[i])\n",
    "\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "    combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    for l in range(N_LAYERS):\n",
    "        operator_layer(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "        Sigma_layer_vec(sigma_params[l], token_ancilla_ws, time=1.0, combos=combos)\n",
    "        qml.adjoint(operator_layer)(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "\n",
    "    return qml.probs(wires=token_wires)\n",
    "\n",
    "# Vectorize over batch\n",
    "batched_qnode = jax.vmap(\n",
    "    single_qnode,\n",
    "    in_axes=(0, 0, None, None, None, 0)  # batch token_bits, props, output_i; params are shared\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitstr_to_array(bitstr):\n",
    "    \"\"\"Convert a string of bits (e.g., '010101') to a numpy float32 array.\"\"\"\n",
    "    return np.array([int(b) for b in bitstr], dtype=np.float32)\n",
    "\n",
    "def build_training_data(df):\n",
    "    \"\"\"\n",
    "    Build dataset tuples of (input_token_bits, molecular_properties, target_token_bits)\n",
    "    from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing molecular properties and token bit strings.\n",
    "        n_token_cols (int): Number of token columns in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (x_token: np.array, x_props: np.array, y_target: np.array)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract molecular properties as a numpy float32 array\n",
    "        props = [row['logP'], row['qed'], row['mw']]\n",
    "        x_props = np.array(props, dtype=np.float32)\n",
    "\n",
    "        tokens = row[3:]  # token columns after properties\n",
    "\n",
    "        # Iterate over token sequence to create input-target pairs\n",
    "        for i in range(len(tokens) - 1):\n",
    "            current_token = tokens.iloc[i]\n",
    "            next_token = tokens.iloc[i + 1]\n",
    "\n",
    "            # Skip missing or NaN tokens\n",
    "            if current_token is None or (isinstance(current_token, float) and math.isnan(current_token)):\n",
    "                continue\n",
    "            if next_token is None or (isinstance(next_token, float) and math.isnan(next_token)):\n",
    "                continue\n",
    "\n",
    "            # Convert token strings (e.g., '01011') to bit arrays\n",
    "            x_token = bitstr_to_array(current_token)\n",
    "            y_target = bitstr_to_array(next_token)\n",
    "\n",
    "            dataset.append((x_token, x_props, y_target))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "token_cols = [f\"token_{i}\" for i in range(n_tokens)]\n",
    "df = pd.read_csv(\"structured_data_selfies.csv\", dtype={col: str for col in token_cols})\n",
    "dataset = build_training_data(df)  # Should return list/array of (x_token, x_props, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_index(bits):\n",
    "    powers = 2 ** jnp.arange(BITS_PER_TOKEN - 1, -1, -1)\n",
    "    return jnp.dot(bits, powers).astype(jnp.int32)\n",
    "\n",
    "def categorical_crossentropy(pred_probs, target_index):\n",
    "    epsilon = 1e-10\n",
    "    return -jnp.log(pred_probs[target_index] + epsilon)\n",
    "\n",
    "def label_smoothing_crossentropy_normalized(pred_probs, target_index, epsilon=0.1):\n",
    "    \"\"\"Cross-entropy loss with label smoothing, normalized to [0,1].\"\"\"\n",
    "    num_classes = pred_probs.shape[0] # Number of classes (tokens)\n",
    "    \n",
    "    # Build smoothed target\n",
    "    smooth_target = jnp.full_like(pred_probs, epsilon / (num_classes - 1))\n",
    "    smooth_target = smooth_target.at[target_index].set(1.0 - epsilon)\n",
    "    \n",
    "    # Compute cross-entropy\n",
    "    loss = -jnp.sum(smooth_target * jnp.log(pred_probs + 1e-10))\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    max_loss = jnp.log(num_classes)  # worst-case (uniform distribution)\n",
    "    norm_loss = loss / max_loss\n",
    "    \n",
    "    return norm_loss\n",
    "\n",
    "\n",
    "def compute_accuracy(pred_probs_batch, y_targets):\n",
    "    \"\"\"\n",
    "    pred_probs_batch: (batch_size, vocab_size)\n",
    "    y_targets: (batch_size,) integer indices\n",
    "    \"\"\"\n",
    "    pred_indices = jnp.argmax(pred_probs_batch, axis=1)  # predicted token index\n",
    "    correct = pred_indices == y_targets\n",
    "    return jnp.mean(correct)  # fraction correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token embedding\n",
    "key = jax.random.PRNGKey(42)\n",
    "EMBEDDING_SIZE = 8      # size of embeddings\n",
    "key, k_emb = jax.random.split(key)\n",
    "embedding_table = jax.random.normal(k_emb, (VOCABULARY_SIZE, EMBEDDING_SIZE)) * 0.1\n",
    "\n",
    "# Projection matrices\n",
    "key = jax.random.PRNGKey(42)\n",
    "proj_dim = BITS_PER_TOKEN  # number of qubits for quantum attention\n",
    "key, k_WQ, k_WK, k_WV = jax.random.split(key, 4)\n",
    "W_Q = jax.random.normal(k_WQ, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_K = jax.random.normal(k_WK, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_V = jax.random.normal(k_WV, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "\n",
    "\n",
    "# Effective qubit counts in variational layers\n",
    "n_token_ancilla = BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "# Initialize theta and sigma params\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, k_theta, k_theta_prop, k_sigma = jax.random.split(key, 4)\n",
    "\n",
    "# Precompute Z-string combos once\n",
    "token_ancilla_ws = token_wires + ancilla_wires\n",
    "combos = zstring_combos(token_ancilla_ws)\n",
    "n_strings = len(combos)\n",
    "\n",
    "# Combine all trainable parameters into a single dictionary\n",
    "combined_params = {\n",
    "    'theta': jax.random.normal(k_theta, (N_LAYERS, n_token_ancilla, 3)) * 0.1,\n",
    "    'theta_prop': jax.random.normal(k_theta_prop, (N_LAYERS, n_prop_qubits, n_token_ancilla, 4)) * 0.1,\n",
    "    'sigma': jax.random.normal(k_sigma, (N_LAYERS, n_strings)) * 0.1,\n",
    "    'embedding_table': embedding_table,\n",
    "    'W_Q': W_Q,\n",
    "    'W_K': W_K,\n",
    "    'W_V': W_V\n",
    "}\n",
    "# Training hyperparams\n",
    "learning_rate = 0.001\n",
    "n_epochs = 25\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(combined_params)\n",
    "\n",
    "MAX_PAST = 10  # maximum number of past tokens to keep\n",
    "\n",
    "# --- Prepare past tokens as fixed-size array ---\n",
    "def prepare_past_tokens(past_token_indices):\n",
    "    \"\"\"Convert dynamic list of past token indices to fixed-size array for JAX\"\"\"\n",
    "    arr = jnp.zeros(MAX_PAST, dtype=jnp.int32)\n",
    "    n_past = min(len(past_token_indices), MAX_PAST)\n",
    "    if n_past > 0:\n",
    "        arr = arr.at[:n_past].set(jnp.array(past_token_indices[-n_past:], dtype=jnp.int32))\n",
    "    return arr, n_past\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "\n",
    "def get_valid_embeddings(indices, table, W):\n",
    "    \"\"\"\n",
    "    Gather embeddings while ignoring zero-padding indices.\n",
    "    indices: (max_history,) int array\n",
    "    table: embedding table (VOCAB_SIZE, EMBED_DIM)\n",
    "    W: projection matrix (EMBED_DIM, proj_dim)\n",
    "    \"\"\"\n",
    "    # indices shape: (max_history,)\n",
    "    embeddings = table[indices] @ W         # shape: (max_history, proj_dim)\n",
    "    mask = (indices != 0).astype(jnp.float32)[:, None]  # (max_history, 1)\n",
    "    embeddings = embeddings * mask          # zero out padding\n",
    "    return jnp.sum(embeddings, axis=0)     # sum valid embeddings -> (proj_dim,)\n",
    "\n",
    "def compute_output_i(q, x_i, past_idx, embedding_table, W_K, W_V):\n",
    "    \"\"\"\n",
    "    Compute output_i for one element.\n",
    "    q: (proj_dim,)\n",
    "    x_i: (embed_dim,)\n",
    "    past_idx: (max_history,)\n",
    "    \"\"\"\n",
    "    def no_past(_):\n",
    "        return x_i @ W_V\n",
    "\n",
    "    def has_past(past_idx_inner):\n",
    "        K_vecs_sum = get_valid_embeddings(past_idx_inner, embedding_table, W_K)\n",
    "        V_vecs_sum = get_valid_embeddings(past_idx_inner, embedding_table, W_V)\n",
    "        return classical_attention(q, K_vecs_sum, V_vecs_sum)\n",
    "\n",
    "    return jax.lax.cond(jnp.all(past_idx == 0), no_past, has_past, operand=past_idx)\n",
    "\n",
    "def batched_training_step(params, opt_state, x_tokens, x_props, y_targets, past_tokens_batch):\n",
    "    \"\"\"\n",
    "    Batched training step.\n",
    "    x_tokens: (batch_size, BITS_PER_TOKEN)\n",
    "    x_props:  (batch_size, n_props)\n",
    "    y_targets:(batch_size,)\n",
    "    past_tokens_batch: (batch_size, max_history)\n",
    "    \"\"\"\n",
    "    theta_params = params['theta']\n",
    "    theta_prop   = params['theta_prop']\n",
    "    sigma_params = params['sigma']\n",
    "    embedding_table = params['embedding_table']\n",
    "    W_Q = params['W_Q']\n",
    "    W_K = params['W_K']\n",
    "    W_V = params['W_V']\n",
    "\n",
    "    batch_size = x_tokens.shape[0]\n",
    "\n",
    "    def loss_fn(params):\n",
    "        theta_params = params['theta']\n",
    "        theta_prop   = params['theta_prop']\n",
    "        sigma_params = params['sigma']\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        token_indices = vmap(bits_to_index)(x_tokens)       # (batch,)\n",
    "        x_i = embedding_table[token_indices]               # (batch, embed_dim)\n",
    "\n",
    "        # --- Q/K/V projections ---\n",
    "        Q = x_i @ W_Q                                      # (batch, proj_dim)\n",
    "\n",
    "        # --- Compute output_i for each batch element ---\n",
    "        output_i = vmap(\n",
    "            compute_output_i,\n",
    "            in_axes=(0, 0, 0, None, None, None)\n",
    "        )(Q, x_i, past_tokens_batch, embedding_table, W_K, W_V)  # (batch, proj_dim)\n",
    "\n",
    "        # --- QNode for each batch element ---\n",
    "        def qnode_per_example(x_token, x_prop, out_i):\n",
    "            return autoregressive_model(x_token, x_prop, theta_params, theta_prop, sigma_params, out_i)\n",
    "\n",
    "        pred_probs = vmap(qnode_per_example)(x_tokens, x_props, output_i)  # (batch, num_classes)\n",
    "\n",
    "        # --- Loss & accuracy ---\n",
    "        target_indices = vmap(bits_to_index)(y_targets)\n",
    "        losses = vmap(label_smoothing_crossentropy_normalized)(pred_probs, target_indices)\n",
    "        avg_loss = jnp.mean(losses)\n",
    "\n",
    "        acc = jnp.mean(jnp.argmax(pred_probs, axis=1) == target_indices)\n",
    "        return avg_loss, (acc, pred_probs)\n",
    "\n",
    "    (avg_loss, (avg_acc, pred_probs)), grads = value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return params, avg_loss, opt_state, grads, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/api.py:1175\u001b[0m, in \u001b[0;36m_mapped_axis_size.<locals>._get_axis_size\u001b[0;34m(name, shape, axis)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_token, x_props, y_target \u001b[38;5;129;01min\u001b[39;00m dataset:  \u001b[38;5;66;03m# each batch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     past_tokens_arr \u001b[38;5;241m=\u001b[39m prepare_past_tokens(past_token_indices)\n\u001b[0;32m----> 7\u001b[0m     combined_params, loss, opt_state, grads, acc \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_training_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_props\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_tokens_arr\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m x_token\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m     total_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc \u001b[38;5;241m*\u001b[39m x_token\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[44], line 85\u001b[0m, in \u001b[0;36mbatched_training_step\u001b[0;34m(params, opt_state, x_tokens, x_props, y_targets, past_tokens_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     acc \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(jnp\u001b[38;5;241m.\u001b[39margmax(pred_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m target_indices)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, (acc, pred_probs)\n\u001b[0;32m---> 85\u001b[0m (avg_loss, (avg_acc, pred_probs)), grads \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state, params)\n\u001b[1;32m     88\u001b[0m params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[44], line 66\u001b[0m, in \u001b[0;36mbatched_training_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     63\u001b[0m Q \u001b[38;5;241m=\u001b[39m x_i \u001b[38;5;241m@\u001b[39m W_Q                                      \u001b[38;5;66;03m# (batch, proj_dim)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# --- Compute output_i for each batch element ---\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m output_i \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_output_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_tokens_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_V\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, proj_dim)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# --- QNode for each batch element ---\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mqnode_per_example\u001b[39m(x_token, x_prop, out_i):\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/api.py:1179\u001b[0m, in \u001b[0;36m_mapped_axis_size.<locals>._get_axis_size\u001b[0;34m(name, shape, axis)\u001b[0m\n\u001b[1;32m   1177\u001b[0m min_rank \u001b[38;5;241m=\u001b[39m axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39maxis\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# TODO(mattjj): better error message here\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was requested to map its argument along axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich implies that its rank should be at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (its shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = total_acc = 0.0\n",
    "    past_token_indices = []\n",
    "\n",
    "    for x_token, x_props, y_target in dataset:  # each batch\n",
    "        past_tokens_arr = prepare_past_tokens(past_token_indices)\n",
    "        combined_params, loss, opt_state, grads, acc = batched_training_step(\n",
    "            combined_params,\n",
    "            opt_state,\n",
    "            x_token,\n",
    "            x_props,\n",
    "            y_target,\n",
    "            past_tokens_arr\n",
    "        )\n",
    "        total_loss += loss * x_token.shape[0]\n",
    "        total_acc += acc * x_token.shape[0]\n",
    "\n",
    "        # Update past_token_indices (append new batch)\n",
    "        past_token_indices = update_past_tokens(past_token_indices, x_token)\n",
    "\n",
    "    avg_loss = total_loss / dataset_size\n",
    "    avg_acc  = total_acc / dataset_size\n",
    "    print(f\"Epoch {epoch+1} | Loss = {avg_loss:.4f} | Accuracy = {avg_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
