{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Preprocessing\n",
    "**Objective:** To construct a robust dataset of drug-like molecules for the Quantum Sequence-to-Sequence model.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Data Source:** ChEMBL Database via the official API.\n",
    "2.  **Filtering:** We apply the \"Rule of Five\" and specific molecular weight/atom count constraints to ensure the molecules are suitable for drug discovery and small enough to be encoded into current quantum simulation capabilities.\n",
    "3.  **Representation:** We convert SMILES (Simplified Molecular Input Line Entry System) to SELFIES (Self-Referencing Embedded Strings) to ensure 100% chemical validity during the generation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import SaltRemover, GetFormalCharge\n",
    "from sklearn.model_selection import train_test_split\n",
    "import selfies as sf\n",
    "from math import ceil, log2\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from chembl_webresource_client.new_client import new_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering and Property Selection\n",
    "\n",
    "To construct a dataset that is both pharmaceutically relevant and suitable for current **Quantum Machine Learning (QML)** hardware (NISQ era), we filter the ChEMBL database using specific physicochemical constraints.\n",
    "\n",
    "Our goal is to select **\"Fragment-like\"** or **\"Lead-like\"** molecules. These are smaller than typical drug candidates, making them ideal for quantum simulation because they require shorter sequence lengths (fewer qubits) while retaining the core structural features of valid drugs.\n",
    "\n",
    "We apply the following filters:\n",
    "\n",
    "* **Molecular Weight (MW $\\le$ 300 Da):**\n",
    "    * *Reason:* Restricts the dataset to small molecules. This minimizes the sequence length (number of SELFIES tokens), reducing the depth of the quantum circuit required for processing.\n",
    "\n",
    "* **LogP (Partition Coefficient $\\le$ 5):**\n",
    "    * *Reason:* Measures lipophilicity. Adhering to **Lipinski's Rule of 5**, a LogP under 5 suggests the molecule is likely to be orally active and membrane-permeable.\n",
    "\n",
    "* **QED (Quantitative Estimation of Drug-likeness $\\ge$ 0.5):**\n",
    "    * *Reason:* A composite score (0 to 1) that aggregates multiple properties (solubility, polarity, structural alerts) to quantify how \"drug-like\" a structure is. A score above 0.5 ensures we are training on high-quality chemical matter.\n",
    "\n",
    "* **Heavy Atoms ($\\le$ 15):**\n",
    "    * *Reason:* A hard constraint to limit molecular complexity. Fewer atoms generally translate to shorter string representations, which is critical for efficient encoding into our quantum states (Basis Embedding).\n",
    "\n",
    "* **Rule of 5 Violations ($\\le$ 1):**\n",
    "    * *Reason:* Ensures general adherence to established guidelines for drug bioavailability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training molecules set:  65778\n"
     ]
    }
   ],
   "source": [
    "# Using the ChEMBL API to get the molecules dataset\n",
    "molecule = new_client.molecule\n",
    "\n",
    "# Filter for drug-like small molecules interesting for human use\n",
    "druglike_molecules = molecule.filter(\n",
    "    molecule_properties__heavy_atoms__lte=15,           # Heavy atoms less than 15\n",
    "    molecule_properties__alogp__lte=5,                  # LogP less than 5 (Lipophilicity and membrane permeability)\n",
    "    molecule_properties__mw_freebase__lte=300,          # Molecular weight less than 300 g/mol\n",
    "    molecule_properties__qed_weighted__gte=0.5,         # QED weighted greater than 0.5 (Drug-likeness)\n",
    "    molecule_properties__num_ro5_violations__lte=1,     # At most 1 Rule of 5 violation (Drug-likeness filter)\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Training molecules set: \", len(druglike_molecules))  # Check how many molecules match the filter criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular Filtering and Encoding\n",
    "Here we process the raw API data.\n",
    "* **Salt Removal:** We strip counter-ions to focus on the active pharmacophore.\n",
    "* **Stereochemistry:** We remove isomeric information to simplify the vocabulary size for the quantum embedding.\n",
    "* **Validation:** We ensure every molecule can be successfully translated to SELFIES without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 1000 molecules...\n",
      "\n",
      "--- Filtering Stats ---\n",
      "Total molecules processed: 1000\n",
      "Skipped (charged/zwitterion): 47\n",
      "Skipped (selfies valency error): 0\n",
      "Skipped (mixture/'.'): 0\n",
      "Kept for training: 953\n",
      "\n",
      "--- Final Results ---\n",
      "Final Alphabet of SELFIES characters: ['<SOS>', '[#Branch1]', '[#Branch2]', '[#C]', '[#N]', '[=Branch1]', '[=Branch2]', '[=C]', '[=N]', '[=O]', '[=P]', '[=Ring1]', '[=S]', '[Br]', '[Branch1]', '[Branch2]', '[C]', '[Cl]', '[F]', '[I]', '[NH1]', '[N]', '[O]', '[PH1]', '[P]', '[Ring1]', '[Ring2]', '[S]', '<EOS>', '<PAD>']\n",
      "Total unique characters in SELFIES: 30\n",
      "Maximum length of SELFIES in dataset: 31\n",
      "Bits per token: 5\n"
     ]
    }
   ],
   "source": [
    "# --- Set up filters ---\n",
    "remover = SaltRemover.SaltRemover()\n",
    "\n",
    "N_MOLECS = 1000\n",
    "molecules_subset = druglike_molecules[:N_MOLECS]\n",
    "\n",
    "MAX_LEN = 0\n",
    "alphabet = set()\n",
    "valid_molecules_for_training = [] \n",
    "total_processed = 0\n",
    "charged_skipped = 0\n",
    "selfies_error_skipped = 0\n",
    "mixture_skipped = 0\n",
    "\n",
    "print(f\"Starting with {len(molecules_subset)} molecules...\")\n",
    "\n",
    "for mol_data in molecules_subset:\n",
    "    total_processed += 1\n",
    "    smiles = mol_data.get('molecule_structures', {}).get('canonical_smiles')\n",
    "    if not smiles:\n",
    "        continue\n",
    "        \n",
    "    rdkit_mol = Chem.MolFromSmiles(smiles)\n",
    "    if rdkit_mol is None:\n",
    "        continue\n",
    "\n",
    "    # Remove salts (counter-ions)\n",
    "    neutral_mol = remover.StripMol(rdkit_mol)\n",
    "    \n",
    "    # Check every atom for formal charge (filters zwitterions)\n",
    "    has_charge = False\n",
    "    for atom in neutral_mol.GetAtoms():\n",
    "        if atom.GetFormalCharge() != 0:\n",
    "            has_charge = True\n",
    "            break # Found a charged atom\n",
    "    if has_charge:\n",
    "        charged_skipped += 1\n",
    "        continue # Skip this zwitterion/charged molecule\n",
    "\n",
    "    # Remove ALL stereochemistry\n",
    "    cleaned_smiles = Chem.MolToSmiles(neutral_mol, isomericSmiles=False)\n",
    "    \n",
    "    if not cleaned_smiles:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        selfies = sf.encoder(cleaned_smiles)\n",
    "    except sf.EncoderError:\n",
    "        # Skip molecules with exotic valency (like hypervalent Iodine)\n",
    "        selfies_error_skipped += 1\n",
    "        continue \n",
    "    \n",
    "    if selfies:\n",
    "        # Final check for mixtures\n",
    "        if \".\" in selfies:\n",
    "            mixture_skipped += 1\n",
    "            continue # Skip any remaining molecules with '.'\n",
    "            \n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if MAX_LEN < len(tokens):\n",
    "            MAX_LEN = len(tokens)\n",
    "        alphabet.update(tokens)\n",
    "        \n",
    "        # This one is good! Store it.\n",
    "        valid_molecules_for_training.append((mol_data, cleaned_smiles, selfies, tokens))\n",
    "\n",
    "# Build Final Alphabet ---\n",
    "alphabet = sorted(list(alphabet))\n",
    "alphabet = ['<SOS>'] + alphabet + ['<EOS>'] + ['<PAD>']\n",
    "\n",
    "VOCABULARY_SIZE = len(alphabet)\n",
    "BITS_PER_TOKEN = ceil(log2(VOCABULARY_SIZE))\n",
    "MAX_LEN += 2  # For <SOS> and <EOS>\n",
    "\n",
    "print(f\"\\n--- Filtering Stats ---\")\n",
    "print(f\"Total molecules processed: {total_processed}\")\n",
    "print(f\"Skipped (charged/zwitterion): {charged_skipped}\")\n",
    "print(f\"Skipped (selfies valency error): {selfies_error_skipped}\")\n",
    "print(f\"Skipped (mixture/'.'): {mixture_skipped}\")\n",
    "print(f\"Kept for training: {len(valid_molecules_for_training)}\")\n",
    "\n",
    "print(f\"\\n--- Final Results ---\")\n",
    "print(f\"Final Alphabet of SELFIES characters: {alphabet}\")\n",
    "print(f\"Total unique characters in SELFIES: {VOCABULARY_SIZE}\")\n",
    "print(f\"Maximum length of SELFIES in dataset: {MAX_LEN}\")\n",
    "print(f\"Bits per token: {BITS_PER_TOKEN}\")\n",
    "\n",
    "# Create token to index mapping\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<SOS>' → index 0 → 00000\n",
      "'[#Branch1]' → index 1 → 00001\n",
      "'[#Branch2]' → index 2 → 00010\n",
      "'[#C]' → index 3 → 00011\n",
      "'[#N]' → index 4 → 00100\n",
      "'[=Branch1]' → index 5 → 00101\n",
      "'[=Branch2]' → index 6 → 00110\n",
      "'[=C]' → index 7 → 00111\n",
      "'[=N]' → index 8 → 01000\n",
      "'[=O]' → index 9 → 01001\n",
      "'[=P]' → index 10 → 01010\n",
      "'[=Ring1]' → index 11 → 01011\n",
      "'[=S]' → index 12 → 01100\n",
      "'[Br]' → index 13 → 01101\n",
      "'[Branch1]' → index 14 → 01110\n",
      "'[Branch2]' → index 15 → 01111\n",
      "'[C]' → index 16 → 10000\n",
      "'[Cl]' → index 17 → 10001\n",
      "'[F]' → index 18 → 10010\n",
      "'[I]' → index 19 → 10011\n",
      "'[NH1]' → index 20 → 10100\n",
      "'[N]' → index 21 → 10101\n",
      "'[O]' → index 22 → 10110\n",
      "'[PH1]' → index 23 → 10111\n",
      "'[P]' → index 24 → 11000\n",
      "'[Ring1]' → index 25 → 11001\n",
      "'[Ring2]' → index 26 → 11010\n",
      "'[S]' → index 27 → 11011\n",
      "'<EOS>' → index 28 → 11100\n",
      "'<PAD>' → index 29 → 11101\n"
     ]
    }
   ],
   "source": [
    "# Diccionario token → índice\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def print_token_bits(tokens, token_to_index):\n",
    "    for tok in tokens:\n",
    "        idx = token_to_index.get(tok, None)\n",
    "        if idx is None:\n",
    "            print(f\"Token '{tok}' no está en el diccionario.\")\n",
    "            continue\n",
    "        binary = format(idx, f'0{BITS_PER_TOKEN}b')\n",
    "        print(f\"'{tok}' → index {idx} → {binary}\")\n",
    "\n",
    "print_token_bits(alphabet, token_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bit-Basis Encoding\n",
    "To process discrete tokens in a quantum circuit, we map the vocabulary index of each token to a binary string. This allows us to use **Basis Embedding** (preparing qubits in state |0> or |1>) as the input state for our variational circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_encoded_dataset = []\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def smiles_to_bits(tokens: list) -> np.ndarray:\n",
    "    \"\"\"Convert tokens to a 2D array\"\"\"\n",
    "    padded_tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "    bit_matrix = []\n",
    "    for tok in padded_tokens:\n",
    "        idx = token_to_index[tok]\n",
    "        bits = list(f\"{idx:0{BITS_PER_TOKEN}b}\")  # length of the binary string depends on the number of bits required to represent the alphabet\n",
    "        bit_matrix.append([int(b) for b in bits])\n",
    "    return np.array(bit_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP range: -1.53 to 4.66\n",
      "QED range: 0.5 to 0.92\n",
      "MW range: 73.14 to 298.11\n"
     ]
    }
   ],
   "source": [
    "def normalize(value, min_val, max_val, target_max=np.pi):\n",
    "    ''' Normalize a value to a range [0, [0, pi] to later encode them as rotation angles'''\n",
    "    norm = (value - min_val) / (max_val - min_val) * target_max\n",
    "    return float(f\"{norm:.3f}\")\n",
    "    \n",
    "min_logp = float('inf')\n",
    "max_logp = float('-inf')\n",
    "min_qed = float('inf')\n",
    "max_qed = float('-inf')\n",
    "min_mw = float('inf')\n",
    "max_mw = float('-inf')\n",
    "\n",
    "# Iterate through the subset of molecules to find min/max properties to normalize them\n",
    "for mol in molecules_subset:\n",
    "    logP = mol.get('molecule_properties', {}).get('alogp')\n",
    "    qed = mol.get('molecule_properties', {}).get('qed_weighted')\n",
    "    mw = mol.get('molecule_properties', {}).get('mw_freebase')\n",
    "\n",
    "    if logP is None or qed is None or mw is None:\n",
    "        continue  # Skip if any property is missing\n",
    "\n",
    "    logP = float(logP)\n",
    "    qed = float(qed)\n",
    "    mw = float(mw)\n",
    "\n",
    "    if logP < min_logp:\n",
    "        min_logp = logP\n",
    "    if logP > max_logp:\n",
    "        max_logp = logP\n",
    "\n",
    "    if qed < min_qed:\n",
    "        min_qed = qed\n",
    "    if qed > max_qed:\n",
    "        max_qed = qed\n",
    "\n",
    "    if mw < min_mw:\n",
    "        min_mw = mw\n",
    "    if mw > max_mw:\n",
    "        max_mw = mw\n",
    "\n",
    "print(f\"LogP range: {min_logp} to {max_logp}\")\n",
    "print(f\"QED range: {min_qed} to {max_qed}\")\n",
    "print(f\"MW range: {min_mw} to {max_mw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to ../data/metadata_selfies_1000.json.\n"
     ]
    }
   ],
   "source": [
    "# --- Save metadata to a JSON file \n",
    "metadata = {\n",
    "    \"vocabulary_size\": VOCABULARY_SIZE,\n",
    "    \"bits_per_token\": BITS_PER_TOKEN,\n",
    "    \"alphabet\": alphabet,\n",
    "    \"max_sequence_length\": MAX_LEN,\n",
    "    \"min_logP\": min_logp,\n",
    "    \"max_logP\": max_logp,\n",
    "    \"min_qed\": min_qed,\n",
    "    \"max_qed\": max_qed,\n",
    "    \"min_mw\": min_mw,\n",
    "    \"max_mw\": max_mw\n",
    "}\n",
    "METADATA_PATH = F\"../data/metadata_selfies_{N_MOLECS}.json\"\n",
    "with open(METADATA_PATH, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(f\"Metadata saved to {METADATA_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validate - Test\n",
    "\n",
    "In this step the data is split so that every model can be trained, validated and tested over the same data.\n",
    "\n",
    "**Training Set (~70%):** Used by the optimizer to calculate gradients and update model weights ($\\theta$). The goal is to fit the model to the data.\n",
    "\n",
    "**Validation Set (~15%):** It detects overfitting. Used for hyperparameter tunning (e.g., number of layers).\n",
    "\n",
    "**Test Set (~15%):** Used to generate the final accuracy numbers and molecular plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 953 molecules...\n",
      "Total valid rows processed: 953\n",
      "Split sizes - Train: 667, Val: 143, Test: 143\n",
      "Saved ../data/structured_data_selfies_1000_train.csv\n",
      "Saved ../data/structured_data_selfies_1000_val.csv\n",
      "Saved ../data/structured_data_selfies_1000_test.csv\n"
     ]
    }
   ],
   "source": [
    "all_processed_rows = []\n",
    "header = [\"logP\", \"qed\", \"mw\"] + [f\"token_{i}\" for i in range(MAX_LEN)]\n",
    "\n",
    "print(f\"Processing {len(valid_molecules_for_training)} molecules...\")\n",
    "\n",
    "for mol in valid_molecules_for_training:\n",
    "    smiles = mol[1]\n",
    "    selfies = mol[-1]\n",
    "    props = mol[0].get('molecule_properties', {})\n",
    "    \n",
    "    if not selfies or \".\" in selfies:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        logP = float(props.get('alogp'))\n",
    "        qed = float(props.get('qed_weighted'))\n",
    "        mw = float(props.get('mw_freebase'))\n",
    "    except (TypeError, ValueError):\n",
    "        continue\n",
    "\n",
    "    # Normalize properties\n",
    "    norm_logp = normalize(logP, min_logp, max_logp)\n",
    "    norm_qed = normalize(qed, min_qed, max_qed)\n",
    "    norm_mw = normalize(mw, min_mw, max_mw)\n",
    "\n",
    "    if not all(tok in token_to_index for tok in selfies):\n",
    "        continue\n",
    "\n",
    "    # Encode to bits\n",
    "    bit_matrix = smiles_to_bits(selfies)\n",
    "    token_bits_as_strings = [\"\".join(map(str, row)) for row in bit_matrix]\n",
    "    \n",
    "    # Construct the row\n",
    "    row = [norm_logp, norm_qed, norm_mw] + token_bits_as_strings\n",
    "\n",
    "    # Padding\n",
    "    while len(row) < len(header):\n",
    "        pad_idx = token_to_index['<PAD>']\n",
    "        pad_bits = f\"{pad_idx:0{BITS_PER_TOKEN}b}\"\n",
    "        row.append(pad_bits)\n",
    "    \n",
    "    # Append to list instead of writing to file\n",
    "    all_processed_rows.append(row)\n",
    "\n",
    "print(f\"Total valid rows processed: {len(all_processed_rows)}\")\n",
    "\n",
    "# 2. Perform the Train / Validation / Test Split\n",
    "# Split: 70% Train, 15% Val, 15% Test\n",
    "train_rows, test_val_rows = train_test_split(all_processed_rows, test_size=0.3, random_state=42)\n",
    "val_rows, test_rows = train_test_split(test_val_rows, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Split sizes - Train: {len(train_rows)}, Val: {len(val_rows)}, Test: {len(test_rows)}\")\n",
    "\n",
    "# 3. Helper function to write CSVs\n",
    "def save_split_csv(filename, data_rows, header):\n",
    "    with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data_rows)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# 4. Save the three files\n",
    "base_path = f\"../data/structured_data_selfies_{N_MOLECS}\"\n",
    "save_split_csv(f\"{base_path}_train.csv\", train_rows, header)\n",
    "save_split_csv(f\"{base_path}_val.csv\", val_rows, header)\n",
    "save_split_csv(f\"{base_path}_test.csv\", test_rows, header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
