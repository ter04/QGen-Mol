{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import SaltRemover, GetFormalCharge\n",
    "import selfies as sf\n",
    "from math import ceil, log2\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from chembl_webresource_client.new_client import new_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training molecules set:  65778\n"
     ]
    }
   ],
   "source": [
    "# Using the ChEMBL API to get the molecules dataset\n",
    "molecule = new_client.molecule\n",
    "\n",
    "# Filter for drug-like small molecules interesting for human use\n",
    "druglike_molecules = molecule.filter(\n",
    "    molecule_properties__heavy_atoms__lte=15,           # Heavy atoms less than 15\n",
    "    molecule_properties__alogp__lte=5,                  # LogP less than 5 (Lipophilicity and membrane permeability)\n",
    "    molecule_properties__mw_freebase__lte=300,          # Molecular weight less than 300 g/mol\n",
    "    molecule_properties__qed_weighted__gte=0.5,         # QED weighted greater than 0.5 (Drug-likeness)\n",
    "    molecule_properties__num_ro5_violations__lte=1,     # At most 1 Rule of 5 violation (Drug-likeness filter)\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Training molecules set: \", len(druglike_molecules))  # Check how many molecules match the filter criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 10000 molecules...\n",
      "\n",
      "--- Filtering Stats ---\n",
      "Total molecules processed: 10000\n",
      "Skipped (charged/zwitterion): 529\n",
      "Skipped (selfies valency error): 7\n",
      "Skipped (mixture/'.'): 2\n",
      "Kept for training: 9462\n",
      "\n",
      "--- Final Results ---\n",
      "Final Alphabet of SELFIES characters: ['<SOS>', '[#Branch1]', '[#Branch2]', '[#C]', '[#N]', '[=Branch1]', '[=Branch2]', '[=C]', '[=N]', '[=O]', '[=PH1]', '[=P]', '[=Ring1]', '[=S]', '[Br]', '[Branch1]', '[Branch2]', '[C]', '[Cl]', '[F]', '[H]', '[I]', '[NH1]', '[N]', '[O]', '[PH1]', '[P]', '[Ring1]', '[Ring2]', '[S]', '<EOS>', '<PAD>']\n",
      "Total unique characters in SELFIES: 32\n",
      "Maximum length of SELFIES in dataset: 34\n",
      "Bits per token: 5\n"
     ]
    }
   ],
   "source": [
    "# --- Set up filters ---\n",
    "remover = SaltRemover.SaltRemover()\n",
    "\n",
    "# --- Use a larger, cleaner subset ---\n",
    "molecules_subset = druglike_molecules[:10000]\n",
    "\n",
    "MAX_LEN = 0\n",
    "alphabet = set()\n",
    "valid_molecules_for_training = [] \n",
    "total_processed = 0\n",
    "charged_skipped = 0\n",
    "selfies_error_skipped = 0\n",
    "mixture_skipped = 0\n",
    "\n",
    "print(f\"Starting with {len(molecules_subset)} molecules...\")\n",
    "\n",
    "for mol_data in molecules_subset:\n",
    "    total_processed += 1\n",
    "    smiles = mol_data.get('molecule_structures', {}).get('canonical_smiles')\n",
    "    if not smiles:\n",
    "        continue\n",
    "        \n",
    "    rdkit_mol = Chem.MolFromSmiles(smiles)\n",
    "    if rdkit_mol is None:\n",
    "        continue\n",
    "\n",
    "    # Remove salts (counter-ions)\n",
    "    neutral_mol = remover.StripMol(rdkit_mol)\n",
    "    \n",
    "    # Check every atom for formal charge (filters zwitterions)\n",
    "    has_charge = False\n",
    "    for atom in neutral_mol.GetAtoms():\n",
    "        if atom.GetFormalCharge() != 0:\n",
    "            has_charge = True\n",
    "            break # Found a charged atom\n",
    "    if has_charge:\n",
    "        charged_skipped += 1\n",
    "        continue # Skip this zwitterion/charged molecule\n",
    "\n",
    "    # Remove ALL stereochemistry\n",
    "    cleaned_smiles = Chem.MolToSmiles(neutral_mol, isomericSmiles=False)\n",
    "    \n",
    "    if not cleaned_smiles:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        selfies = sf.encoder(cleaned_smiles)\n",
    "    except sf.EncoderError:\n",
    "        # Skip molecules with exotic valency (like hypervalent Iodine)\n",
    "        selfies_error_skipped += 1\n",
    "        continue \n",
    "    \n",
    "    if selfies:\n",
    "        # Final check for mixtures\n",
    "        if \".\" in selfies:\n",
    "            mixture_skipped += 1\n",
    "            continue # Skip any remaining molecules with '.'\n",
    "            \n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if MAX_LEN < len(tokens):\n",
    "            MAX_LEN = len(tokens)\n",
    "        alphabet.update(tokens)\n",
    "        \n",
    "        # This one is good! Store it.\n",
    "        valid_molecules_for_training.append((mol_data, cleaned_smiles, selfies, tokens))\n",
    "\n",
    "# Build Final Alphabet ---\n",
    "alphabet = sorted(list(alphabet))\n",
    "alphabet = ['<SOS>'] + alphabet + ['<EOS>'] + ['<PAD>']\n",
    "\n",
    "VOCABULARY_SIZE = len(alphabet)\n",
    "BITS_PER_TOKEN = ceil(log2(VOCABULARY_SIZE))\n",
    "MAX_LEN += 2  # For <SOS> and <EOS>\n",
    "\n",
    "print(f\"\\n--- Filtering Stats ---\")\n",
    "print(f\"Total molecules processed: {total_processed}\")\n",
    "print(f\"Skipped (charged/zwitterion): {charged_skipped}\")\n",
    "print(f\"Skipped (selfies valency error): {selfies_error_skipped}\")\n",
    "print(f\"Skipped (mixture/'.'): {mixture_skipped}\")\n",
    "print(f\"Kept for training: {len(valid_molecules_for_training)}\")\n",
    "\n",
    "print(f\"\\n--- Final Results ---\")\n",
    "print(f\"Final Alphabet of SELFIES characters: {alphabet}\")\n",
    "print(f\"Total unique characters in SELFIES: {VOCABULARY_SIZE}\")\n",
    "print(f\"Maximum length of SELFIES in dataset: {MAX_LEN}\")\n",
    "print(f\"Bits per token: {BITS_PER_TOKEN}\")\n",
    "\n",
    "# Create token to index mapping\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<SOS>' → index 0 → 00000\n",
      "'[#Branch1]' → index 1 → 00001\n",
      "'[#Branch2]' → index 2 → 00010\n",
      "'[#C]' → index 3 → 00011\n",
      "'[#N]' → index 4 → 00100\n",
      "'[=Branch1]' → index 5 → 00101\n",
      "'[=Branch2]' → index 6 → 00110\n",
      "'[=C]' → index 7 → 00111\n",
      "'[=N]' → index 8 → 01000\n",
      "'[=O]' → index 9 → 01001\n",
      "'[=PH1]' → index 10 → 01010\n",
      "'[=P]' → index 11 → 01011\n",
      "'[=Ring1]' → index 12 → 01100\n",
      "'[=S]' → index 13 → 01101\n",
      "'[Br]' → index 14 → 01110\n",
      "'[Branch1]' → index 15 → 01111\n",
      "'[Branch2]' → index 16 → 10000\n",
      "'[C]' → index 17 → 10001\n",
      "'[Cl]' → index 18 → 10010\n",
      "'[F]' → index 19 → 10011\n",
      "'[H]' → index 20 → 10100\n",
      "'[I]' → index 21 → 10101\n",
      "'[NH1]' → index 22 → 10110\n",
      "'[N]' → index 23 → 10111\n",
      "'[O]' → index 24 → 11000\n",
      "'[PH1]' → index 25 → 11001\n",
      "'[P]' → index 26 → 11010\n",
      "'[Ring1]' → index 27 → 11011\n",
      "'[Ring2]' → index 28 → 11100\n",
      "'[S]' → index 29 → 11101\n",
      "'<EOS>' → index 30 → 11110\n",
      "'<PAD>' → index 31 → 11111\n"
     ]
    }
   ],
   "source": [
    "# Diccionario token → índice\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def print_token_bits(tokens, token_to_index):\n",
    "    for tok in tokens:\n",
    "        idx = token_to_index.get(tok, None)\n",
    "        if idx is None:\n",
    "            print(f\"Token '{tok}' no está en el diccionario.\")\n",
    "            continue\n",
    "        binary = format(idx, f'0{BITS_PER_TOKEN}b')\n",
    "        print(f\"'{tok}' → index {idx} → {binary}\")\n",
    "\n",
    "print_token_bits(alphabet, token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_encoded_dataset = []\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def smiles_to_bits(tokens: list) -> np.ndarray:\n",
    "    \"\"\"Convert tokens to a 2D array\"\"\"\n",
    "    padded_tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "    bit_matrix = []\n",
    "    for tok in padded_tokens:\n",
    "        idx = token_to_index[tok]\n",
    "        bits = list(f\"{idx:0{BITS_PER_TOKEN}b}\")  # length of the binary string depends on the number of bits required to represent the alphabet\n",
    "        bit_matrix.append([int(b) for b in bits])\n",
    "    return np.array(bit_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP range: -2.51 to 4.89\n",
      "QED range: 0.5 to 0.94\n",
      "MW range: 73.14 to 299.09\n"
     ]
    }
   ],
   "source": [
    "min_logp = float('inf')\n",
    "max_logp = float('-inf')\n",
    "min_qed = float('inf')\n",
    "max_qed = float('-inf')\n",
    "min_mw = float('inf')\n",
    "max_mw = float('-inf')\n",
    "\n",
    "\n",
    "# Iterate through the subset of molecules to find min/max properties to normalize them\n",
    "for mol in molecules_subset:\n",
    "    logP = mol.get('molecule_properties', {}).get('alogp')\n",
    "    qed = mol.get('molecule_properties', {}).get('qed_weighted')\n",
    "    mw = mol.get('molecule_properties', {}).get('mw_freebase')\n",
    "\n",
    "    if logP is None or qed is None or mw is None:\n",
    "        continue  # Skip if any property is missing\n",
    "\n",
    "    logP = float(logP)\n",
    "    qed = float(qed)\n",
    "    mw = float(mw)\n",
    "\n",
    "    if logP < min_logp:\n",
    "        min_logp = logP\n",
    "    if logP > max_logp:\n",
    "        max_logp = logP\n",
    "\n",
    "    if qed < min_qed:\n",
    "        min_qed = qed\n",
    "    if qed > max_qed:\n",
    "        max_qed = qed\n",
    "\n",
    "    if mw < min_mw:\n",
    "        min_mw = mw\n",
    "    if mw > max_mw:\n",
    "        max_mw = mw\n",
    "\n",
    "print(f\"LogP range: {min_logp} to {max_logp}\")\n",
    "print(f\"QED range: {min_qed} to {max_qed}\")\n",
    "print(f\"MW range: {min_mw} to {max_mw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_val, max_val, target_max=np.pi):\n",
    "    ''' Normalize a value to a range [0, [0, pi] to later encode them as rotation angles'''\n",
    "    norm = (value - min_val) / (max_val - min_val) * target_max\n",
    "    return float(f\"{norm:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequences in the subset: 34\n"
     ]
    }
   ],
   "source": [
    "# Write the structured data to a CSV file\n",
    "DATA_PATH = \"../data/structured_data_selfies.csv\"\n",
    "with open(DATA_PATH, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    header = [\"logP\", \"qed\", \"mw\"] + [f\"token_{i}\" for i in range(MAX_LEN)]\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for mol in valid_molecules_for_training:\n",
    "        smiles = mol[1]\n",
    "        selfies = mol[-1]\n",
    "        props = mol[0].get('molecule_properties', {})\n",
    "        if not selfies:\n",
    "            continue\n",
    "        if \".\" in selfies:\n",
    "            continue\n",
    "        try:\n",
    "            logP = float(props.get('alogp'))\n",
    "            qed = float(props.get('qed_weighted'))\n",
    "            mw = float(props.get('mw_freebase'))\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "\n",
    "        norm_logp = normalize(logP, min_logp, max_logp)\n",
    "        norm_qed = normalize(qed, min_qed, max_qed)\n",
    "        norm_mw = normalize(mw, min_mw, max_mw)\n",
    "\n",
    "        if not all(tok in token_to_index for tok in selfies):\n",
    "            continue\n",
    "\n",
    "        bit_matrix = smiles_to_bits(selfies)  # shape (n_tokens, 6)\n",
    "        token_bits_as_strings = [\"\".join(map(str, row)) for row in bit_matrix]\n",
    "        row = [norm_logp, norm_qed, norm_mw] + token_bits_as_strings\n",
    "\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Maximum length of sequences in the subset:\", MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitstr_to_array(bitstr):\n",
    "    \"\"\"Convert a string of bits (e.g., '010101') to a numpy float32 array.\"\"\"\n",
    "    return np.array([int(b) for b in bitstr], dtype=np.float32)\n",
    "\n",
    "def build_training_data(df):\n",
    "    \"\"\"\n",
    "    Build dataset tuples of (input_token_bits, molecular_properties, target_token_bits)\n",
    "    from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing molecular properties and token bit strings.\n",
    "        n_token_cols (int): Number of token columns in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (x_token: np.array, x_props: np.array, y_target: np.array)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract molecular properties as a numpy float32 array\n",
    "        props = [row['logP'], row['qed'], row['mw']]\n",
    "        x_props = np.array(props, dtype=np.float32)\n",
    "\n",
    "        tokens = row[3:]  # token columns after properties\n",
    "\n",
    "        # Iterate over token sequence to create input-target pairs\n",
    "        for i in range(len(tokens) - 1):\n",
    "            current_token = tokens.iloc[i]\n",
    "            next_token = tokens.iloc[i + 1]\n",
    "\n",
    "            # Skip missing or NaN tokens\n",
    "            if current_token is None or (isinstance(current_token, float) and math.isnan(current_token)):\n",
    "                continue\n",
    "            if next_token is None or (isinstance(next_token, float) and math.isnan(next_token)):\n",
    "                continue\n",
    "\n",
    "            x_token = bitstr_to_array(current_token)\n",
    "            y_target = bitstr_to_array(next_token)\n",
    "\n",
    "            dataset.append((x_token, x_props, y_target))\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset saved to ../data/training_data_selfies.pickle with 198650 samples.\n",
      "Metadata saved to ../data/metadata_selfies.json.\n"
     ]
    }
   ],
   "source": [
    "# --- Load dataset\n",
    "token_cols = [f\"token_{i}\" for i in range(MAX_LEN)]\n",
    "df = pd.read_csv(DATA_PATH, dtype={col: str for col in token_cols})\n",
    "dataset = build_training_data(df)  # Should return list/array of (x_token, x_props, y_target)\n",
    "\n",
    "# --- Save dataset as CSV\n",
    "DATASET_PATH = \"../data/training_data_selfies.pickle\"\n",
    "with open(DATASET_PATH, \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "print(f\"Training dataset saved to {DATASET_PATH} with {len(dataset)} samples.\")\n",
    "\n",
    "# --- Save metadata to a JSON file\n",
    "metadata = {\n",
    "    \"vocabulary_size\": VOCABULARY_SIZE,\n",
    "    \"bits_per_token\": BITS_PER_TOKEN,\n",
    "    \"alphabet\": alphabet,\n",
    "    \"max_sequence_length\": MAX_LEN\n",
    "}\n",
    "METADATA_PATH = \"../data/metadata_selfies.json\"\n",
    "with open(METADATA_PATH, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(f\"Metadata saved to {METADATA_PATH}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
