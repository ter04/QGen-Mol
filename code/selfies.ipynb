{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import selfies as sf\n",
    "\n",
    "import numpy as np\n",
    "from math import ceil, log2\n",
    "import re\n",
    "import pandas as pd\n",
    "import optax\n",
    "import csv\n",
    "import json\n",
    "import flax.linen as nn\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import qchem\n",
    "from pennylane.templates import StronglyEntanglingLayers\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn.initializers import normal\n",
    "\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the alphabet used for the SMILEs representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the alphabet considering the structure of some special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tSímbolos de enlaces y paréntesis: #, (, ), /, \\, =\n",
    "\n",
    "•\tDígitos simples para cierres de anillos: '1', '2', '3', '4', '5'\n",
    "\n",
    "•\tÁtomos orgánicos y halógenos comunes, tanto mayúsculas (alifáticos) como minúsculas (aromáticos)\n",
    "\n",
    "•\tTokens entre corchetes para isótopos, estados de carga, quiralidad, etc.\n",
    "\n",
    "•\tUn token especial '<PAD>' para padding en modelos ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input SMILEs must be the same size, so we need to use the padding to make it uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain the molecular properties of interest\n",
    "\n",
    "logP (o cx_logp) -> Coeficiente de partición octanol/agua (lipofilia)\n",
    "\n",
    "QED (quantitative estimate of drug-likeness) -> Escala combinada que evalúa qué tan “drug-like” es una molécula\n",
    "\n",
    "SAS (Synthetic Accessibility Score) -> Qué tan difícil sería sintetizar la molécula en laboratorio * Needs to be calculated separately!!\n",
    "\n",
    "MW (peso molecular) -> Masa total, típicamente ≤ 500 Da para buenos fármacos orales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the max and min range values in order to later normalize the properties in the range (0, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 29\n",
      "Bits per Token: 5\n",
      "Max Sequence Length: 31\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from JSON\n",
    "N_MOLECS = 500\n",
    "META_DATA_PATH = f\"../data/metadata_selfies_{N_MOLECS}.json\"\n",
    "\n",
    "with open(META_DATA_PATH, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "VOCABULARY_SIZE = metadata['vocabulary_size']\n",
    "BITS_PER_TOKEN = metadata['bits_per_token']\n",
    "MAX_LEN = metadata['max_sequence_length']\n",
    "ALPHABET = metadata['alphabet']\n",
    "\n",
    "print(\"Vocabulary Size:\", VOCABULARY_SIZE)\n",
    "print(\"Bits per Token:\", BITS_PER_TOKEN)\n",
    "print(\"Max Sequence Length:\", MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from CSV\n",
    "DATA_PATH = f\"../data/training_data_selfies_{N_MOLECS}.pickle\"\n",
    "dataset = pd.read_pickle(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "def normalize(value, min_val, max_val, target_max=np.pi):\n",
    "    ''' Normalize a value to a range [0, [0, pi] to later encode them as rotation angles'''\n",
    "    norm = (value - min_val) / (max_val - min_val) * target_max\n",
    "    return float(f\"{norm:.3f}\")\n",
    "\n",
    "def token_to_index(token):\n",
    "    ''' Map a SELFIES token to its corresponding index in the ALPHABET'''\n",
    "    if token in ALPHABET:\n",
    "        return ALPHABET.index(token)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def bits_to_index(bits):\n",
    "    powers = 2 ** jnp.arange(len(bits) - 1, -1, -1)\n",
    "    return jnp.dot(bits, powers).astype(jnp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantum Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zstring_combos(wires):\n",
    "    \"\"\"\n",
    "    Return an ordered list of wire-tuples for all Z-strings up to order H_LOCAL.\n",
    "    Order: all 1-local, then all 2-local, ..., up to H_LOCAL.\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    for k in range(1, H_LOCAL + 1):\n",
    "        L.extend(itertools.combinations(wires, k))\n",
    "    return [tuple(c) for c in L]\n",
    "\n",
    "def num_zstrings(n_wires):\n",
    "    \"\"\"Count how many Z-strings up to order H_LOCAL.\"\"\"\n",
    "    from math import comb\n",
    "    return sum(comb(n_wires, k) for k in range(1, H_LOCAL + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Attention mechanism using SWAP test\n",
    "\n",
    "# --- Device for attention ---\n",
    "n_past = 5\n",
    "attn_dev = qml.device(\"default.qubit\", wires=(BITS_PER_TOKEN+1)*n_past)\n",
    "\n",
    "@qml.qnode(attn_dev, interface=\"jax\")\n",
    "def quantum_attention_qnode(Q_vec, K_vecs):\n",
    "    \"\"\"\n",
    "    Q_vec: projected query vector of current token\n",
    "    K_vecs: list of projected key vectors for past tokens\n",
    "    Returns: attention scores ⟨q_i | k_j⟩ for each j\n",
    "    \"\"\"\n",
    "    n_tokens = len(K_vecs)\n",
    "    q_wires = list(range(BITS_PER_TOKEN))\n",
    "\n",
    "    # Encode Q and all K in parallel (different wire registers)\n",
    "    def encode_token(angles, wires):\n",
    "        # Use AngleEmbedding for compactness, then entangle\n",
    "        qml.templates.AngleEmbedding(angles, wires=wires, rotation=\"Y\")\n",
    "        for i in range(len(wires)-1):\n",
    "            qml.CNOT(wires=[wires[i], wires[i+1]])\n",
    "\n",
    "    # Encode Q\n",
    "    encode_token(Q_vec, wires=q_wires)\n",
    "\n",
    "    # Encode K_j\n",
    "    # Collect expectation values (one per K_j)\n",
    "    measurements = []\n",
    "    for j, K_j in enumerate(K_vecs):\n",
    "        start = (BITS_PER_TOKEN+1) + j*(BITS_PER_TOKEN+1)\n",
    "        k_wires = list(range(start, start+BITS_PER_TOKEN))\n",
    "        encode_token(K_j, k_wires)\n",
    "\n",
    "        # SWAP test\n",
    "        ancilla = start + BITS_PER_TOKEN\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "        for qw, kw in zip(q_wires, k_wires):\n",
    "            qml.CSWAP(wires=[ancilla, qw, kw])\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "\n",
    "        measurements.append(qml.expval(qml.PauliZ(ancilla)))\n",
    "\n",
    "    # **Return as tuple** so PennyLane converts to JAX array\n",
    "    return tuple(measurements)\n",
    "\n",
    "\n",
    "def quantum_attention(Q_vec, K_vecs, V_vecs):\n",
    "    # Make non-traced (concrete) copies for the QNode\n",
    "    Q_safe = jax.lax.stop_gradient(Q_vec)\n",
    "    K_safe = [jax.lax.stop_gradient(k) for k in K_vecs]\n",
    "\n",
    "    raw_expvals = quantum_attention_qnode(Q_safe, K_safe)\n",
    "    raw_expvals = jnp.asarray(raw_expvals)\n",
    "    # Convert from expectation values (in [-1,1]) to probabilities [0,1]\n",
    "    overlaps = (1.0 - raw_expvals) / 2.0\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def classical_attention(Q_vec, K_vecs, V_vecs, mask, scale=True):\n",
    "    \"\"\"\n",
    "    Calculates classical dot-product attention with padding mask.\n",
    "\n",
    "    Args:\n",
    "        Q_vec: Query vector, shape (proj_dim,)\n",
    "        K_vecs: Key matrix, shape (n_past, proj_dim)\n",
    "        V_vecs: Value matrix, shape (n_past, proj_dim)\n",
    "        mask: Padding mask, shape (n_past,). \n",
    "              Contains 0.0 for valid tokens and -jnp.inf for padding.\n",
    "        scale: Whether to scale scores by sqrt(proj_dim).\n",
    "        \n",
    "    Returns:\n",
    "        Output context vector, shape (proj_dim,)\n",
    "    \"\"\"\n",
    "    if len(K_vecs) == 0:\n",
    "        return V_vecs[0] if len(V_vecs) > 0 else jnp.zeros_like(Q_vec)\n",
    "\n",
    "    K_mat = jnp.stack(K_vecs)  # shape (n_past, proj_dim)\n",
    "    V_mat = jnp.stack(V_vecs)  # shape (n_past, proj_dim)\n",
    "    # dot-product softmax (fully differentiable)\n",
    "    scores = jnp.dot(K_mat, Q_vec)  # shape (n_past,)\n",
    "    if scale:\n",
    "        scores = scores / jnp.sqrt(Q_vec.shape[0])\n",
    "    # Causal mask: only past tokens (K_vecs are already past tokens)\n",
    "    # Adding -inf makes the softmax probability of pad tokens zero.\n",
    "    scores = scores + mask\n",
    "    weights = jax.nn.softmax(scores)\n",
    "\n",
    "    # Weighted sum over classical V\n",
    "    output = jnp.dot(weights, V_mat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and qubit setup\n",
    "# BITS_PER_TOKEN number of qubits needed to encode each token\n",
    "n_prop_qubits = 3  # number of qubits needed to encode properties (logP, QED, MW)\n",
    "n_ancillas = 1  # number of ancilla qubits that represent the environment\n",
    "n_total_qubits = n_prop_qubits + BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "N_LAYERS = 2  # number of variational layers\n",
    "H_LOCAL = 2 # h_local sets the maximum number of qubits that can interact in each Z-string term of Σ\n",
    "\n",
    "\n",
    "# Name them explicitly\n",
    "prop_wires = [f\"prop_{i}\" for i in range(n_prop_qubits)]\n",
    "token_wires = [f\"token_{i}\" for i in range(BITS_PER_TOKEN)]\n",
    "ancilla_wires = [f\"ancilla_{i}\" for i in range(n_ancillas)]\n",
    "all_wires = prop_wires + token_wires + ancilla_wires\n",
    "\n",
    "#dev = qml.device(\"default.qubit\", wires=all_wires)\n",
    "dev = qml.device(\"lightning.qubit\", wires=all_wires)\n",
    "\n",
    "def molecular_property_encoder(props):\n",
    "    \"\"\"Encode continuous props on property qubits via RY rotations\"\"\"\n",
    "    for wire, val in zip(prop_wires, props):\n",
    "        qml.RY(val, wires=wire)\n",
    "\n",
    "def token_encoder(token_bits):\n",
    "    \"\"\"Basis-encode token bits on token qubits\"\"\"\n",
    "    qml.BasisState(token_bits, wires=token_wires)\n",
    "\n",
    "\n",
    "def operator_layer(theta_params, theta_prop, wires):\n",
    "    \"\"\"\n",
    "    Variational layer where:\n",
    "      - theta_params[...] are rotations for token + ancilla qubits\n",
    "      - theta_prop encodes property→token entanglement\n",
    "    \"\"\"\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "\n",
    "    # Property → token entanglement \n",
    "    for p, prop_wire in enumerate(prop_wires):\n",
    "        for t, t_a_wire in enumerate(token_ancilla_ws):\n",
    "            qml.CRX(theta_prop[p, t, 0], wires=[prop_wire, t_a_wire])\n",
    "            qml.CRY(theta_prop[p, t, 1], wires=[prop_wire, t_a_wire])\n",
    "\n",
    " \n",
    "    qml.StronglyEntanglingLayers(\n",
    "        weights=theta_params[None,:,:],  # shape: (n_token_ancilla, 3)\n",
    "        wires=token_ancilla_ws\n",
    "    )\n",
    "\n",
    "def Sigma_layer_vec(gamma_vec, token_ancilla_ws, time=1.0, combos=None):\n",
    "    \"\"\"\n",
    "    Diagonal multi-Z unitary Σ = exp(i * sum_s gamma_s * Z^{⊗|s|} * t)\n",
    "    using a flat parameter vector 'gamma_vec' aligned with 'combos'.\n",
    "    \"\"\"\n",
    "    #token_ancilla_ws = list(wires)  # pass token+ancilla here\n",
    "    if combos is None:\n",
    "        combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    # Safety: ensure the vector length matches the number of combos\n",
    "    assert gamma_vec.shape[0] == len(combos), \\\n",
    "        f\"gamma_vec has length {gamma_vec.shape[0]} but expected {len(combos)}\"\n",
    "\n",
    "    # MultiRZ(phi) = exp(-i * phi/2 * Z^{⊗k}); choose phi = -2 * gamma * time\n",
    "    for gamma, combo in zip(gamma_vec, combos):\n",
    "        qml.MultiRZ(-2.0 * gamma * time, wires=list(combo))\n",
    "\n",
    "\n",
    "# QNode combining encoding and variational layers\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def autoregressive_model(token_bits, props, theta_params, theta_prop, sigma_params, output_i):\n",
    "    molecular_property_encoder(props)      # Encode MW, logP, QED\n",
    "    token_encoder(token_bits)              # Basis-encode token bits\n",
    "\n",
    "    # --- Encode output_i embedding safely ---\n",
    "    for i, val in enumerate(output_i):\n",
    "        qml.RY(val, wires=token_wires[i])\n",
    "\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "    combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    for l in range(N_LAYERS):\n",
    "        # Forward V(θ)\n",
    "        operator_layer(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "\n",
    "        # Diagonal Σ(γ,t): vector API\n",
    "        Sigma_layer_vec(sigma_params[l], token_ancilla_ws, time=1.0, combos=combos)\n",
    "\n",
    "        # Backward V(θ)†\n",
    "        # qml.adjoint(operator_layer)(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "\n",
    "    return qml.probs(wires=token_wires), [qml.expval(qml.PauliZ(w)) for w in prop_wires]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_crossentropy(pred_probs, target_index):\n",
    "    epsilon = 1e-10\n",
    "    return -jnp.log(pred_probs[target_index] + epsilon)\n",
    "\n",
    "def total_loss_fn(pred_probs, prop_expvals, target_index, props, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"Cross-entropy loss with label smoothing, normalized to [0,1].\"\"\"\n",
    "    num_classes = pred_probs.shape[0] # Number of classes (tokens)\n",
    "    \n",
    "    # Build smoothed target\n",
    "    smooth_target = jnp.full_like(pred_probs, epsilon / (num_classes - 1))\n",
    "    smooth_target = smooth_target.at[target_index].set(1.0 - epsilon)\n",
    "    \n",
    "    # --- Compute cross-entropy loss\n",
    "    ce_loss = -jnp.sum(smooth_target * jnp.log(pred_probs + 1e-10))\n",
    "    \n",
    "    # --- Property preservation loss (MSE) ---\n",
    "    prop_expvals = jnp.array(prop_expvals)  # convert list -> JAX array\n",
    "    prop_loss = jnp.mean((prop_expvals - jnp.cos(props)) ** 2)  # in [0,4]\n",
    "\n",
    "    # --- Combine ---\n",
    "    combined_loss = ce_loss + alpha * prop_loss\n",
    "\n",
    "    # --- Normalize only once ---\n",
    "    max_loss = jnp.log(num_classes) + alpha * 4.0\n",
    "    final_loss = combined_loss / max_loss\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def compute_accuracy(pred_probs, target_index):\n",
    "    predicted_index = jnp.argmax(pred_probs)\n",
    "    return jnp.array(predicted_index == target_index, dtype=jnp.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token embedding\n",
    "key = jax.random.PRNGKey(42)\n",
    "EMBEDDING_SIZE = BITS_PER_TOKEN + n_ancillas      # size of embeddings\n",
    "key, k_emb = jax.random.split(key)\n",
    "embedding_table = jax.random.normal(k_emb, (VOCABULARY_SIZE, EMBEDDING_SIZE)) * 0.1\n",
    "\n",
    "# Projection matrices\n",
    "key = jax.random.PRNGKey(42)\n",
    "proj_dim = BITS_PER_TOKEN  # number of qubits for quantum attention\n",
    "key, k_WQ, k_WK, k_WV = jax.random.split(key, 4)\n",
    "W_Q = jax.random.normal(k_WQ, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_K = jax.random.normal(k_WK, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_V = jax.random.normal(k_WV, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "\n",
    "\n",
    "# Effective qubit counts in variational layers\n",
    "n_token_ancilla = BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "# Initialize theta and sigma params\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, k_theta, k_theta_prop, k_sigma = jax.random.split(key, 4)\n",
    "\n",
    "# Precompute Z-string combos once\n",
    "token_ancilla_ws = token_wires + ancilla_wires\n",
    "combos = zstring_combos(token_ancilla_ws)\n",
    "n_strings = len(combos)\n",
    "\n",
    "# Combine all trainable parameters into a single dictionary\n",
    "combined_params = {\n",
    "    'theta': jax.random.normal(k_theta, (N_LAYERS, n_token_ancilla, 3)) * 0.1,\n",
    "    'theta_prop': jax.random.normal(k_theta_prop, (N_LAYERS, n_prop_qubits, n_token_ancilla, 4)) * 0.1,\n",
    "    'sigma': jax.random.normal(k_sigma, (N_LAYERS, n_strings)) * 0.1,\n",
    "    'embedding_table': embedding_table,\n",
    "    'W_Q': W_Q,\n",
    "    'W_K': W_K,\n",
    "    'W_V': W_V\n",
    "}\n",
    "# Training hyperparams\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(combined_params)\n",
    "\n",
    "\n",
    "PAD_index = token_to_index(\"<PAD>\")\n",
    "SOS_index = token_to_index(\"<SOS>\")\n",
    "EOS_index = token_to_index(\"<EOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def training_step(params, opt_state, x_token, x_props, y_target, past_token_indices=None):\n",
    "    \"\"\"\n",
    "    Performs a single, JIT-compiled training step.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of all model parameters.\n",
    "        opt_state: Current state of the optimizer.\n",
    "        x_token: JAX array for the current token, shape (BITS_PER_TOKEN,)\n",
    "        x_props: JAX array for molecular properties, shape (3,)\n",
    "        y_target: JAX array for the target token, shape (BITS_PER_TOKEN,)\n",
    "        past_token_indices: JAX array of *fixed size*, shape (N_PAST,)\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        # --- Unpack parameters ---\n",
    "        theta_params = params['theta']\n",
    "        theta_prop = params['theta_prop']\n",
    "        sigma_params = params['sigma']\n",
    "        embedding_table = params['embedding_table']\n",
    "        W_Q = params['W_Q']\n",
    "        W_K = params['W_K']\n",
    "        W_V = params['W_V']\n",
    "\n",
    "        # --- Embedding lookup ---\n",
    "        #start = time.time()\n",
    "        token_index = bits_to_index(x_token)\n",
    "        x_i = embedding_table[token_index]\n",
    "        #print(\"Embedding lookup:\", time.time() - start, \"seconds\")\n",
    "        \n",
    "        # Positional encoding (sin/cos)\n",
    "        # We calculate position by counting non-pad tokens\n",
    "        position = jnp.sum(past_token_indices != PAD_index)\n",
    "        dim_indices = jnp.arange(EMBEDDING_SIZE)\n",
    "        pos_enc = jnp.where(\n",
    "            dim_indices % 2 == 0,\n",
    "            jnp.sin(position / (10000 ** (dim_indices / EMBEDDING_SIZE))),\n",
    "            jnp.cos(position / (10000 ** ((dim_indices-1) / EMBEDDING_SIZE)))\n",
    "        )\n",
    "\n",
    "        # Combine token embedding + positional encoding\n",
    "        x_i_pos = x_i + pos_enc\n",
    "\n",
    "        # --- Q projection for current token ---\n",
    "        #start = time.time()\n",
    "        Q_i = x_i_pos @ W_Q\n",
    "        \n",
    "        # --- Parallel K, V projections for past tokens (JIT-friendly) ---\n",
    "        \n",
    "        # Use jax.vmap to apply the embedding lookup and projection\n",
    "        # in parallel across the (N_PAST,) array. This is extremely fast.\n",
    "        past_embeddings = jax.vmap(lambda idx: embedding_table[idx])(past_token_indices)\n",
    "        \n",
    "        K_vecs = jax.vmap(lambda x: x @ W_K)(past_embeddings) # shape (N_PAST, proj_dim)\n",
    "        V_vecs = jax.vmap(lambda x: x @ W_V)(past_embeddings) # shape (N_PAST, proj_dim)\n",
    "\n",
    "        # --- Create padding mask ---\n",
    "        # Create a mask of 0.0 for real tokens and -inf for <PAD> tokens\n",
    "        mask = jnp.where(past_token_indices == PAD_index, -jnp.inf, 0.0)\n",
    "\n",
    "        # --- Classical Attention (JIT-friendly) ---\n",
    "        # We pass the mask to the attention function\n",
    "        output_i = classical_attention(Q_i, K_vecs, V_vecs, mask)\n",
    "\n",
    "        # --- Variational model ---\n",
    "        #start = time.time()\n",
    "        pred_probs, expval_props = autoregressive_model(\n",
    "            x_token, x_props, theta_params, theta_prop, sigma_params, output_i\n",
    "        )\n",
    "        #print(\"Variational model:\", time.time() - start, \"seconds\")\n",
    "\n",
    "        # --- Loss computation ---\n",
    "        target_index = bits_to_index(y_target)\n",
    "        # Return scalar loss for gradient computation\n",
    "        return total_loss_fn(pred_probs, expval_props, target_index, x_props), pred_probs\n",
    "\n",
    "    # value_and_grad computes both loss and grads in one pass\n",
    "    (loss, pred_probs), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    # --- Update parameters ---\n",
    "    #start = time.time()\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    #print(\"Update params:\", time.time() - start, \"seconds\")\n",
    "    \n",
    "    # --- Compute accuracy ---\n",
    "    target_index = bits_to_index(y_target)\n",
    "    acc = compute_accuracy(pred_probs, target_index)\n",
    "\n",
    "    return new_params, loss, opt_state, grads, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m past_token_indices \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m     25\u001b[0m     is_sos,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: jnp\u001b[38;5;241m.\u001b[39mfull((N_PAST,), PAD_index, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32), \u001b[38;5;66;03m# true_fun (reset)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: past_token_indices                               \u001b[38;5;66;03m# false_fun (keep)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Perform a training step\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m combined_params, loss, opt_state, grads, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_props\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_token_indices\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Update loss and accuracy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/pjit.py:292\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mno_tracing\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m    288\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info\u001b[38;5;241m.\u001b[39mfun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`jit`, but \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_tracing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, box_data,\n\u001b[0;32m--> 292\u001b[0m  executable, pgle_profiler) \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m maybe_fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m    295\u001b[0m     executable, out_tree, args_flat, out_flat, attrs_tracked, box_data,\n\u001b[1;32m    296\u001b[0m     jaxpr\u001b[38;5;241m.\u001b[39meffects, jaxpr\u001b[38;5;241m.\u001b[39mconsts, jit_info\u001b[38;5;241m.\u001b[39mabstracted_axes, pgle_profiler)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/pjit.py:153\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(core\u001b[38;5;241m.\u001b[39mfull_lower, args_flat)\n\u001b[1;32m    152\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_eval_args(args_flat)\n\u001b[0;32m--> 153\u001b[0m   out_flat, compiled, profiler \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m pjit_p\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs_flat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/pjit.py:1877\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[0m\n\u001b[1;32m   1869\u001b[0m     fingerprint \u001b[38;5;241m=\u001b[39m fingerprint\u001b[38;5;241m.\u001b[39mhex()\n\u001b[1;32m   1870\u001b[0m   distributed_debug_log((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning pjit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md function\u001b[39m\u001b[38;5;124m\"\u001b[39m, name),\n\u001b[1;32m   1871\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_shardings\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_shardings),\n\u001b[1;32m   1872\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_shardings\u001b[39m\u001b[38;5;124m\"\u001b[39m, out_shardings),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1875\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract args\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmap\u001b[39m(core\u001b[38;5;241m.\u001b[39mabstractify, args)),\n\u001b[1;32m   1876\u001b[0m                         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfingerprint\u001b[39m\u001b[38;5;124m\"\u001b[39m, fingerprint))\n\u001b[0;32m-> 1877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsafe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m, compiled, pgle_profiler\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/profiler.py:354\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1297\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_unordered_effects\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_host_callbacks):\n\u001b[1;32m   1296\u001b[0m   input_bufs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_tokens_to_inputs(input_bufs)\n\u001b[0;32m-> 1297\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1299\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m   result_token_bufs \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_prefix_into_single_device_arrays(\n\u001b[1;32m   1302\u001b[0m       \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects))\n\u001b[1;32m   1303\u001b[0m   sharded_runtime_token \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mconsume_token()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax.lax\n",
    "import time\n",
    "\n",
    "N_PAST = 10  # Define a fixed context window size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = total_acc = 0.0\n",
    "    # Initialize a fixed-size array for past indices\n",
    "    past_token_indices = jnp.full((N_PAST,), PAD_index, dtype=jnp.int32)\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for i, (x_token, x_props, y_target) in enumerate(dataset):\n",
    "        # Convert to JAX arrays\n",
    "        x_token = jnp.array(x_token, dtype=jnp.int32)\n",
    "        x_props = jnp.array(x_props, dtype=jnp.float32)\n",
    "        y_target = jnp.array(y_target, dtype=jnp.int32)\n",
    "        '''\n",
    "        if jnp.all(x_token == 0):\n",
    "            # If current token is <SOS>, reset past tokens\n",
    "            past_token_indices = jnp.full((N_PAST,), PAD_index, dtype=jnp.int32)\n",
    "        '''\n",
    "        # Check if current token is <SOS>\n",
    "        is_sos = (bits_to_index(x_token) == 0)\n",
    "        past_token_indices = jax.lax.cond(\n",
    "            is_sos,\n",
    "            lambda: jnp.full((N_PAST,), PAD_index, dtype=jnp.int32), # true_fun (reset)\n",
    "            lambda: past_token_indices                               # false_fun (keep)\n",
    "        )\n",
    "\n",
    "        # Perform a training step\n",
    "        combined_params, loss, opt_state, grads, acc = training_step(\n",
    "            combined_params, \n",
    "            opt_state, \n",
    "            x_token, \n",
    "            x_props, \n",
    "            y_target, \n",
    "            past_token_indices\n",
    "        )\n",
    "\n",
    "        # Update loss and accuracy\n",
    "        total_loss += loss\n",
    "        total_acc  += acc\n",
    "\n",
    "        # Update the history: roll and add the new token index\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        current_token_index = bits_to_index(x_token)\n",
    "        past_token_indices = jnp.roll(past_token_indices, shift=-1).at[-1].set(current_token_index)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    avg_acc  = total_acc / len(dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss = {avg_loss:.4f} | Accuracy = {avg_acc:.4f} | Time = {epoch_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 | Loss = 0.5735 | Accuracy = 0.4376\n",
    "Epoch 2 | Loss = 0.5484 | Accuracy = 0.4612\n",
    "Epoch 3 | Loss = 0.5449 | Accuracy = 0.4657\n",
    "Epoch 4 | Loss = 0.5455 | Accuracy = 0.4658\n",
    "Epoch 5 | Loss = 0.5433 | Accuracy = 0.4705\n",
    "Epoch 6 | Loss = 0.5404 | Accuracy = 0.4729\n",
    "Epoch 7 | Loss = 0.5363 | Accuracy = 0.4798\n",
    "Epoch 8 | Loss = 0.5343 | Accuracy = 0.4819\n",
    "Epoch 9 | Loss = 0.5319 | Accuracy = 0.4833\n",
    "Epoch 10 | Loss = 0.5321 | Accuracy = 0.4859"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
    "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
    "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.complex128'> requested in astype is not available, and will be truncated to dtype complex64. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
    "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
    "Epoch 1 | Loss = 0.6477 | Accuracy = 0.3118\n",
    "Epoch 2 | Loss = 0.5822 | Accuracy = 0.3786\n",
    "Epoch 3 | Loss = 0.5650 | Accuracy = 0.3998\n",
    "Epoch 4 | Loss = 0.5578 | Accuracy = 0.4121\n",
    "Epoch 5 | Loss = 0.5515 | Accuracy = 0.4201\n",
    "Epoch 6 | Loss = 0.5460 | Accuracy = 0.4265\n",
    "Epoch 7 | Loss = 0.5449 | Accuracy = 0.4255\n",
    "Epoch 8 | Loss = 0.5456 | Accuracy = 0.4230\n",
    "Epoch 9 | Loss = 0.5442 | Accuracy = 0.4215\n",
    "Epoch 10 | Loss = 0.5412 | Accuracy = 0.4266\n",
    "Epoch 11 | Loss = 0.5383 | Accuracy = 0.4258\n",
    "Epoch 12 | Loss = 0.5382 | Accuracy = 0.4179\n",
    "Epoch 13 | Loss = 0.5359 | Accuracy = 0.4187\n",
    "Epoch 14 | Loss = 0.5346 | Accuracy = 0.4224\n",
    "Epoch 15 | Loss = 0.5306 | Accuracy = 0.4264\n",
    "Epoch 16 | Loss = 0.5274 | Accuracy = 0.4250\n",
    "Epoch 17 | Loss = 0.5232 | Accuracy = 0.4365\n",
    "Epoch 18 | Loss = 0.5178 | Accuracy = 0.4450\n",
    "Epoch 19 | Loss = 0.5158 | Accuracy = 0.4438\n",
    "Epoch 20 | Loss = 0.5136 | Accuracy = 0.4475\n",
    "Epoch 21 | Loss = 0.5134 | Accuracy = 0.4498\n",
    "Epoch 22 | Loss = 0.5122 | Accuracy = 0.4468\n",
    "Epoch 23 | Loss = 0.5103 | Accuracy = 0.4546\n",
    "Epoch 24 | Loss = 0.5092 | Accuracy = 0.4526\n",
    "Epoch 25 | Loss = 0.5074 | Accuracy = 0.4580\n",
    "Epoch 26 | Loss = 0.5094 | Accuracy = 0.4573\n",
    "Epoch 27 | Loss = 0.5053 | Accuracy = 0.4572\n",
    "Epoch 28 | Loss = 0.5036 | Accuracy = 0.4560\n",
    "Epoch 29 | Loss = 0.5023 | Accuracy = 0.4598\n",
    "Epoch 30 | Loss = 0.5006 | Accuracy = 0.4628\n",
    "Epoch 31 | Loss = 0.5003 | Accuracy = 0.4611\n",
    "Epoch 32 | Loss = 0.4995 | Accuracy = 0.4638\n",
    "Epoch 33 | Loss = 0.5002 | Accuracy = 0.4606\n",
    "Epoch 34 | Loss = 0.4991 | Accuracy = 0.4589\n",
    "Epoch 35 | Loss = 0.4979 | Accuracy = 0.4584\n",
    "Epoch 36 | Loss = 0.4965 | Accuracy = 0.4657\n",
    "Epoch 37 | Loss = 0.4954 | Accuracy = 0.4635\n",
    "Epoch 38 | Loss = 0.4958 | Accuracy = 0.4634\n",
    "Epoch 39 | Loss = 0.4948 | Accuracy = 0.4638\n",
    "Epoch 40 | Loss = 0.4946 | Accuracy = 0.4664\n",
    "Epoch 41 | Loss = 0.4945 | Accuracy = 0.4644\n",
    "Epoch 42 | Loss = 0.4948 | Accuracy = 0.4633\n",
    "Epoch 43 | Loss = 0.4935 | Accuracy = 0.4682\n",
    "Epoch 44 | Loss = 0.4941 | Accuracy = 0.4703\n",
    "Epoch 45 | Loss = 0.4945 | Accuracy = 0.4681\n",
    "Epoch 46 | Loss = 0.4930 | Accuracy = 0.4688\n",
    "Epoch 47 | Loss = 0.4925 | Accuracy = 0.4685\n",
    "Epoch 48 | Loss = 0.4925 | Accuracy = 0.4657\n",
    "Epoch 49 | Loss = 0.4922 | Accuracy = 0.4670\n",
    "Epoch 50 | Loss = 0.4912 | Accuracy = 0.4697\n",
    "Epoch 51 | Loss = 0.4910 | Accuracy = 0.4695\n",
    "Epoch 52 | Loss = 0.4908 | Accuracy = 0.4680\n",
    "Epoch 53 | Loss = 0.4905 | Accuracy = 0.4717\n",
    "Epoch 54 | Loss = 0.4905 | Accuracy = 0.4697\n",
    "Epoch 55 | Loss = 0.4916 | Accuracy = 0.4666\n",
    "Epoch 56 | Loss = 0.4899 | Accuracy = 0.4687\n",
    "Epoch 57 | Loss = 0.4920 | Accuracy = 0.4661\n",
    "Epoch 58 | Loss = 0.4910 | Accuracy = 0.4704\n",
    "Epoch 59 | Loss = 0.4913 | Accuracy = 0.4729\n",
    "Epoch 60 | Loss = 0.4913 | Accuracy = 0.4698\n",
    "Epoch 61 | Loss = 0.4912 | Accuracy = 0.4732\n",
    "Epoch 62 | Loss = 0.4895 | Accuracy = 0.4721\n",
    "Epoch 63 | Loss = 0.4901 | Accuracy = 0.4688\n",
    "Epoch 64 | Loss = 0.4898 | Accuracy = 0.4704\n",
    "Epoch 65 | Loss = 0.4889 | Accuracy = 0.4740\n",
    "Epoch 66 | Loss = 0.4883 | Accuracy = 0.4729\n",
    "Epoch 67 | Loss = 0.4888 | Accuracy = 0.4735\n",
    "Epoch 68 | Loss = 0.4886 | Accuracy = 0.4709\n",
    "Epoch 69 | Loss = 0.4888 | Accuracy = 0.4742\n",
    "Epoch 70 | Loss = 0.4884 | Accuracy = 0.4718\n",
    "Epoch 71 | Loss = 0.4884 | Accuracy = 0.4713\n",
    "Epoch 72 | Loss = 0.4880 | Accuracy = 0.4723\n",
    "Epoch 73 | Loss = 0.4888 | Accuracy = 0.4721\n",
    "Epoch 74 | Loss = 0.4894 | Accuracy = 0.4732\n",
    "Epoch 75 | Loss = 0.4939 | Accuracy = 0.4699\n",
    "Epoch 76 | Loss = 0.4911 | Accuracy = 0.4727\n",
    "Epoch 77 | Loss = 0.4895 | Accuracy = 0.4768\n",
    "Epoch 78 | Loss = 0.4901 | Accuracy = 0.4724\n",
    "Epoch 79 | Loss = 0.4889 | Accuracy = 0.4751\n",
    "Epoch 80 | Loss = 0.4892 | Accuracy = 0.4737\n",
    "Epoch 81 | Loss = 0.4883 | Accuracy = 0.4749\n",
    "Epoch 82 | Loss = 0.4878 | Accuracy = 0.4740\n",
    "Epoch 83 | Loss = 0.4877 | Accuracy = 0.4738\n",
    "Epoch 84 | Loss = 0.4878 | Accuracy = 0.4735\n",
    "Epoch 85 | Loss = 0.4874 | Accuracy = 0.4744\n",
    "Epoch 86 | Loss = 0.4884 | Accuracy = 0.4724\n",
    "Epoch 87 | Loss = 0.4884 | Accuracy = 0.4739\n",
    "Epoch 88 | Loss = 0.4875 | Accuracy = 0.4729\n",
    "Epoch 89 | Loss = 0.4872 | Accuracy = 0.4760\n",
    "Epoch 90 | Loss = 0.4869 | Accuracy = 0.4754\n",
    "Epoch 91 | Loss = 0.4870 | Accuracy = 0.4730\n",
    "Epoch 92 | Loss = 0.4879 | Accuracy = 0.4747\n",
    "Epoch 93 | Loss = 0.4873 | Accuracy = 0.4735\n",
    "Epoch 94 | Loss = 0.4874 | Accuracy = 0.4723\n",
    "Epoch 95 | Loss = 0.4869 | Accuracy = 0.4745\n",
    "Epoch 96 | Loss = 0.4864 | Accuracy = 0.4719\n",
    "Epoch 97 | Loss = 0.4862 | Accuracy = 0.4717\n",
    "Epoch 98 | Loss = 0.4864 | Accuracy = 0.4715\n",
    "Epoch 99 | Loss = 0.4860 | Accuracy = 0.4734\n",
    "Epoch 100 | Loss = 0.4864 | Accuracy = 0.4737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define the directory path (relative to your current location, assuming you are in /content/QGen-Mol/code)\n",
    "target_dir = '../data/params/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Now, run your pickle code:\n",
    "with open(os.path.join(target_dir, 'selfies_params.pkl'), \"wb\") as f:\n",
    "    pickle.dump(combined_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "MAX_LEN = 32  # maximum length of generated SELFIES (including <SOS> and <EOS>)\n",
    "def generate_molecule_selfies_stochastic(key, props, combined_params, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates a molecule stochastically using Attention weights for context,\n",
    "    suitable for the SELFIES model.\n",
    "    \"\"\"\n",
    "    # --- 1. Parameter Unpacking (Match SELFIES/Attention model) ---\n",
    "    embedding_table = combined_params['embedding_table']\n",
    "    W_Q = combined_params['W_Q']\n",
    "    W_K = combined_params['W_K']\n",
    "    W_V = combined_params['W_V']\n",
    "    theta_params = combined_params['theta']\n",
    "    theta_prop = combined_params['theta_prop']\n",
    "    sigma_params = combined_params['sigma']\n",
    "    \n",
    "    generated_bits = []\n",
    "    past_token_indices = []\n",
    "    \n",
    "    # Start with <SOS>\n",
    "    current_token_index = SOS_index\n",
    "    # Create the <SOS> bit array (jnp.int32 array of shape (BITS_PER_TOKEN,))\n",
    "    current_token_bits = jnp.array(list(map(int, format(SOS_index, f'0{BITS_PER_TOKEN}b'))), dtype=jnp.int32)\n",
    "    \n",
    "    # Store the local RNG state\n",
    "    local_rng = key \n",
    "    \n",
    "    # --- 2. Generation Loop ---\n",
    "    for t in range(MAX_LEN):\n",
    "        # Split the key for inner-loop stochasticity\n",
    "        local_rng, subkey = random.split(local_rng)\n",
    "\n",
    "        # Map current token bits to index and lookup embedding\n",
    "        x_token = current_token_bits \n",
    "        current_token_index = bits_to_index(x_token)\n",
    "        x_i = embedding_table[current_token_index]\n",
    "\n",
    "        # Apply Positional Encoding\n",
    "        position = len(past_token_indices)\n",
    "        dim_indices = jnp.arange(EMBEDDING_SIZE)\n",
    "        pos_enc = jnp.where(\n",
    "            dim_indices % 2 == 0,\n",
    "            jnp.sin(position / (10000 ** (dim_indices / EMBEDDING_SIZE))),\n",
    "            jnp.cos(position / (10000 ** ((dim_indices-1) / EMBEDDING_SIZE)))\n",
    "        )\n",
    "        x_i_pos = x_i + pos_enc\n",
    "\n",
    "        # --- Attention Calculation (Replaces theta-Embedding MLP) ---\n",
    "        Q_i = x_i_pos @ W_Q\n",
    "\n",
    "        if len(past_token_indices) == 0:\n",
    "            # First token (after <SOS>), no past context yet\n",
    "            output_i = x_i_pos @ W_V\n",
    "        else:\n",
    "            # Calculate K and V vectors for all past tokens\n",
    "            past_embeddings = [embedding_table[idx] for idx in past_token_indices]\n",
    "            K_vecs = [x @ W_K for x in past_embeddings]\n",
    "            V_vecs = [x @ W_V for x in past_embeddings]\n",
    "            output_i = classical_attention(Q_i, K_vecs, V_vecs)\n",
    "        # -------------------------------------------------------------\n",
    "        \n",
    "        # 3. Predict probabilities (theta_effective is just theta_params here)\n",
    "        # Note: Your SELFIES training used constant theta_params (no dynamic embedding).\n",
    "        # We pass the Attention output as the final parameter, output_i.\n",
    "        pred_probs, _ = autoregressive_model(x_token, props, theta_params, theta_prop, sigma_params, output_i)\n",
    "        \n",
    "        # 4. Tempering and Sampling\n",
    "        # Ensure we only consider probabilities for valid tokens (0 to 41)\n",
    "        logits = jnp.log(pred_probs[:VOCABULARY_SIZE] + 1e-10)\n",
    "\n",
    "        # Scale logits by Temperature\n",
    "        tempered_logits = logits / temperature\n",
    "\n",
    "        # Apply softmax to get the new, tempered probability distribution\n",
    "        tempered_probs = nn.softmax(tempered_logits)\n",
    "        \n",
    "        # Sample the next index from the Tempered probability distribution\n",
    "        token_indices = jnp.arange(VOCABULARY_SIZE)\n",
    "        next_index = random.choice(\n",
    "            subkey, \n",
    "            token_indices, \n",
    "            p=tempered_probs \n",
    "        )\n",
    "\n",
    "        # Convert index to bits \n",
    "        next_bits_str = format(int(next_index), f'0{BITS_PER_TOKEN}b')\n",
    "        next_bits = jnp.array([int(b) for b in next_bits_str], dtype=jnp.int32)\n",
    "\n",
    "        # 5. Check for <EOS>\n",
    "        if int(next_index) == EOS_index:\n",
    "            break\n",
    "\n",
    "        # Update previous tokens for the next step\n",
    "        generated_bits.append(next_bits)\n",
    "        past_token_indices.append(current_token_index) # Store index of the token just consumed\n",
    "        current_token_bits = next_bits # Set the new token for the next iteration\n",
    "\n",
    "    return generated_bits\n",
    "\n",
    "# Convert generated bits to SELFIES string (adjusted for SELFIES alphabet)\n",
    "def bits_to_selfies_smiles(generated_bits):\n",
    "    selfies_tokens = []\n",
    "    for bits in generated_bits:\n",
    "        index = int(\"\".join(map(str, bits)), 2)\n",
    "        \n",
    "        # Safety clip, though the sampling process above should enforce this\n",
    "        if index >= VOCABULARY_SIZE or index == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "        token = alphabet[int(index)]\n",
    "        if token == '<EOS>':\n",
    "            break\n",
    "        selfies_tokens.append(token)\n",
    "    \n",
    "    selfies_str = ''.join(selfies_tokens)\n",
    "    smiles_str = sf.decoder(selfies_str)\n",
    "    return selfies_str, smiles_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Properties (Normalized to [0, pi]):\n",
      "   LogP: 1.574, QED: 1.571, MW: 1.577\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n"
     ]
    }
   ],
   "source": [
    "N_MOLECS = 5\n",
    "MASTER_KEY = jr.PRNGKey(42)  # Fixed key for reproducibility\n",
    "\n",
    "# Target properties (mid-range example normalized to [0, pi])\n",
    "desired_logp = 1.2\n",
    "desired_qed = 0.71\n",
    "desired_mw = 205.0\n",
    "\n",
    "norm_logp = normalize(desired_logp, min_logp, max_logp)\n",
    "norm_qed = normalize(desired_qed, min_qed, max_qed)\n",
    "norm_mw = normalize(desired_mw, min_mw, max_mw)\n",
    "desired_props = jnp.array([norm_logp, norm_qed, norm_mw], dtype=jnp.float32)\n",
    "\n",
    "print(f\"Target Properties (Normalized to [0, pi]):\")\n",
    "print(f\"   LogP: {norm_logp:.3f}, QED: {norm_qed:.3f}, MW: {norm_mw:.3f}\\n\")\n",
    "\n",
    "\n",
    "def generate_molecules(props, params):\n",
    "    selfies_list = []\n",
    "    smiles_list = []\n",
    "    keys = jr.split(MASTER_KEY, N_MOLECS)\n",
    "    for i in range(N_MOLECS):\n",
    "        rng_i = keys[i]\n",
    "        generated_bits = generate_molecule_selfies_stochastic(rng_i, props, params)\n",
    "        generated_selfies, generated_smiles = bits_to_selfies_smiles(generated_bits)\n",
    "        selfies_list.append(generated_selfies)\n",
    "        smiles_list.append(generated_smiles)\n",
    "    return selfies_list, smiles_list\n",
    "\n",
    "selfies_list, smiles_list = generate_molecules(desired_props, combined_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target Properties ---\n",
      "LogP: 1.20\n",
      "QED: 0.71\n",
      "MW: 205.00 g/mol\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/bmwl8d950jz75tsw3l1xlnm80000gn/T/ipykernel_18247/3349772234.py:63: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_styled = df_styled.apply(pd.to_numeric, errors='ignore').round(2)\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import QED\n",
    "\n",
    "def analyze_molecule_properties(selfies_list, smiles_list, target_logp, target_qed, target_mw):\n",
    "    \"\"\"Calculates and prints the physicochemical properties for generated molecules,\n",
    "       comparing them directly against the denormalized targets.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # --- Print Denormalized Target Properties ---\n",
    "    print(f\"\\n--- Target Properties ---\")\n",
    "    print(f\"LogP: {target_logp:.2f}\")\n",
    "    print(f\"QED: {target_qed:.2f}\")\n",
    "    print(f\"MW: {target_mw:.2f} g/mol\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for i, (smiles, selfies) in enumerate(zip(smiles_list, selfies_list)):\n",
    "        # Handle empty/invalid SMILES from generation failure\n",
    "        if not smiles:\n",
    "            results.append({\"Molecule\": i+1, \"SMILES\": \"N/A\", \"LogP\": np.nan, \"QED\": np.nan, \"MW\": np.nan})\n",
    "            continue\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        if mol is not None:\n",
    "            try:\n",
    "                logp = Descriptors.MolLogP(mol)\n",
    "                qed_score = QED.qed(mol)\n",
    "                mw = Descriptors.ExactMolWt(mol)\n",
    "\n",
    "                results.append({\n",
    "                    \"Molecule\": i+1,\n",
    "                    \"SELFIES\": selfies,\n",
    "                    \"SMILES\": smiles,\n",
    "                    \"LogP\": logp,\n",
    "                    \"QED\": qed_score,\n",
    "                    \"MW\": mw,\n",
    "                    \"Validity\": \"Valid\"\n",
    "                })\n",
    "            except Exception:\n",
    "                 results.append({\"Molecule\": i+1, \"SELFIES\": selfies, \"SMILES\": smiles, \"LogP\": np.nan, \"QED\": np.nan, \"MW\": np.nan, \"Validity\": \"Error\"})\n",
    "        else:\n",
    "            results.append({\"Molecule\": i+1, \"SELFIES\": selfies, \"SMILES\": smiles, \"LogP\": np.nan, \"QED\": np.nan, \"MW\": np.nan, \"Validity\": \"Invalid\"})\n",
    "\n",
    "    # Create and display the DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add a row for the target properties for easy comparison\n",
    "    target_row = pd.Series({\n",
    "        \"Molecule\": \"TARGET\", \n",
    "        \"SELFIES\": \"TARGET\",\n",
    "        \"SMILES\": \"TARGET\", \n",
    "        \"LogP\": target_logp, \n",
    "        \"QED\": target_qed, \n",
    "        \"MW\": target_mw\n",
    "    }, name=\"TARGET\").to_frame().T\n",
    "    \n",
    "    # Concatenate the target row and the results for visual comparison\n",
    "    df_styled = pd.concat([target_row.set_index('Molecule'), df.set_index('Molecule')])\n",
    "    \n",
    "    # Format numerical columns for presentation\n",
    "    df_styled = df_styled.apply(pd.to_numeric, errors='ignore').round(2)\n",
    "    \n",
    "    return df_styled\n",
    "\n",
    "df_styled = analyze_molecule_properties(selfies_list, smiles_list, desired_logp, desired_qed, desired_mw)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_csv_path = \"../generation/generated_selfies.csv\"\n",
    "df_styled.to_csv(output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
