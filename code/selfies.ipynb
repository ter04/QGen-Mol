{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import selfies as sf\n",
    "\n",
    "import numpy as np\n",
    "from math import ceil, log2\n",
    "import re\n",
    "import pandas as pd\n",
    "import optax\n",
    "import csv\n",
    "import flax.linen as nn\n",
    "import math\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import qchem\n",
    "from pennylane.templates import StronglyEntanglingLayers\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn.initializers import normal\n",
    "\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training molecules set:  65778\n"
     ]
    }
   ],
   "source": [
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "# Using the ChEMBL API to get the molecules dataset\n",
    "\n",
    "molecule = new_client.molecule\n",
    "\n",
    "# Filter for drug-like small molecules interesting for human use\n",
    "druglike_molecules = molecule.filter(\n",
    "    molecule_properties__heavy_atoms__lte=15,           # Heavy atoms less than 20\n",
    "    molecule_properties__alogp__lte=5,                  # LogP less than 5 (Lipophilicity and membrane permeability)\n",
    "    molecule_properties__mw_freebase__lte=300,          # Molecular weight less than 300 g/mol\n",
    "    molecule_properties__qed_weighted__gte=0.5,         # QED weighted greater than 0.5 (Drug-likeness)\n",
    "    molecule_properties__num_ro5_violations__lte=1,     # At most 1 Rule of 5 violation (Drug-likeness filter)\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Training molecules set: \", len(druglike_molecules))  # Check how many molecules match the filter criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the alphabet used for the SMILEs representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the alphabet considering the structure of some special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subset of molecules we are going to train the model with\n",
    "molecules_subset = druglike_molecules[:500]\n",
    "\n",
    "max_len = 0\n",
    "alphabet = set()\n",
    "# We use a subset of molecules to build the alphabet\n",
    "for mol in molecules_subset:  # Limiting to 1000 molecules for performance\n",
    "    smiles = mol.get('molecule_structures', {}).get('canonical_smiles')\n",
    "    selfies = sf.encoder(smiles)\n",
    "    if selfies:\n",
    "        # Skip if contains '.'\n",
    "        if \".\" in selfies:\n",
    "            continue\n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if max_len < len(tokens):\n",
    "            max_len = len(tokens)\n",
    "        alphabet.update(tokens)\n",
    "\n",
    "alphabet = sorted(alphabet)\n",
    "alphabet = ['<SOS>'] + alphabet  + ['<EOS>'] # Add Start-of-Secuence, End-of-secuence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet of SMILES characters: ['<SOS>', '[#Branch1]', '[#Branch2]', '[#C]', '[/C]', '[/N]', '[/S]', '[=Branch1]', '[=Branch2]', '[=C]', '[=N+1]', '[=N]', '[=O]', '[=P]', '[=Ring1]', '[=S]', '[Br]', '[Branch1]', '[Branch2]', '[C@@H1]', '[C@@]', '[C@H1]', '[C@]', '[C]', '[Cl]', '[F]', '[I]', '[N+1]', '[NH1]', '[N]', '[O-1]', '[O]', '[PH1]', '[P]', '[Ring1]', '[Ring2]', '[S+1]', '[S]', '[\\\\C]', '[\\\\Cl]', '[\\\\N]', '<EOS>']\n",
      "Total unique characters in SMILES: 42\n",
      "Maximum length of SMILES in dataset: 29\n",
      "Bits per token: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Alphabet of SELFIES characters:\", alphabet)\n",
    "\n",
    "VOCABULARY_SIZE = len(alphabet)\n",
    "BITS_PER_TOKEN = ceil(log2(VOCABULARY_SIZE))  # n bits por token\n",
    "\n",
    "print(\"Total unique characters in SELFIES:\", VOCABULARY_SIZE)\n",
    "print(\"Maximum length of SELFIES in dataset:\", max_len)\n",
    "print(\"Bits per token:\", BITS_PER_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tSímbolos de enlaces y paréntesis: #, (, ), /, \\, =\n",
    "\n",
    "•\tDígitos simples para cierres de anillos: '1', '2', '3', '4', '5'\n",
    "\n",
    "•\tÁtomos orgánicos y halógenos comunes, tanto mayúsculas (alifáticos) como minúsculas (aromáticos)\n",
    "\n",
    "•\tTokens entre corchetes para isótopos, estados de carga, quiralidad, etc.\n",
    "\n",
    "•\tUn token especial '<PAD>' para padding en modelos ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<SOS>' → index 0 → 000000\n",
      "'[#Branch1]' → index 1 → 000001\n",
      "'[#Branch2]' → index 2 → 000010\n",
      "'[#C]' → index 3 → 000011\n",
      "'[/C]' → index 4 → 000100\n",
      "'[/N]' → index 5 → 000101\n",
      "'[/S]' → index 6 → 000110\n",
      "'[=Branch1]' → index 7 → 000111\n",
      "'[=Branch2]' → index 8 → 001000\n",
      "'[=C]' → index 9 → 001001\n",
      "'[=N+1]' → index 10 → 001010\n",
      "'[=N]' → index 11 → 001011\n",
      "'[=O]' → index 12 → 001100\n",
      "'[=P]' → index 13 → 001101\n",
      "'[=Ring1]' → index 14 → 001110\n",
      "'[=S]' → index 15 → 001111\n",
      "'[Br]' → index 16 → 010000\n",
      "'[Branch1]' → index 17 → 010001\n",
      "'[Branch2]' → index 18 → 010010\n",
      "'[C@@H1]' → index 19 → 010011\n",
      "'[C@@]' → index 20 → 010100\n",
      "'[C@H1]' → index 21 → 010101\n",
      "'[C@]' → index 22 → 010110\n",
      "'[C]' → index 23 → 010111\n",
      "'[Cl]' → index 24 → 011000\n",
      "'[F]' → index 25 → 011001\n",
      "'[I]' → index 26 → 011010\n",
      "'[N+1]' → index 27 → 011011\n",
      "'[NH1]' → index 28 → 011100\n",
      "'[N]' → index 29 → 011101\n",
      "'[O-1]' → index 30 → 011110\n",
      "'[O]' → index 31 → 011111\n",
      "'[PH1]' → index 32 → 100000\n",
      "'[P]' → index 33 → 100001\n",
      "'[Ring1]' → index 34 → 100010\n",
      "'[Ring2]' → index 35 → 100011\n",
      "'[S+1]' → index 36 → 100100\n",
      "'[S]' → index 37 → 100101\n",
      "'[\\C]' → index 38 → 100110\n",
      "'[\\Cl]' → index 39 → 100111\n",
      "'[\\N]' → index 40 → 101000\n",
      "'<EOS>' → index 41 → 101001\n"
     ]
    }
   ],
   "source": [
    "# Diccionario token → índice\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def print_token_bits(tokens, token_to_index):\n",
    "    for tok in tokens:\n",
    "        idx = token_to_index.get(tok, None)\n",
    "        if idx is None:\n",
    "            print(f\"Token '{tok}' no está en el diccionario.\")\n",
    "            continue\n",
    "        binary = format(idx, f'0{BITS_PER_TOKEN}b')\n",
    "        print(f\"'{tok}' → index {idx} → {binary}\")\n",
    "\n",
    "print_token_bits(alphabet, token_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input SMILEs must be the same size, so we need to use the padding to make it uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_encoded_dataset = []\n",
    "token_to_index = {tok: i for i, tok in enumerate(alphabet)}\n",
    "\n",
    "def smiles_to_bits(tokens: list) -> np.ndarray:\n",
    "    \"\"\"Convert tokens to a 2D array\"\"\"\n",
    "    padded_tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "    bit_matrix = []\n",
    "    for tok in padded_tokens:\n",
    "        idx = token_to_index[tok]\n",
    "        bits = list(f\"{idx:0{BITS_PER_TOKEN}b}\")  # length of the binary string depends on the number of bits required to represent the alphabet\n",
    "        bit_matrix.append([int(b) for b in bits])\n",
    "    return np.array(bit_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain the molecular properties of interest\n",
    "\n",
    "logP (o cx_logp) -> Coeficiente de partición octanol/agua (lipofilia)\n",
    "\n",
    "QED (quantitative estimate of drug-likeness) -> Escala combinada que evalúa qué tan “drug-like” es una molécula\n",
    "\n",
    "SAS (Synthetic Accessibility Score) -> Qué tan difícil sería sintetizar la molécula en laboratorio * Needs to be calculated separately!!\n",
    "\n",
    "MW (peso molecular) -> Masa total, típicamente ≤ 500 Da para buenos fármacos orales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the max and min range values in order to later normalize the properties in the range (0, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP range: -1.5 to 3.89\n",
      "QED range: 0.5 to 0.92\n",
      "MW range: 111.14 to 298.11\n"
     ]
    }
   ],
   "source": [
    "min_logp = float('inf')\n",
    "max_logp = float('-inf')\n",
    "min_qed = float('inf')\n",
    "max_qed = float('-inf')\n",
    "min_mw = float('inf')\n",
    "max_mw = float('-inf')\n",
    "\n",
    "\n",
    "# Iterate through the subset of molecules to find min/max properties to normalize them\n",
    "for mol in molecules_subset:\n",
    "    logP = mol.get('molecule_properties', {}).get('alogp')\n",
    "    qed = mol.get('molecule_properties', {}).get('qed_weighted')\n",
    "    mw = mol.get('molecule_properties', {}).get('mw_freebase')\n",
    "\n",
    "    if logP is None or qed is None or mw is None:\n",
    "        continue  # Skip if any property is missing\n",
    "\n",
    "    logP = float(logP)\n",
    "    qed = float(qed)\n",
    "    mw = float(mw)\n",
    "\n",
    "    if logP < min_logp:\n",
    "        min_logp = logP\n",
    "    if logP > max_logp:\n",
    "        max_logp = logP\n",
    "\n",
    "    if qed < min_qed:\n",
    "        min_qed = qed\n",
    "    if qed > max_qed:\n",
    "        max_qed = qed\n",
    "\n",
    "    if mw < min_mw:\n",
    "        min_mw = mw\n",
    "    if mw > max_mw:\n",
    "        max_mw = mw\n",
    "\n",
    "print(f\"LogP range: {min_logp} to {max_logp}\")\n",
    "print(f\"QED range: {min_qed} to {max_qed}\")\n",
    "print(f\"MW range: {min_mw} to {max_mw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_val, max_val, target_max=np.pi):\n",
    "    ''' Normalize a value to a range [0, [0, pi] to later encode them as rotation angles'''\n",
    "    norm = (value - min_val) / (max_val - min_val) * target_max\n",
    "    return float(f\"{norm:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequences in the subset: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum length of sequences in the subset:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the structured data to a CSV file\n",
    "DATA_PATH = \"../data/structured_data_selfies.csv\"\n",
    "with open(DATA_PATH, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    n_tokens = max_len + 2  # +2 for <SOS> and <EOS>\n",
    "    header = [\"logP\", \"qed\", \"mw\"] + [f\"token_{i}\" for i in range(n_tokens)]\n",
    "    writer.writerow(header)\n",
    "\n",
    "\n",
    "    for mol in molecules_subset:\n",
    "        smiles = mol.get('molecule_structures', {}).get('canonical_smiles')\n",
    "        selfies = sf.encoder(smiles)\n",
    "        props = mol.get('molecule_properties', {})\n",
    "        if not selfies:\n",
    "            continue\n",
    "        if \".\" in selfies:\n",
    "            continue\n",
    "        try:\n",
    "            logP = float(props.get('alogp'))\n",
    "            qed = float(props.get('qed_weighted'))\n",
    "            mw = float(props.get('mw_freebase'))\n",
    "        except (TypeError, ValueError):\n",
    "            continue\n",
    "\n",
    "        norm_logp = normalize(logP, min_logp, max_logp)\n",
    "        norm_qed = normalize(qed, min_qed, max_qed)\n",
    "        norm_mw = normalize(mw, min_mw, max_mw)\n",
    "\n",
    "        tokens = list(sf.split_selfies(selfies))\n",
    "        if not all(tok in token_to_index for tok in tokens):\n",
    "            continue\n",
    "\n",
    "        bit_matrix = smiles_to_bits(tokens)  # shape (n_tokens, 6)\n",
    "        token_bits_as_strings = [\"\".join(map(str, row)) for row in bit_matrix]\n",
    "\n",
    "        row = [norm_logp, norm_qed, norm_mw] + token_bits_as_strings\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantum Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def zstring_combos(wires):\n",
    "    \"\"\"\n",
    "    Return an ordered list of wire-tuples for all Z-strings up to order H_LOCAL.\n",
    "    Order: all 1-local, then all 2-local, ..., up to H_LOCAL.\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    for k in range(1, H_LOCAL + 1):\n",
    "        L.extend(itertools.combinations(wires, k))\n",
    "    return [tuple(c) for c in L]\n",
    "\n",
    "def num_zstrings(n_wires):\n",
    "    \"\"\"Count how many Z-strings up to order H_LOCAL.\"\"\"\n",
    "    from math import comb\n",
    "    return sum(comb(n_wires, k) for k in range(1, H_LOCAL + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Attention mechanism using SWAP test\n",
    "\n",
    "# --- Device for attention ---\n",
    "n_past = 5\n",
    "attn_dev = qml.device(\"default.qubit\", wires=(BITS_PER_TOKEN+1)*n_past)\n",
    "\n",
    "@qml.qnode(attn_dev, interface=\"jax\")\n",
    "def quantum_attention_qnode(Q_vec, K_vecs):\n",
    "    \"\"\"\n",
    "    Q_vec: projected query vector of current token\n",
    "    K_vecs: list of projected key vectors for past tokens\n",
    "    Returns: attention scores ⟨q_i | k_j⟩ for each j\n",
    "    \"\"\"\n",
    "    n_tokens = len(K_vecs)\n",
    "    q_wires = list(range(BITS_PER_TOKEN))\n",
    "\n",
    "    # Encode Q and all K in parallel (different wire registers)\n",
    "    def encode_token(angles, wires):\n",
    "        # Use AngleEmbedding for compactness, then entangle\n",
    "        qml.templates.AngleEmbedding(angles, wires=wires, rotation=\"Y\")\n",
    "        for i in range(len(wires)-1):\n",
    "            qml.CNOT(wires=[wires[i], wires[i+1]])\n",
    "\n",
    "    # Encode Q\n",
    "    encode_token(Q_vec, wires=q_wires)\n",
    "\n",
    "    # Encode K_j\n",
    "    # Collect expectation values (one per K_j)\n",
    "    measurements = []\n",
    "    for j, K_j in enumerate(K_vecs):\n",
    "        start = (BITS_PER_TOKEN+1) + j*(BITS_PER_TOKEN+1)\n",
    "        k_wires = list(range(start, start+BITS_PER_TOKEN))\n",
    "        encode_token(K_j, k_wires)\n",
    "\n",
    "        # SWAP test\n",
    "        ancilla = start + BITS_PER_TOKEN\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "        for qw, kw in zip(q_wires, k_wires):\n",
    "            qml.CSWAP(wires=[ancilla, qw, kw])\n",
    "        qml.Hadamard(wires=ancilla)\n",
    "\n",
    "        measurements.append(qml.expval(qml.PauliZ(ancilla)))\n",
    "\n",
    "    # **Return as tuple** so PennyLane converts to JAX array\n",
    "    return tuple(measurements)\n",
    "\n",
    "\n",
    "def quantum_attention(Q_vec, K_vecs, V_vecs):\n",
    "    # Make non-traced (concrete) copies for the QNode\n",
    "    Q_safe = jax.lax.stop_gradient(Q_vec)\n",
    "    K_safe = [jax.lax.stop_gradient(k) for k in K_vecs]\n",
    "\n",
    "    raw_expvals = quantum_attention_qnode(Q_safe, K_safe)\n",
    "    raw_expvals = jnp.asarray(raw_expvals)\n",
    "    # Convert from expectation values (in [-1,1]) to probabilities [0,1]\n",
    "    overlaps = (1.0 - raw_expvals) / 2.0\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def classical_attention(Q_vec, K_vecs, V_vecs, scale=True):\n",
    "    # Q_vec: (proj_dim,) ; K_vecs: list of (proj_dim,) ; V_vecs: list of (proj_dim,)\n",
    "    if len(K_vecs) == 0:\n",
    "        return V_vecs[0] if len(V_vecs) > 0 else jnp.zeros_like(Q_vec)\n",
    "\n",
    "    K_mat = jnp.stack(K_vecs)  # shape (n_past, proj_dim)\n",
    "    # dot-product softmax (fully differentiable)\n",
    "    scores = jnp.dot(K_mat, Q_vec)  # shape (n_past,)\n",
    "    if scale:\n",
    "        scores = scores / jnp.sqrt(Q_vec.shape[0])\n",
    "    # Causal mask: only past tokens (K_vecs are already past tokens)\n",
    "    weights = jax.nn.softmax(scores)\n",
    "    # Weighted sum over classical V\n",
    "    output = jnp.sum(weights[:, None] * jnp.stack(V_vecs), axis=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and qubit setup\n",
    "# BITS_PER_TOKEN number of qubits needed to encode each token\n",
    "n_prop_qubits = 3  # number of qubits needed to encode properties (logP, QED, MW)\n",
    "n_ancillas = 3  # number of ancilla qubits that represent the environment\n",
    "n_total_qubits = n_prop_qubits + BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "N_LAYERS = 6  # number of variational layers\n",
    "H_LOCAL = 3 # h_local sets the maximum number of qubits that can interact in each Z-string term of Σ\n",
    "\n",
    "\n",
    "# Name them explicitly\n",
    "prop_wires = [f\"prop_{i}\" for i in range(n_prop_qubits)]\n",
    "token_wires = [f\"token_{i}\" for i in range(BITS_PER_TOKEN)]\n",
    "ancilla_wires = [f\"ancilla_{i}\" for i in range(n_ancillas)]\n",
    "all_wires = prop_wires + token_wires + ancilla_wires\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=all_wires)\n",
    "\n",
    "\n",
    "def molecular_property_encoder(props):\n",
    "    \"\"\"Encode continuous props on property qubits via RY rotations\"\"\"\n",
    "    for wire, val in zip(prop_wires, props):\n",
    "        qml.RY(val, wires=wire)\n",
    "\n",
    "def token_encoder(token_bits):\n",
    "    \"\"\"Basis-encode token bits on token qubits\"\"\"\n",
    "    qml.BasisState(token_bits, wires=token_wires)\n",
    "\n",
    "\n",
    "def operator_layer(theta_params, theta_prop, wires):\n",
    "    \"\"\"\n",
    "    Variational layer where:\n",
    "      - theta_params[...] are rotations for token + ancilla qubits\n",
    "      - theta_prop encodes property→token entanglement\n",
    "    \"\"\"\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "\n",
    "    # Property → token entanglement \n",
    "    for p, prop_wire in enumerate(prop_wires):\n",
    "        for t, t_a_wire in enumerate(token_ancilla_ws):\n",
    "            qml.CRX(theta_prop[p, t, 0], wires=[prop_wire, t_a_wire])\n",
    "            qml.CRY(theta_prop[p, t, 1], wires=[prop_wire, t_a_wire])\n",
    "\n",
    " \n",
    "    qml.StronglyEntanglingLayers(\n",
    "        weights=theta_params[None,:,:],  # shape: (n_token_ancilla, 3)\n",
    "        wires=token_ancilla_ws\n",
    "    )\n",
    "\n",
    "def Sigma_layer_vec(gamma_vec, token_ancilla_ws, time=1.0, combos=None):\n",
    "    \"\"\"\n",
    "    Diagonal multi-Z unitary Σ = exp(i * sum_s gamma_s * Z^{⊗|s|} * t)\n",
    "    using a flat parameter vector 'gamma_vec' aligned with 'combos'.\n",
    "    \"\"\"\n",
    "    #token_ancilla_ws = list(wires)  # pass token+ancilla here\n",
    "    if combos is None:\n",
    "        combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    # Safety: ensure the vector length matches the number of combos\n",
    "    assert gamma_vec.shape[0] == len(combos), \\\n",
    "        f\"gamma_vec has length {gamma_vec.shape[0]} but expected {len(combos)}\"\n",
    "\n",
    "    # MultiRZ(phi) = exp(-i * phi/2 * Z^{⊗k}); choose phi = -2 * gamma * time\n",
    "    for gamma, combo in zip(gamma_vec, combos):\n",
    "        qml.MultiRZ(-2.0 * gamma * time, wires=list(combo))\n",
    "\n",
    "\n",
    "# QNode combining encoding and variational layers\n",
    "@qml.qnode(dev, interface=\"jax\")\n",
    "def autoregressive_model(token_bits, props, theta_params, theta_prop, sigma_params, output_i):\n",
    "    molecular_property_encoder(props)      # Encode MW, logP, QED\n",
    "    token_encoder(token_bits)              # Basis-encode token bits\n",
    "\n",
    "    # --- Encode output_i embedding safely ---\n",
    "    for i, val in enumerate(output_i):\n",
    "        qml.RY(val, wires=token_wires[i])\n",
    "\n",
    "    token_ancilla_ws = token_wires + ancilla_wires\n",
    "    combos = zstring_combos(token_ancilla_ws)\n",
    "\n",
    "    for l in range(N_LAYERS):\n",
    "        # Forward V(θ)\n",
    "        operator_layer(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "\n",
    "        # Diagonal Σ(γ,t): vector API\n",
    "        Sigma_layer_vec(sigma_params[l], token_ancilla_ws, time=1.0, combos=combos)\n",
    "\n",
    "        # Backward V(θ)†\n",
    "        qml.adjoint(operator_layer)(theta_params[l], theta_prop[l], wires=all_wires)\n",
    "\n",
    "    return qml.probs(wires=token_wires), [qml.expval(qml.PauliZ(w)) for w in prop_wires]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitstr_to_array(bitstr):\n",
    "    \"\"\"Convert a string of bits (e.g., '010101') to a numpy float32 array.\"\"\"\n",
    "    return np.array([int(b) for b in bitstr], dtype=np.float32)\n",
    "\n",
    "def build_training_data(df):\n",
    "    \"\"\"\n",
    "    Build dataset tuples of (input_token_bits, molecular_properties, target_token_bits)\n",
    "    from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing molecular properties and token bit strings.\n",
    "        n_token_cols (int): Number of token columns in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (x_token: np.array, x_props: np.array, y_target: np.array)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract molecular properties as a numpy float32 array\n",
    "        props = [row['logP'], row['qed'], row['mw']]\n",
    "        x_props = np.array(props, dtype=np.float32)\n",
    "\n",
    "        tokens = row[3:]  # token columns after properties\n",
    "\n",
    "        # Iterate over token sequence to create input-target pairs\n",
    "        for i in range(len(tokens) - 1):\n",
    "            current_token = tokens.iloc[i]\n",
    "            next_token = tokens.iloc[i + 1]\n",
    "\n",
    "            # Skip missing or NaN tokens\n",
    "            if current_token is None or (isinstance(current_token, float) and math.isnan(current_token)):\n",
    "                continue\n",
    "            if next_token is None or (isinstance(next_token, float) and math.isnan(next_token)):\n",
    "                continue\n",
    "\n",
    "            # Convert token strings (e.g., '01011') to bit arrays\n",
    "            x_token = bitstr_to_array(current_token)\n",
    "            y_target = bitstr_to_array(next_token)\n",
    "\n",
    "            dataset.append((x_token, x_props, y_target))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "token_cols = [f\"token_{i}\" for i in range(n_tokens)]\n",
    "df = pd.read_csv(DATA_PATH, dtype={col: str for col in token_cols})\n",
    "dataset = build_training_data(df)  # Should return list/array of (x_token, x_props, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_index(bits):\n",
    "    powers = 2 ** jnp.arange(len(bits) - 1, -1, -1)\n",
    "    return jnp.dot(bits, powers).astype(jnp.int32)\n",
    "\n",
    "def categorical_crossentropy(pred_probs, target_index):\n",
    "    epsilon = 1e-10\n",
    "    return -jnp.log(pred_probs[target_index] + epsilon)\n",
    "\n",
    "def total_loss_fn(pred_probs, prop_expvals, target_index, props, alpha=0.1, epsilon=0.1):\n",
    "    \"\"\"Cross-entropy loss with label smoothing, normalized to [0,1].\"\"\"\n",
    "    num_classes = pred_probs.shape[0] # Number of classes (tokens)\n",
    "    \n",
    "    # Build smoothed target\n",
    "    smooth_target = jnp.full_like(pred_probs, epsilon / (num_classes - 1))\n",
    "    smooth_target = smooth_target.at[target_index].set(1.0 - epsilon)\n",
    "    \n",
    "    # --- Compute cross-entropy loss\n",
    "    ce_loss = -jnp.sum(smooth_target * jnp.log(pred_probs + 1e-10))\n",
    "    \n",
    "    # --- Property preservation loss (MSE) ---\n",
    "    prop_expvals = jnp.array(prop_expvals)  # convert list -> JAX array\n",
    "    prop_loss = jnp.mean((prop_expvals - jnp.cos(props)) ** 2)  # in [0,4]\n",
    "\n",
    "    # --- Combine ---\n",
    "    combined_loss = ce_loss + alpha * prop_loss\n",
    "\n",
    "    # --- Normalize only once ---\n",
    "    max_loss = jnp.log(num_classes) + alpha * 4.0\n",
    "    final_loss = combined_loss / max_loss\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def compute_accuracy(pred_probs, target_index):\n",
    "    predicted_index = jnp.argmax(pred_probs)\n",
    "    return jnp.array(predicted_index == target_index, dtype=jnp.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token embedding\n",
    "key = jax.random.PRNGKey(42)\n",
    "EMBEDDING_SIZE = BITS_PER_TOKEN + n_ancillas      # size of embeddings\n",
    "key, k_emb = jax.random.split(key)\n",
    "embedding_table = jax.random.normal(k_emb, (VOCABULARY_SIZE, EMBEDDING_SIZE)) * 0.1\n",
    "\n",
    "# Projection matrices\n",
    "key = jax.random.PRNGKey(42)\n",
    "proj_dim = BITS_PER_TOKEN  # number of qubits for quantum attention\n",
    "key, k_WQ, k_WK, k_WV = jax.random.split(key, 4)\n",
    "W_Q = jax.random.normal(k_WQ, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_K = jax.random.normal(k_WK, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "W_V = jax.random.normal(k_WV, (EMBEDDING_SIZE, proj_dim)) * 0.1\n",
    "\n",
    "\n",
    "# Effective qubit counts in variational layers\n",
    "n_token_ancilla = BITS_PER_TOKEN + n_ancillas\n",
    "\n",
    "# Initialize theta and sigma params\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, k_theta, k_theta_prop, k_sigma = jax.random.split(key, 4)\n",
    "\n",
    "# Precompute Z-string combos once\n",
    "token_ancilla_ws = token_wires + ancilla_wires\n",
    "combos = zstring_combos(token_ancilla_ws)\n",
    "n_strings = len(combos)\n",
    "\n",
    "# Combine all trainable parameters into a single dictionary\n",
    "combined_params = {\n",
    "    'theta': jax.random.normal(k_theta, (N_LAYERS, n_token_ancilla, 3)) * 0.1,\n",
    "    'theta_prop': jax.random.normal(k_theta_prop, (N_LAYERS, n_prop_qubits, n_token_ancilla, 4)) * 0.1,\n",
    "    'sigma': jax.random.normal(k_sigma, (N_LAYERS, n_strings)) * 0.1,\n",
    "    'embedding_table': embedding_table,\n",
    "    'W_Q': W_Q,\n",
    "    'W_K': W_K,\n",
    "    'W_V': W_V\n",
    "}\n",
    "# Training hyperparams\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(combined_params)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def training_step(params, opt_state, x_token, x_props, y_target, past_token_indices=None):\n",
    "    if past_token_indices is None:\n",
    "        past_token_indices = []\n",
    "\n",
    "    def loss_fn(params):\n",
    "        theta_params = params['theta']\n",
    "        theta_prop = params['theta_prop']\n",
    "        sigma_params = params['sigma']\n",
    "        embedding_table = params['embedding_table']\n",
    "        W_Q = params['W_Q']\n",
    "        W_K = params['W_K']\n",
    "        W_V = params['W_V']\n",
    "\n",
    "        # --- 1. Embedding lookup ---\n",
    "        #start = time.time()\n",
    "        token_index = bits_to_index(x_token)\n",
    "        x_i = embedding_table[token_index]\n",
    "        #print(\"Embedding lookup:\", time.time() - start, \"seconds\")\n",
    "        \n",
    "        # Positional encoding (sin/cos)\n",
    "        position = len(past_token_indices)   # current position in the sequence\n",
    "        dim_indices = jnp.arange(EMBEDDING_SIZE)\n",
    "        pos_enc = jnp.where(\n",
    "            dim_indices % 2 == 0,\n",
    "            jnp.sin(position / (10000 ** (dim_indices / EMBEDDING_SIZE))),\n",
    "            jnp.cos(position / (10000 ** ((dim_indices-1) / EMBEDDING_SIZE)))\n",
    "        )\n",
    "\n",
    "        # Combine token embedding + positional encoding\n",
    "        x_i_pos = x_i + pos_enc\n",
    "        # --- 2. Q/K/V projections ---\n",
    "        #start = time.time()\n",
    "        Q_i = x_i_pos @ W_Q\n",
    "        \n",
    "        if len(past_token_indices) == 0:\n",
    "            # No past tokens: use V projection of current token as output\n",
    "            output_i = x_i_pos @ W_V\n",
    "        else:\n",
    "            # Normal attention over past tokens\n",
    "            past_embeddings = [embedding_table[idx] for idx in past_token_indices]\n",
    "            K_vecs = [x @ W_K for x in past_embeddings]\n",
    "            V_vecs = [x @ W_V for x in past_embeddings]\n",
    "            # --- 3. Quantum attention ---\n",
    "            # Use classical attention during training (differentiable)\n",
    "            output_i = classical_attention(Q_i, K_vecs, V_vecs)\n",
    "        #print(\"Attention:\", time.time() - start, \"seconds\")\n",
    "\n",
    "        # --- 4. Variational model ---\n",
    "        # Before calling autoregressive_model, make output non-traced\n",
    "        #start = time.time()\n",
    "        pred_probs, expval_props = autoregressive_model(x_token, x_props, theta_params, theta_prop, sigma_params, output_i)\n",
    "        #print(\"Variational model:\", time.time() - start, \"seconds\")\n",
    "\n",
    "        target_index = bits_to_index(y_target)\n",
    "        # Return scalar loss for gradient computation\n",
    "        return total_loss_fn(pred_probs, expval_props, target_index, x_props), pred_probs\n",
    "\n",
    "    # value_and_grad computes both loss and grads in one pass\n",
    "    (loss, pred_probs), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    # Update parameters\n",
    "    #start = time.time()\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    #print(\"Update params:\", time.time() - start, \"seconds\")\n",
    "    \n",
    "    # Accuracy from pred_probs (already computed!)\n",
    "    target_index = bits_to_index(y_target)\n",
    "    acc = compute_accuracy(pred_probs, target_index)\n",
    "\n",
    "    return new_params, loss, opt_state, grads, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.complex128'> requested in astype is not available, and will be truncated to dtype complex64. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 0.6477 | Accuracy = 0.3118\n",
      "Epoch 2 | Loss = 0.5822 | Accuracy = 0.3786\n",
      "Epoch 3 | Loss = 0.5650 | Accuracy = 0.3998\n",
      "Epoch 4 | Loss = 0.5578 | Accuracy = 0.4121\n",
      "Epoch 5 | Loss = 0.5515 | Accuracy = 0.4201\n",
      "Epoch 6 | Loss = 0.5460 | Accuracy = 0.4265\n",
      "Epoch 7 | Loss = 0.5449 | Accuracy = 0.4255\n",
      "Epoch 8 | Loss = 0.5456 | Accuracy = 0.4230\n",
      "Epoch 9 | Loss = 0.5442 | Accuracy = 0.4215\n",
      "Epoch 10 | Loss = 0.5412 | Accuracy = 0.4266\n",
      "Epoch 11 | Loss = 0.5383 | Accuracy = 0.4258\n",
      "Epoch 12 | Loss = 0.5382 | Accuracy = 0.4179\n",
      "Epoch 13 | Loss = 0.5359 | Accuracy = 0.4187\n",
      "Epoch 14 | Loss = 0.5346 | Accuracy = 0.4224\n",
      "Epoch 15 | Loss = 0.5306 | Accuracy = 0.4264\n",
      "Epoch 16 | Loss = 0.5274 | Accuracy = 0.4250\n",
      "Epoch 17 | Loss = 0.5232 | Accuracy = 0.4365\n",
      "Epoch 18 | Loss = 0.5178 | Accuracy = 0.4450\n",
      "Epoch 19 | Loss = 0.5158 | Accuracy = 0.4438\n",
      "Epoch 20 | Loss = 0.5136 | Accuracy = 0.4475\n",
      "Epoch 21 | Loss = 0.5134 | Accuracy = 0.4498\n",
      "Epoch 22 | Loss = 0.5122 | Accuracy = 0.4468\n",
      "Epoch 23 | Loss = 0.5103 | Accuracy = 0.4546\n",
      "Epoch 24 | Loss = 0.5092 | Accuracy = 0.4526\n",
      "Epoch 25 | Loss = 0.5074 | Accuracy = 0.4580\n",
      "Epoch 26 | Loss = 0.5094 | Accuracy = 0.4573\n",
      "Epoch 27 | Loss = 0.5053 | Accuracy = 0.4572\n",
      "Epoch 28 | Loss = 0.5036 | Accuracy = 0.4560\n",
      "Epoch 29 | Loss = 0.5023 | Accuracy = 0.4598\n",
      "Epoch 30 | Loss = 0.5006 | Accuracy = 0.4628\n",
      "Epoch 31 | Loss = 0.5003 | Accuracy = 0.4611\n",
      "Epoch 32 | Loss = 0.4995 | Accuracy = 0.4638\n",
      "Epoch 33 | Loss = 0.5002 | Accuracy = 0.4606\n",
      "Epoch 34 | Loss = 0.4991 | Accuracy = 0.4589\n",
      "Epoch 35 | Loss = 0.4979 | Accuracy = 0.4584\n",
      "Epoch 36 | Loss = 0.4965 | Accuracy = 0.4657\n",
      "Epoch 37 | Loss = 0.4954 | Accuracy = 0.4635\n",
      "Epoch 38 | Loss = 0.4958 | Accuracy = 0.4634\n",
      "Epoch 39 | Loss = 0.4948 | Accuracy = 0.4638\n",
      "Epoch 40 | Loss = 0.4946 | Accuracy = 0.4664\n",
      "Epoch 41 | Loss = 0.4945 | Accuracy = 0.4644\n",
      "Epoch 42 | Loss = 0.4948 | Accuracy = 0.4633\n",
      "Epoch 43 | Loss = 0.4935 | Accuracy = 0.4682\n",
      "Epoch 44 | Loss = 0.4941 | Accuracy = 0.4703\n",
      "Epoch 45 | Loss = 0.4945 | Accuracy = 0.4681\n",
      "Epoch 46 | Loss = 0.4930 | Accuracy = 0.4688\n",
      "Epoch 47 | Loss = 0.4925 | Accuracy = 0.4685\n",
      "Epoch 48 | Loss = 0.4925 | Accuracy = 0.4657\n",
      "Epoch 49 | Loss = 0.4922 | Accuracy = 0.4670\n",
      "Epoch 50 | Loss = 0.4912 | Accuracy = 0.4697\n",
      "Epoch 51 | Loss = 0.4910 | Accuracy = 0.4695\n",
      "Epoch 52 | Loss = 0.4908 | Accuracy = 0.4680\n",
      "Epoch 53 | Loss = 0.4905 | Accuracy = 0.4717\n",
      "Epoch 54 | Loss = 0.4905 | Accuracy = 0.4697\n",
      "Epoch 55 | Loss = 0.4916 | Accuracy = 0.4666\n",
      "Epoch 56 | Loss = 0.4899 | Accuracy = 0.4687\n",
      "Epoch 57 | Loss = 0.4920 | Accuracy = 0.4661\n",
      "Epoch 58 | Loss = 0.4910 | Accuracy = 0.4704\n",
      "Epoch 59 | Loss = 0.4913 | Accuracy = 0.4729\n",
      "Epoch 60 | Loss = 0.4913 | Accuracy = 0.4698\n",
      "Epoch 61 | Loss = 0.4912 | Accuracy = 0.4732\n",
      "Epoch 62 | Loss = 0.4895 | Accuracy = 0.4721\n",
      "Epoch 63 | Loss = 0.4901 | Accuracy = 0.4688\n",
      "Epoch 64 | Loss = 0.4898 | Accuracy = 0.4704\n",
      "Epoch 65 | Loss = 0.4889 | Accuracy = 0.4740\n",
      "Epoch 66 | Loss = 0.4883 | Accuracy = 0.4729\n",
      "Epoch 67 | Loss = 0.4888 | Accuracy = 0.4735\n",
      "Epoch 68 | Loss = 0.4886 | Accuracy = 0.4709\n",
      "Epoch 69 | Loss = 0.4888 | Accuracy = 0.4742\n",
      "Epoch 70 | Loss = 0.4884 | Accuracy = 0.4718\n",
      "Epoch 71 | Loss = 0.4884 | Accuracy = 0.4713\n",
      "Epoch 72 | Loss = 0.4880 | Accuracy = 0.4723\n",
      "Epoch 73 | Loss = 0.4888 | Accuracy = 0.4721\n",
      "Epoch 74 | Loss = 0.4894 | Accuracy = 0.4732\n",
      "Epoch 75 | Loss = 0.4939 | Accuracy = 0.4699\n",
      "Epoch 76 | Loss = 0.4911 | Accuracy = 0.4727\n",
      "Epoch 77 | Loss = 0.4895 | Accuracy = 0.4768\n",
      "Epoch 78 | Loss = 0.4901 | Accuracy = 0.4724\n",
      "Epoch 79 | Loss = 0.4889 | Accuracy = 0.4751\n",
      "Epoch 80 | Loss = 0.4892 | Accuracy = 0.4737\n",
      "Epoch 81 | Loss = 0.4883 | Accuracy = 0.4749\n",
      "Epoch 82 | Loss = 0.4878 | Accuracy = 0.4740\n",
      "Epoch 83 | Loss = 0.4877 | Accuracy = 0.4738\n",
      "Epoch 84 | Loss = 0.4878 | Accuracy = 0.4735\n",
      "Epoch 85 | Loss = 0.4874 | Accuracy = 0.4744\n",
      "Epoch 86 | Loss = 0.4884 | Accuracy = 0.4724\n",
      "Epoch 87 | Loss = 0.4884 | Accuracy = 0.4739\n",
      "Epoch 88 | Loss = 0.4875 | Accuracy = 0.4729\n",
      "Epoch 89 | Loss = 0.4872 | Accuracy = 0.4760\n",
      "Epoch 90 | Loss = 0.4869 | Accuracy = 0.4754\n",
      "Epoch 91 | Loss = 0.4870 | Accuracy = 0.4730\n",
      "Epoch 92 | Loss = 0.4879 | Accuracy = 0.4747\n",
      "Epoch 93 | Loss = 0.4873 | Accuracy = 0.4735\n",
      "Epoch 94 | Loss = 0.4874 | Accuracy = 0.4723\n",
      "Epoch 95 | Loss = 0.4869 | Accuracy = 0.4745\n",
      "Epoch 96 | Loss = 0.4864 | Accuracy = 0.4719\n",
      "Epoch 97 | Loss = 0.4862 | Accuracy = 0.4717\n",
      "Epoch 98 | Loss = 0.4864 | Accuracy = 0.4715\n",
      "Epoch 99 | Loss = 0.4860 | Accuracy = 0.4734\n",
      "Epoch 100 | Loss = 0.4864 | Accuracy = 0.4737\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = total_acc = 0.0\n",
    "    past_token_indices = [] # Reset past tokens at the start of each epoch\n",
    "\n",
    "    for x_token, x_props, y_target in dataset:\n",
    "        x_token = jnp.array(x_token, dtype=jnp.int32)\n",
    "        x_props = jnp.array(x_props, dtype=jnp.float32)\n",
    "        y_target = jnp.array(y_target, dtype=jnp.int32)\n",
    "\n",
    "        if jnp.all(x_token == 0):\n",
    "            # If current token is <SOS>, reset past tokens\n",
    "            past_token_indices = []\n",
    "\n",
    "        combined_params, loss, opt_state, grads, acc = training_step(combined_params, opt_state, x_token, x_props, y_target, past_token_indices)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc  += acc\n",
    "\n",
    "        past_token_indices.append(bits_to_index(x_token))\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    avg_acc  = total_acc / len(dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss = {avg_loss:.4f} | Accuracy = {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define the directory path (relative to your current location, assuming you are in /content/QGen-Mol/code)\n",
    "target_dir = '../data/params/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Now, run your pickle code:\n",
    "with open(os.path.join(target_dir, 'selfies_params.pkl'), \"wb\") as f:\n",
    "    pickle.dump(combined_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "def generate_molecule_selfies_stochastic(key, props, combined_params, temperature=1.0, max_length=29):\n",
    "    \"\"\"\n",
    "    Generates a molecule stochastically using Attention weights for context,\n",
    "    suitable for the SELFIES model.\n",
    "    \"\"\"\n",
    "    # --- 1. Parameter Unpacking (Match SELFIES/Attention model) ---\n",
    "    embedding_table = combined_params['embedding_table']\n",
    "    W_Q = combined_params['W_Q']\n",
    "    W_K = combined_params['W_K']\n",
    "    W_V = combined_params['W_V']\n",
    "    theta_params = combined_params['theta']\n",
    "    theta_prop = combined_params['theta_prop']\n",
    "    sigma_params = combined_params['sigma']\n",
    "    \n",
    "    generated_bits = []\n",
    "    past_token_indices = []\n",
    "    \n",
    "    # Start with <SOS>\n",
    "    SOS_index = token_to_index['<SOS>']\n",
    "    current_token_index = SOS_index\n",
    "    # Create the <SOS> bit array (jnp.int32 array of shape (BITS_PER_TOKEN,))\n",
    "    current_token_bits = jnp.array(list(map(int, format(SOS_index, f'0{BITS_PER_TOKEN}b'))), dtype=jnp.int32)\n",
    "    \n",
    "    # Store the local RNG state\n",
    "    local_rng = key \n",
    "    \n",
    "    # --- 2. Generation Loop ---\n",
    "    for t in range(max_length):\n",
    "        # Split the key for inner-loop stochasticity\n",
    "        local_rng, subkey = random.split(local_rng)\n",
    "\n",
    "        # Map current token bits to index and lookup embedding\n",
    "        x_token = current_token_bits \n",
    "        current_token_index = bits_to_index(x_token)\n",
    "        x_i = embedding_table[current_token_index]\n",
    "\n",
    "        # Apply Positional Encoding\n",
    "        position = len(past_token_indices)\n",
    "        dim_indices = jnp.arange(EMBEDDING_SIZE)\n",
    "        pos_enc = jnp.where(\n",
    "            dim_indices % 2 == 0,\n",
    "            jnp.sin(position / (10000 ** (dim_indices / EMBEDDING_SIZE))),\n",
    "            jnp.cos(position / (10000 ** ((dim_indices-1) / EMBEDDING_SIZE)))\n",
    "        )\n",
    "        x_i_pos = x_i + pos_enc\n",
    "\n",
    "        # --- Attention Calculation (Replaces theta-Embedding MLP) ---\n",
    "        Q_i = x_i_pos @ W_Q\n",
    "\n",
    "        if len(past_token_indices) == 0:\n",
    "            # First token (after <SOS>), no past context yet\n",
    "            output_i = x_i_pos @ W_V\n",
    "        else:\n",
    "            # Calculate K and V vectors for all past tokens\n",
    "            past_embeddings = [embedding_table[idx] for idx in past_token_indices]\n",
    "            K_vecs = [x @ W_K for x in past_embeddings]\n",
    "            V_vecs = [x @ W_V for x in past_embeddings]\n",
    "            output_i = classical_attention(Q_i, K_vecs, V_vecs)\n",
    "        # -------------------------------------------------------------\n",
    "        \n",
    "        # 3. Predict probabilities (theta_effective is just theta_params here)\n",
    "        # Note: Your SELFIES training used constant theta_params (no dynamic embedding).\n",
    "        # We pass the Attention output as the final parameter, output_i.\n",
    "        pred_probs, _ = autoregressive_model(x_token, props, theta_params, theta_prop, sigma_params, output_i)\n",
    "        \n",
    "        # 4. Tempering and Sampling\n",
    "        # Ensure we only consider probabilities for valid tokens (0 to 41)\n",
    "        logits = jnp.log(pred_probs[:VOCABULARY_SIZE] + 1e-10)\n",
    "\n",
    "        # Scale logits by Temperature\n",
    "        tempered_logits = logits / temperature\n",
    "\n",
    "        # Apply softmax to get the new, tempered probability distribution\n",
    "        tempered_probs = nn.softmax(tempered_logits)\n",
    "        \n",
    "        # Sample the next index from the Tempered probability distribution\n",
    "        token_indices = jnp.arange(VOCABULARY_SIZE)\n",
    "        next_index = random.choice(\n",
    "            subkey, \n",
    "            token_indices, \n",
    "            p=tempered_probs \n",
    "        )\n",
    "\n",
    "        # Convert index to bits \n",
    "        next_bits_str = format(int(next_index), f'0{BITS_PER_TOKEN}b')\n",
    "        next_bits = jnp.array([int(b) for b in next_bits_str], dtype=jnp.int32)\n",
    "\n",
    "        # 5. Check for <EOS>\n",
    "        if int(next_index) == token_to_index['<EOS>']:\n",
    "            break\n",
    "\n",
    "        # Update previous tokens for the next step\n",
    "        generated_bits.append(next_bits)\n",
    "        past_token_indices.append(current_token_index) # Store index of the token just consumed\n",
    "        current_token_bits = next_bits # Set the new token for the next iteration\n",
    "\n",
    "    return generated_bits\n",
    "\n",
    "# Convert generated bits to SELFIES string (adjusted for SELFIES alphabet)\n",
    "def bits_to_selfies_smiles(generated_bits):\n",
    "    selfies_tokens = []\n",
    "    for bits in generated_bits:\n",
    "        index = int(\"\".join(map(str, bits)), 2)\n",
    "        \n",
    "        # Safety clip, though the sampling process above should enforce this\n",
    "        if index >= VOCABULARY_SIZE or index == 0:\n",
    "            continue\n",
    "\n",
    "        token = alphabet[int(index)]\n",
    "        if token == '<EOS>':\n",
    "            break\n",
    "        selfies_tokens.append(token)\n",
    "    \n",
    "    selfies_str = ''.join(selfies_tokens)\n",
    "    smiles_str = sf.decoder(selfies_str)\n",
    "    return selfies_str, smiles_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Properties (Normalized to [0, pi]):\n",
      "   LogP: 1.574, QED: 1.571, MW: 1.577\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ter/Apps/anaconda3/envs/tfm/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:122: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n"
     ]
    }
   ],
   "source": [
    "N_MOLECS = 50\n",
    "MASTER_KEY = jr.PRNGKey(50)  # Fixed key for reproducibility\n",
    "\n",
    "# Target properties (mid-range example normalized to [0, pi])\n",
    "desired_logp = 1.2\n",
    "desired_qed = 0.71\n",
    "desired_mw = 205.0\n",
    "\n",
    "norm_logp = normalize(desired_logp, min_logp, max_logp)\n",
    "norm_qed = normalize(desired_qed, min_qed, max_qed)\n",
    "norm_mw = normalize(desired_mw, min_mw, max_mw)\n",
    "desired_props = jnp.array([norm_logp, norm_qed, norm_mw], dtype=jnp.float32)\n",
    "\n",
    "print(f\"Target Properties (Normalized to [0, pi]):\")\n",
    "print(f\"   LogP: {norm_logp:.3f}, QED: {norm_qed:.3f}, MW: {norm_mw:.3f}\\n\")\n",
    "\n",
    "\n",
    "def generate_molecules(props, params):\n",
    "    selfies_list = []\n",
    "    smiles_list = []\n",
    "    keys = jr.split(MASTER_KEY, N_MOLECS)\n",
    "    for i in range(N_MOLECS):\n",
    "        rng_i = keys[i]\n",
    "        generated_bits = generate_molecule_selfies_stochastic(rng_i, props, params)\n",
    "        generated_selfies, generated_smiles = bits_to_selfies_smiles(generated_bits)\n",
    "        selfies_list.append(generated_selfies)\n",
    "        smiles_list.append(generated_smiles)\n",
    "    return selfies_list, smiles_list\n",
    "\n",
    "selfies_list, smiles_list = generate_molecules(desired_props, combined_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target Properties ---\n",
      "LogP: 1.20\n",
      "QED: 0.71\n",
      "MW: 205.00 g/mol\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/bmwl8d950jz75tsw3l1xlnm80000gn/T/ipykernel_18247/1474125880.py:60: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_styled = df_styled.apply(pd.to_numeric, errors='ignore').round(2)\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import QED\n",
    "\n",
    "def analyze_molecule_properties(selfies_list, smiles_list, target_logp, target_qed, target_mw):\n",
    "    \"\"\"Calculates and prints the physicochemical properties for generated molecules,\n",
    "       comparing them directly against the denormalized targets.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # --- Print Denormalized Target Properties ---\n",
    "    print(f\"\\n--- Target Properties ---\")\n",
    "    print(f\"LogP: {target_logp:.2f}\")\n",
    "    print(f\"QED: {target_qed:.2f}\")\n",
    "    print(f\"MW: {target_mw:.2f} g/mol\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for i, (smiles, selfies) in enumerate(zip(smiles_list, selfies_list)):\n",
    "        # Handle empty/invalid SMILES from generation failure\n",
    "        if not smiles:\n",
    "            results.append({\"Molecule\": i+1, \"SMILES\": \"N/A\", \"LogP\": np.nan, \"QED\": np.nan, \"MW\": np.nan})\n",
    "            continue\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        if mol:\n",
    "            try:\n",
    "                logp = Descriptors.MolLogP(mol)\n",
    "                qed_score = QED.qed(mol)\n",
    "                mw = Descriptors.ExactMolWt(mol)\n",
    "\n",
    "                results.append({\n",
    "                    \"Molecule\": i+1,\n",
    "                    \"SELFIES\": selfies,\n",
    "                    \"SMILES\": smiles,\n",
    "                    \"LogP\": logp,\n",
    "                    \"QED\": qed_score,\n",
    "                    \"MW\": mw\n",
    "                })\n",
    "            except Exception:\n",
    "                 results.append({\"Molecule\": i+1, \"SELFIES\": selfies, \"SMILES\": smiles, \"LogP\": np.nan, \"QED\": np.nan, \"MW\": np.nan})\n",
    "\n",
    "    # Create and display the DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add a row for the target properties for easy comparison\n",
    "    target_row = pd.Series({\n",
    "        \"Molecule\": \"TARGET\", \n",
    "        \"SELFIES\": \"TARGET\",\n",
    "        \"SMILES\": \"TARGET\", \n",
    "        \"LogP\": target_logp, \n",
    "        \"QED\": target_qed, \n",
    "        \"MW\": target_mw\n",
    "    }, name=\"TARGET\").to_frame().T\n",
    "    \n",
    "    # Concatenate the target row and the results for visual comparison\n",
    "    df_styled = pd.concat([target_row.set_index('Molecule'), df.set_index('Molecule')])\n",
    "    \n",
    "    # Format numerical columns for presentation\n",
    "    df_styled = df_styled.apply(pd.to_numeric, errors='ignore').round(2)\n",
    "    \n",
    "    return df_styled\n",
    "\n",
    "df_styled = analyze_molecule_properties(selfies_list, smiles_list, desired_logp, desired_qed, desired_mw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 | Loss = 1.2955 | Accuracy = 0.0841\n",
    "Epoch 2 | Loss = 0.9395 | Accuracy = 0.1449\n",
    "Epoch 3 | Loss = 0.8566 | Accuracy = 0.2056\n",
    "Epoch 4 | Loss = 0.8215 | Accuracy = 0.2150\n",
    "Epoch 5 | Loss = 0.8026 | Accuracy = 0.2196\n",
    "Epoch 6 | Loss = 0.7900 | Accuracy = 0.2430\n",
    "Epoch 7 | Loss = 0.7800 | Accuracy = 0.2430\n",
    "Epoch 8 | Loss = 0.7710 | Accuracy = 0.2383\n",
    "Epoch 9 | Loss = 0.7621 | Accuracy = 0.2570\n",
    "Epoch 10 | Loss = 0.7531 | Accuracy = 0.2664\n",
    "Epoch 11 | Loss = 0.7447 | Accuracy = 0.2804\n",
    "Epoch 12 | Loss = 0.7367 | Accuracy = 0.2944\n",
    "Epoch 13 | Loss = 0.7290 | Accuracy = 0.3318\n",
    "Epoch 14 | Loss = 0.7221 | Accuracy = 0.3458\n",
    "Epoch 15 | Loss = 0.7160 | Accuracy = 0.3738\n",
    "Epoch 16 | Loss = 0.7108 | Accuracy = 0.3925\n",
    "Epoch 17 | Loss = 0.7063 | Accuracy = 0.3785\n",
    "Epoch 18 | Loss = 0.7022 | Accuracy = 0.3879\n",
    "Epoch 19 | Loss = 0.6984 | Accuracy = 0.3972\n",
    "Epoch 20 | Loss = 0.6950 | Accuracy = 0.4065\n",
    "Epoch 21 | Loss = 0.6917 | Accuracy = 0.4112\n",
    "Epoch 22 | Loss = 0.6885 | Accuracy = 0.4065\n",
    "Epoch 23 | Loss = 0.6854 | Accuracy = 0.4065\n",
    "Epoch 24 | Loss = 0.6824 | Accuracy = 0.4112\n",
    "Epoch 25 | Loss = 0.6795 | Accuracy = 0.4112\n",
    "...\n",
    "Epoch 97 | Loss = 0.5662 | Accuracy = 0.6308\n",
    "Epoch 98 | Loss = 0.5655 | Accuracy = 0.6308\n",
    "Epoch 99 | Loss = 0.5647 | Accuracy = 0.6355\n",
    "Epoch 100 | Loss = 0.5640 | Accuracy = 0.6355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Without JAX JIT for circuit representation and debugging\\ndef training_step(params, opt_state, x_token, x_props, y_target):\\n    def loss_fn(params):\\n        theta_params = params[\\'theta\\']\\n        theta_prop = params[\\'theta_prop\\']\\n        sigma_params = params[\\'sigma\\']\\n\\n        # Predict using theta_effective and sigma_params\\n        pred_probs = autoregressive_model(x_token, x_props, theta_effective, theta_prop, sigma_params)\\n        index = bits_to_index(y_target)\\n        return categorical_crossentropy(pred_probs, index)\\n\\n    grads = jax.grad(loss_fn)(params)\\n    updates, opt_state = optimizer.update(grads, opt_state, params)\\n    new_params = optax.apply_updates(params, updates)\\n    print(\"\\nQuantum Circuit:\")\\n    print(qml.draw(autoregressive_model)(\\n        x_token, x_props, \\n        theta_effective, \\n        new_params[\\'theta_prop\\'], \\n        new_params[\\'sigma\\']\\n    ))    #print(\"Target index:\", int(jax.device_get(bits_to_index(y_target))))\\n    loss = loss_fn(new_params)\\n    return new_params, loss, opt_state, grads'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Without JAX JIT for circuit representation and debugging\n",
    "def training_step(params, opt_state, x_token, x_props, y_target):\n",
    "    def loss_fn(params):\n",
    "        theta_params = params['theta']\n",
    "        theta_prop = params['theta_prop']\n",
    "        sigma_params = params['sigma']\n",
    "\n",
    "        # Predict using theta_effective and sigma_params\n",
    "        pred_probs = autoregressive_model(x_token, x_props, theta_effective, theta_prop, sigma_params)\n",
    "        index = bits_to_index(y_target)\n",
    "        return categorical_crossentropy(pred_probs, index)\n",
    "\n",
    "    grads = jax.grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    print(\"\\nQuantum Circuit:\")\n",
    "    print(qml.draw(autoregressive_model)(\n",
    "        x_token, x_props, \n",
    "        theta_effective, \n",
    "        new_params['theta_prop'], \n",
    "        new_params['sigma']\n",
    "    ))    #print(\"Target index:\", int(jax.device_get(bits_to_index(y_target))))\n",
    "    loss = loss_fn(new_params)\n",
    "    return new_params, loss, opt_state, grads'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_layers = 6\n",
    "h_local = 3\n",
    "prob_mask = NO MASK\n",
    "\n",
    "Epoch 1 | Loss = 0.6208 | Accuracy = 0.4185\n",
    "Epoch 2 | Loss = 0.5437 | Accuracy = 0.4909\n",
    "Epoch 3 | Loss = 0.5238 | Accuracy = 0.5092\n",
    "Epoch 4 | Loss = 0.5112 | Accuracy = 0.5222\n",
    "Epoch 5 | Loss = 0.5034 | Accuracy = 0.5330\n",
    "Epoch 6 | Loss = 0.5005 | Accuracy = 0.5372\n",
    "Epoch 7 | Loss = 0.4984 | Accuracy = 0.5426\n",
    "Epoch 8 | Loss = 0.4958 | Accuracy = 0.5470\n",
    "Epoch 9 | Loss = 0.4954 | Accuracy = 0.5492\n",
    "Epoch 10 | Loss = 0.4933 | Accuracy = 0.5497\n",
    "Epoch 11 | Loss = 0.4887 | Accuracy = 0.5585\n",
    "Epoch 12 | Loss = 0.4865 | Accuracy = 0.5625\n",
    "Epoch 13 | Loss = 0.4855 | Accuracy = 0.5594\n",
    "Epoch 14 | Loss = 0.4844 | Accuracy = 0.5634\n",
    "Epoch 15 | Loss = 0.4821 | Accuracy = 0.5645\n",
    "Epoch 16 | Loss = 0.4787 | Accuracy = 0.5690\n",
    "Epoch 17 | Loss = 0.4776 | Accuracy = 0.5752\n",
    "Epoch 18 | Loss = 0.4783 | Accuracy = 0.5689\n",
    "Epoch 19 | Loss = 0.4774 | Accuracy = 0.5783\n",
    "Epoch 20 | Loss = 0.4756 | Accuracy = 0.5784\n",
    "Epoch 21 | Loss = 0.4753 | Accuracy = 0.5751\n",
    "Epoch 22 | Loss = 0.4741 | Accuracy = 0.5789\n",
    "Epoch 23 | Loss = 0.4762 | Accuracy = 0.5767\n",
    "Epoch 24 | Loss = 0.4747 | Accuracy = 0.5771\n",
    "Epoch 25 | Loss = 0.4732 | Accuracy = 0.5787\n",
    "Epoch 26 | Loss = 0.4715 | Accuracy = 0.5852\n",
    "Epoch 27 | Loss = 0.4725 | Accuracy = 0.5805\n",
    "Epoch 28 | Loss = 0.4746 | Accuracy = 0.5792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h=2\n",
    "\n",
    "Epoch 1 | Loss = 2.6945 | Accuracy = 0.4050 \n",
    "Epoch 2 | Loss = 2.4173 | Accuracy = 0.4816 \n",
    "Epoch 3 | Loss = 2.3229 | Accuracy = 0.5047 \n",
    "Epoch 4 | Loss = 2.2768 | Accuracy = 0.5220\n",
    "Epoch 5 | Loss = 2.2505 | Accuracy = 0.5267 \n",
    "Epoch 6 | Loss = 2.2214 | Accuracy = 0.5399 \n",
    "Epoch 7 | Loss = 2.2051 | Accuracy = 0.5448 \n",
    "Epoch 8 | Loss = 2.1918 | Accuracy = 0.5523 \n",
    "Epoch 9 | Loss = 2.1790 | Accuracy = 0.5544 \n",
    "Epoch 10 | Loss = 2.1683 | Accuracy = 0.5577 \n",
    "Epoch 11 | Loss = 2.1654 | Accuracy = 0.5582 \n",
    "Epoch 12 | Loss = 2.1519 | Accuracy = 0.5617 \n",
    "Epoch 13 | Loss = 2.1452 | Accuracy = 0.5647 \n",
    "Epoch 14 | Loss = 2.1371 | Accuracy = 0.5670 \n",
    "Epoch 15 | Loss = 2.1380 | Accuracy = 0.5630 \n",
    "Epoch 16 | Loss = 2.1330 | Accuracy = 0.5654 \n",
    "Epoch 17 | Loss = 2.1251 | Accuracy = 0.5661 \n",
    "Epoch 18 | Loss = 2.1189 | Accuracy = 0.5703 \n",
    "Epoch 19 | Loss = 2.1176 | Accuracy = 0.5718 \n",
    "Epoch 20 | Loss = 2.1152 | Accuracy = 0.5698 \n",
    "Epoch 21 | Loss = 2.1162 | Accuracy = 0.5682 \n",
    "Epoch 22 | Loss = 2.1117 | Accuracy = 0.5704 \n",
    "Epoch 23 | Loss = 2.1126 | Accuracy = 0.5729 \n",
    "Epoch 24 | Loss = 2.1097 | Accuracy = 0.5735 \n",
    "Epoch 25 | Loss = 2.1040 | Accuracy = 0.5756 \n",
    "Epoch 26 | Loss = 2.1001 | Accuracy = 0.5751 \n",
    "Epoch 27 | Loss = 2.0959 | Accuracy = 0.5794 \n",
    "Epoch 28 | Loss = 2.0881 | Accuracy = 0.5808 \n",
    "Epoch 29 | Loss = 2.0891 | Accuracy = 0.5806 \n",
    "Epoch 30 | Loss = 2.0869 | Accuracy = 0.5808 \n",
    "Epoch 31 | Loss = 2.0843 | Accuracy = 0.5855 \n",
    "Epoch 32 | Loss = 2.0859 | Accuracy = 0.5839 \n",
    "Epoch 33 | Loss = 2.0841 | Accuracy = 0.5830"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
